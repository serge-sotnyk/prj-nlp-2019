{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM, TimeDistributed, Activation\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_file = './data/numberbatch-en-17.06.txt'\n",
    "\n",
    "cn_vectors = KeyedVectors.load_word2vec_format(cn_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tags(string):\n",
    "    return [tuple(i.split(\"/\")) for i in string.split()]\n",
    "\n",
    "def readTrainData(filename):\n",
    "    data = []\n",
    "    for line in open(filename):\n",
    "        line = line.strip()\n",
    "        #read in training or dev data with labels\n",
    "        if len(line.split('\\t')) == 7:\n",
    "            (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = \\\n",
    "            line.split('\\t')\n",
    "        else:\n",
    "            continue\n",
    "        # ignoring the training data that has middle label \n",
    "        nYes = eval(judge)[0]            \n",
    "        if nYes >= 3:\n",
    "            amt_label = True\n",
    "            data.append((split_tags(origsenttag), split_tags(candsenttag), amt_label))\n",
    "        elif nYes <= 1:\n",
    "            amt_label = False\n",
    "            data.append((split_tags(origsenttag), split_tags(candsenttag), amt_label))\n",
    "    return data\n",
    "\n",
    "def readTestData(filename):\n",
    "    data = []\n",
    "    for line in open(filename):\n",
    "        line = line.strip()\n",
    "        #read in training or dev data with labels\n",
    "        if len(line.split('\\t')) == 7:\n",
    "            (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = \\\n",
    "            line.split('\\t')\n",
    "        else:\n",
    "            continue\n",
    "        # ignoring the training data that has middle label \n",
    "        nYes = int(judge[0])\n",
    "        if nYes >= 4:\n",
    "            expert_label = True\n",
    "        elif nYes <= 2:\n",
    "            expert_label = False\n",
    "        else:\n",
    "            expert_label = None\n",
    "        data.append((split_tags(origsenttag), split_tags(candsenttag), expert_label))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11530"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = readTrainData(\"./data/data/dev.data\")\n",
    "train_data = readTrainData(\"./data/data/train.data\")\n",
    "test_data = readTestData(\"./data/data/test.data\")\n",
    "\n",
    "len(dev_data)  #4142\n",
    "len(test_data)  #972\n",
    "len(train_data)  #11530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetEmbedding(tweet):\n",
    "    word_vectors = []\n",
    "    exceptions = 0\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            word_vector = cn_vectors.get_vector(word[0].lower())\n",
    "        except:\n",
    "            continue\n",
    "        word_vectors.append(word_vector)\n",
    "        \n",
    "        \n",
    "    return np.mean(np.array(word_vectors), axis=0), exceptions\n",
    "\n",
    "\n",
    "def getLabel(label):\n",
    "    if label == True:\n",
    "        return 1\n",
    "    if label == False:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def getLabelsFeatures(data, is_train):\n",
    "    labels, features = [], []\n",
    "    for row in data:\n",
    "        lbl = getLabel(row[2])\n",
    "        if lbl is None and is_train:\n",
    "            continue\n",
    "        labels.append(lbl)\n",
    "        original_embedding, _ = getTweetEmbedding(row[0])\n",
    "        cand_embedding, _ = getTweetEmbedding(row[1])\n",
    "#         embedding = np.hstack([original_embedding, cand_embedding])\n",
    "        embedding = np.subtract(original_embedding, cand_embedding)\n",
    "#         embedding = distance.euclidean(original_embedding, cand_embedding)\n",
    "#         embedding = np.mean( np.array([ original_embedding, cand_embedding ]), axis=0 )\n",
    "        features.append(embedding)\n",
    "    return labels, features\n",
    "    \n",
    "    \n",
    "dev_labels, dev_features = getLabelsFeatures(dev_data, is_train=True)\n",
    "test_labels, test_features = getLabelsFeatures(test_data, is_train=False)\n",
    "train_labels, train_features = getLabelsFeatures(train_data, is_train=True)\n",
    "\n",
    "\n",
    "dev_features = np.array(dev_features)\n",
    "dev_labels = np.array(dev_labels)\n",
    "\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11530, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** KERAS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=300))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "11530/11530 [==============================] - 4s 386us/step - loss: 0.5717 - precision: 0.5628 - recall: 0.4133 - f1: 0.4375\n",
      "Epoch 2/15\n",
      "11530/11530 [==============================] - 4s 324us/step - loss: 0.5644 - precision: 0.6071 - recall: 0.4422 - f1: 0.4731\n",
      "Epoch 3/15\n",
      "11530/11530 [==============================] - 4s 328us/step - loss: 0.5629 - precision: 0.6214 - recall: 0.4551 - f1: 0.4877\n",
      "Epoch 4/15\n",
      "11530/11530 [==============================] - 4s 320us/step - loss: 0.5607 - precision: 0.6167 - recall: 0.4393 - f1: 0.4749\n",
      "Epoch 5/15\n",
      "11530/11530 [==============================] - 4s 327us/step - loss: 0.5587 - precision: 0.6175 - recall: 0.4619 - f1: 0.4919\n",
      "Epoch 6/15\n",
      "11530/11530 [==============================] - 4s 317us/step - loss: 0.5607 - precision: 0.6171 - recall: 0.4529 - f1: 0.4846\n",
      "Epoch 7/15\n",
      "11530/11530 [==============================] - 4s 318us/step - loss: 0.5614 - precision: 0.6247 - recall: 0.4592 - f1: 0.4911\n",
      "Epoch 8/15\n",
      "11530/11530 [==============================] - 4s 319us/step - loss: 0.5565 - precision: 0.6313 - recall: 0.4663 - f1: 0.5005 1s - loss: 0.5544 - precision\n",
      "Epoch 9/15\n",
      "11530/11530 [==============================] - 4s 316us/step - loss: 0.5590 - precision: 0.6214 - recall: 0.4602 - f1: 0.4925\n",
      "Epoch 10/15\n",
      "11530/11530 [==============================] - 4s 338us/step - loss: 0.5568 - precision: 0.6206 - recall: 0.4563 - f1: 0.4889\n",
      "Epoch 11/15\n",
      " 1270/11530 [==>...........................] - ETA: 3s - loss: 0.5466 - precision: 0.6546 - recall: 0.4803 - f1: 0.5223"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 1 must be a class",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-31b9b71f3992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_and_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    119\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3495\u001b[0m     \"\"\"\n\u001b[1;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3497\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3517\u001b[0m         \u001b[0mkth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3520\u001b[0m         \u001b[0mkth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \"\"\"\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.fit(train_features, train_labels, epochs=15, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_features)\n",
    "# stacked vectors\n",
    "# 838\tNN\t01_NN\t\tF: 0.187\tPrec: 0.367\tRec: 0.126\t\tP-corr: 0.243\tF1: 0.474\tPrec: 0.359\tRec: 0.697\n",
    "\n",
    "# subtract vectors\n",
    "# 838\tNN\t04_NN\t\tF: 0.522\tPrec: 0.497\tRec: 0.549\t\tP-corr: 0.415\tF1: 0.538\tPrec: 0.527\tRec: 0.549\n",
    "\n",
    "# distance vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\t0.0446\n",
      "false\t0.0111\n",
      "false\t0.4017\n",
      "false\t0.0797\n",
      "false\t0.0141\n",
      "false\t0.4567\n",
      "false\t0.0068\n",
      "false\t0.0000\n",
      "false\t0.0001\n",
      "false\t0.0031\n",
      "false\t0.0104\n",
      "false\t0.0183\n",
      "false\t0.1344\n",
      "false\t0.0046\n",
      "false\t0.0438\n",
      "true\t0.8084\n",
      "false\t0.0288\n",
      "false\t0.0006\n",
      "false\t0.0024\n",
      "false\t0.0001\n",
      "false\t0.0333\n",
      "false\t0.0084\n",
      "false\t0.0003\n",
      "false\t0.0112\n",
      "false\t0.2359\n",
      "false\t0.0854\n",
      "false\t0.2655\n",
      "true\t0.8829\n",
      "false\t0.2466\n",
      "false\t0.4014\n",
      "false\t0.2175\n",
      "true\t0.6120\n",
      "false\t0.2984\n",
      "false\t0.1389\n",
      "true\t0.8156\n",
      "true\t0.9210\n",
      "true\t0.9074\n",
      "false\t0.0844\n",
      "false\t0.0188\n",
      "true\t0.6777\n",
      "false\t0.0001\n",
      "false\t0.0000\n",
      "false\t0.3866\n",
      "false\t0.1778\n",
      "false\t0.0081\n",
      "true\t0.9270\n",
      "false\t0.0138\n",
      "true\t0.5994\n",
      "false\t0.0611\n",
      "false\t0.0000\n",
      "true\t0.9514\n",
      "false\t0.0843\n",
      "true\t0.9042\n",
      "false\t0.0839\n",
      "false\t0.0824\n",
      "false\t0.0064\n",
      "false\t0.0000\n",
      "true\t0.8301\n",
      "false\t0.4640\n",
      "true\t0.9041\n",
      "false\t0.0765\n",
      "false\t0.4209\n",
      "false\t0.0002\n",
      "false\t0.0007\n",
      "false\t0.0002\n",
      "false\t0.0013\n",
      "false\t0.0932\n",
      "false\t0.0000\n",
      "false\t0.0049\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0035\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0017\n",
      "false\t0.0000\n",
      "false\t0.0163\n",
      "false\t0.0007\n",
      "false\t0.0001\n",
      "true\t0.9153\n",
      "false\t0.0013\n",
      "true\t0.9057\n",
      "false\t0.0020\n",
      "false\t0.0265\n",
      "true\t0.6059\n",
      "false\t0.2269\n",
      "true\t0.5955\n",
      "true\t0.5283\n",
      "false\t0.0000\n",
      "true\t0.9261\n",
      "false\t0.0004\n",
      "false\t0.0054\n",
      "false\t0.0000\n",
      "true\t0.7715\n",
      "false\t0.1695\n",
      "true\t0.7464\n",
      "false\t0.0006\n",
      "false\t0.4979\n",
      "false\t0.0799\n",
      "false\t0.0003\n",
      "true\t0.7282\n",
      "false\t0.3598\n",
      "false\t0.0273\n",
      "false\t0.0002\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "true\t0.6075\n",
      "false\t0.0000\n",
      "false\t0.0001\n",
      "true\t0.6041\n",
      "false\t0.1311\n",
      "true\t0.9780\n",
      "false\t0.0000\n",
      "false\t0.0019\n",
      "false\t0.1516\n",
      "false\t0.0008\n",
      "false\t0.3356\n",
      "false\t0.2131\n",
      "true\t0.5279\n",
      "false\t0.2836\n",
      "false\t0.0133\n",
      "false\t0.0020\n",
      "false\t0.0510\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0059\n",
      "false\t0.0864\n",
      "false\t0.4749\n",
      "false\t0.0007\n",
      "false\t0.0077\n",
      "false\t0.0328\n",
      "false\t0.0000\n",
      "false\t0.0246\n",
      "false\t0.2349\n",
      "false\t0.0245\n",
      "true\t0.5959\n",
      "false\t0.0502\n",
      "false\t0.0061\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0014\n",
      "false\t0.0068\n",
      "false\t0.0000\n",
      "false\t0.0025\n",
      "false\t0.0004\n",
      "false\t0.0000\n",
      "false\t0.0886\n",
      "false\t0.0625\n",
      "false\t0.1064\n",
      "true\t0.5142\n",
      "false\t0.0117\n",
      "false\t0.0000\n",
      "false\t0.0001\n",
      "false\t0.0000\n",
      "false\t0.0002\n",
      "false\t0.0000\n",
      "false\t0.0002\n",
      "false\t0.0000\n",
      "false\t0.0001\n",
      "false\t0.0000\n",
      "false\t0.1208\n",
      "false\t0.0099\n",
      "false\t0.0000\n",
      "false\t0.1289\n",
      "false\t0.0001\n",
      "false\t0.0226\n",
      "true\t0.9642\n",
      "false\t0.0022\n",
      "false\t0.0868\n",
      "false\t0.0009\n",
      "false\t0.0048\n",
      "false\t0.0814\n",
      "false\t0.0349\n",
      "false\t0.0036\n",
      "false\t0.0127\n",
      "false\t0.0000\n",
      "false\t0.0001\n",
      "false\t0.0090\n",
      "false\t0.0000\n",
      "false\t0.0003\n",
      "false\t0.0002\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0403\n",
      "false\t0.0648\n",
      "false\t0.0022\n",
      "false\t0.1157\n",
      "false\t0.0000\n",
      "true\t0.8650\n",
      "true\t0.8958\n",
      "true\t0.6025\n",
      "false\t0.0745\n",
      "false\t0.0039\n",
      "false\t0.0048\n",
      "false\t0.4897\n",
      "false\t0.1382\n",
      "false\t0.0521\n",
      "false\t0.0003\n",
      "false\t0.0014\n",
      "false\t0.0026\n",
      "false\t0.0000\n",
      "false\t0.0001\n",
      "false\t0.0015\n",
      "false\t0.0000\n",
      "false\t0.0177\n",
      "false\t0.0766\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.3253\n",
      "false\t0.1907\n",
      "false\t0.4102\n",
      "false\t0.4411\n",
      "false\t0.0156\n",
      "false\t0.0000\n",
      "false\t0.0009\n",
      "false\t0.0263\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0013\n",
      "false\t0.0174\n",
      "false\t0.2774\n",
      "true\t0.7233\n",
      "true\t0.9575\n",
      "false\t0.0367\n",
      "true\t0.5366\n",
      "true\t0.8020\n",
      "false\t0.3943\n",
      "false\t0.1139\n",
      "false\t0.0309\n",
      "false\t0.0787\n",
      "false\t0.4895\n",
      "true\t0.5943\n",
      "false\t0.1082\n",
      "true\t0.9165\n",
      "true\t0.7655\n",
      "true\t0.8337\n",
      "true\t0.5315\n",
      "true\t0.9481\n",
      "true\t0.7993\n",
      "false\t0.0773\n",
      "false\t0.0177\n",
      "false\t0.0086\n",
      "false\t0.0393\n",
      "false\t0.0000\n",
      "false\t0.4549\n",
      "false\t0.0000\n",
      "false\t0.2879\n",
      "true\t0.8570\n",
      "false\t0.0006\n",
      "true\t0.7472\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0040\n",
      "false\t0.1508\n",
      "true\t0.9200\n",
      "false\t0.0624\n",
      "false\t0.0000\n",
      "false\t0.0018\n",
      "false\t0.0001\n",
      "true\t0.5580\n",
      "false\t0.1452\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.1259\n",
      "false\t0.0812\n",
      "false\t0.0000\n",
      "false\t0.0002\n",
      "false\t0.3629\n",
      "false\t0.0001\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0011\n",
      "true\t0.7476\n",
      "false\t0.0047\n",
      "false\t0.0028\n",
      "false\t0.0000\n",
      "false\t0.0313\n",
      "false\t0.0605\n",
      "false\t0.0989\n",
      "false\t0.0775\n",
      "true\t0.7685\n",
      "false\t0.0113\n",
      "false\t0.0017\n",
      "false\t0.0677\n",
      "false\t0.0107\n",
      "false\t0.0000\n",
      "false\t0.1863\n",
      "false\t0.1506\n",
      "false\t0.0002\n",
      "true\t0.5294\n",
      "false\t0.0041\n",
      "false\t0.0049\n",
      "true\t0.6339\n",
      "true\t0.9269\n",
      "false\t0.0343\n",
      "false\t0.3064\n",
      "false\t0.0851\n",
      "true\t0.6640\n",
      "true\t0.5693\n",
      "false\t0.0006\n",
      "false\t0.0115\n",
      "false\t0.0002\n",
      "false\t0.3522\n",
      "false\t0.4979\n",
      "false\t0.1758\n",
      "false\t0.4861\n",
      "false\t0.0499\n",
      "false\t0.0193\n",
      "false\t0.0001\n",
      "true\t0.8297\n",
      "false\t0.4170\n",
      "true\t0.7262\n",
      "false\t0.2661\n",
      "true\t0.7733\n",
      "true\t0.5971\n",
      "false\t0.0021\n",
      "false\t0.0280\n",
      "false\t0.1732\n",
      "true\t0.8715\n",
      "false\t0.0166\n",
      "false\t0.0002\n",
      "true\t0.7036\n",
      "true\t0.7586\n",
      "false\t0.2255\n",
      "false\t0.0021\n",
      "false\t0.0063\n",
      "false\t0.0550\n",
      "true\t0.8507\n",
      "false\t0.1800\n",
      "true\t0.7237\n",
      "false\t0.0310\n",
      "false\t0.3573\n",
      "false\t0.0235\n",
      "false\t0.0000\n",
      "false\t0.1054\n",
      "false\t0.0016\n",
      "false\t0.0053\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "true\t0.7125\n",
      "true\t0.7840\n",
      "false\t0.2031\n",
      "true\t0.7905\n",
      "false\t0.0000\n",
      "false\t0.0002\n",
      "false\t0.0105\n",
      "false\t0.0735\n",
      "false\t0.0002\n",
      "false\t0.0000\n",
      "false\t0.0009\n",
      "false\t0.0002\n",
      "true\t0.5423\n",
      "false\t0.0073\n",
      "false\t0.0882\n",
      "false\t0.0013\n",
      "false\t0.1500\n",
      "true\t0.6249\n",
      "false\t0.3761\n",
      "true\t0.6520\n",
      "true\t0.5311\n",
      "false\t0.4065\n",
      "false\t0.0014\n",
      "true\t0.6228\n",
      "true\t0.9012\n",
      "false\t0.4503\n",
      "true\t0.9386\n",
      "true\t0.9642\n",
      "false\t0.2275\n",
      "true\t0.8624\n",
      "true\t0.8740\n",
      "true\t0.6593\n",
      "true\t0.9454\n",
      "false\t0.1106\n",
      "false\t0.4376\n",
      "false\t0.0012\n",
      "true\t0.8788\n",
      "false\t0.4041\n",
      "true\t0.9674\n",
      "false\t0.0128\n",
      "true\t0.7365\n",
      "true\t0.9552\n",
      "true\t0.9333\n",
      "false\t0.0510\n",
      "false\t0.0463\n",
      "true\t0.9463\n",
      "false\t0.0180\n",
      "false\t0.1012\n",
      "true\t0.7162\n",
      "true\t0.6945\n",
      "true\t0.9677\n",
      "true\t0.7820\n",
      "true\t0.8526\n",
      "false\t0.2699\n",
      "true\t0.9440\n",
      "true\t0.8341\n",
      "false\t0.1677\n",
      "true\t0.6143\n",
      "false\t0.0118\n",
      "true\t0.8203\n",
      "true\t0.9642\n",
      "true\t0.8858\n",
      "true\t0.9603\n",
      "true\t0.7994\n",
      "true\t0.7607\n",
      "false\t0.0619\n",
      "true\t0.7262\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "true\t0.9512\n",
      "true\t0.6408\n",
      "false\t0.0182\n",
      "false\t0.4256\n",
      "true\t0.5334\n",
      "true\t0.6821\n",
      "true\t0.5954\n",
      "true\t0.8650\n",
      "false\t0.2223\n",
      "true\t0.9672\n",
      "false\t0.0062\n",
      "true\t0.9346\n",
      "false\t0.0082\n",
      "false\t0.0597\n",
      "false\t0.0000\n",
      "false\t0.1974\n",
      "false\t0.2700\n",
      "false\t0.1846\n",
      "false\t0.1329\n",
      "false\t0.0000\n",
      "false\t0.0081\n",
      "false\t0.0000\n",
      "false\t0.3873\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.4496\n",
      "false\t0.0105\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.2366\n",
      "true\t0.7531\n",
      "false\t0.2643\n",
      "false\t0.4064\n",
      "false\t0.0021\n",
      "false\t0.0034\n",
      "true\t0.6658\n",
      "false\t0.0125\n",
      "false\t0.0036\n",
      "false\t0.0007\n",
      "false\t0.0012\n",
      "false\t0.0044\n",
      "false\t0.0000\n",
      "false\t0.0006\n",
      "true\t0.8373\n",
      "false\t0.0578\n",
      "true\t0.6659\n",
      "true\t0.9477\n",
      "false\t0.1424\n",
      "true\t0.8985\n",
      "false\t0.0737\n",
      "false\t0.0028\n",
      "false\t0.3455\n",
      "true\t0.7384\n",
      "false\t0.3545\n",
      "false\t0.0380\n",
      "false\t0.0210\n",
      "false\t0.0848\n",
      "false\t0.0001\n",
      "false\t0.1585\n",
      "false\t0.0133\n",
      "false\t0.0321\n",
      "false\t0.1309\n",
      "false\t0.0246\n",
      "false\t0.0161\n",
      "false\t0.0008\n",
      "false\t0.1517\n",
      "true\t0.6492\n",
      "false\t0.4307\n",
      "false\t0.0012\n",
      "false\t0.0084\n",
      "false\t0.0420\n",
      "false\t0.0367\n",
      "false\t0.0004\n",
      "false\t0.0114\n",
      "false\t0.0000\n",
      "false\t0.0028\n",
      "false\t0.0001\n",
      "false\t0.0000\n",
      "false\t0.0001\n",
      "false\t0.0006\n",
      "false\t0.4373\n",
      "false\t0.4711\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0200\n",
      "false\t0.0554\n",
      "true\t0.6340\n",
      "false\t0.1210\n",
      "false\t0.0042\n",
      "false\t0.0000\n",
      "false\t0.4775\n",
      "false\t0.0008\n",
      "false\t0.1041\n",
      "false\t0.0028\n",
      "false\t0.0000\n",
      "true\t0.7029\n",
      "false\t0.0051\n",
      "false\t0.0026\n",
      "true\t0.7686\n",
      "false\t0.0002\n",
      "false\t0.3614\n",
      "true\t0.7566\n",
      "false\t0.2618\n",
      "false\t0.1286\n",
      "false\t0.1176\n",
      "true\t0.9096\n",
      "true\t0.8450\n",
      "true\t0.5396\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0039\n",
      "false\t0.1203\n",
      "true\t0.5310\n",
      "true\t0.8995\n",
      "false\t0.4297\n",
      "true\t0.8864\n",
      "true\t0.9198\n",
      "true\t0.7496\n",
      "true\t0.6796\n",
      "true\t0.9382\n",
      "true\t0.8433\n",
      "false\t0.0215\n",
      "true\t0.7988\n",
      "true\t0.8476\n",
      "false\t0.0648\n",
      "true\t0.8734\n",
      "false\t0.3861\n",
      "false\t0.4243\n",
      "true\t0.6928\n",
      "true\t0.8978\n",
      "true\t0.6345\n",
      "true\t0.8272\n",
      "false\t0.4555\n",
      "true\t0.9086\n",
      "false\t0.4793\n",
      "false\t0.0042\n",
      "false\t0.1303\n",
      "false\t0.0049\n",
      "false\t0.4344\n",
      "true\t0.8837\n",
      "true\t0.8076\n",
      "true\t0.9347\n",
      "true\t0.9172\n",
      "true\t0.7808\n",
      "false\t0.2693\n",
      "false\t0.0000\n",
      "false\t0.0519\n",
      "false\t0.4334\n",
      "false\t0.4024\n",
      "false\t0.0071\n",
      "false\t0.0000\n",
      "false\t0.1977\n",
      "true\t0.9532\n",
      "true\t0.9475\n",
      "false\t0.0005\n",
      "false\t0.0476\n",
      "false\t0.1626\n",
      "false\t0.0169\n",
      "false\t0.1012\n",
      "false\t0.4045\n",
      "false\t0.0040\n",
      "false\t0.0402\n",
      "false\t0.0305\n",
      "false\t0.0001\n",
      "false\t0.0001\n",
      "false\t0.4181\n",
      "false\t0.2999\n",
      "false\t0.0001\n",
      "false\t0.0269\n",
      "false\t0.0491\n",
      "true\t0.7363\n",
      "false\t0.0000\n",
      "true\t0.9422\n",
      "false\t0.0823\n",
      "false\t0.3845\n",
      "false\t0.4020\n",
      "false\t0.0001\n",
      "false\t0.0364\n",
      "false\t0.0041\n",
      "false\t0.0000\n",
      "false\t0.0330\n",
      "true\t0.7103\n",
      "false\t0.0000\n",
      "true\t0.7290\n",
      "true\t0.5183\n",
      "false\t0.0006\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.3525\n",
      "false\t0.0001\n",
      "false\t0.0000\n",
      "true\t0.9373\n",
      "false\t0.0889\n",
      "false\t0.0000\n",
      "false\t0.0003\n",
      "false\t0.0042\n",
      "false\t0.0000\n",
      "false\t0.0001\n",
      "false\t0.2345\n",
      "false\t0.4890\n",
      "false\t0.0471\n",
      "false\t0.0010\n",
      "false\t0.0014\n",
      "false\t0.0093\n",
      "true\t0.9293\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0014\n",
      "false\t0.0000\n",
      "false\t0.1839\n",
      "false\t0.0951\n",
      "false\t0.0539\n",
      "true\t0.5044\n",
      "true\t0.8594\n",
      "true\t0.9275\n",
      "true\t0.7704\n",
      "true\t0.9519\n",
      "true\t0.9901\n",
      "true\t0.9572\n",
      "true\t0.6207\n",
      "true\t0.9604\n",
      "true\t0.9296\n",
      "true\t0.9497\n",
      "true\t0.5795\n",
      "true\t0.9456\n",
      "true\t0.9266\n",
      "true\t0.9364\n",
      "false\t0.0450\n",
      "true\t0.7295\n",
      "false\t0.1722\n",
      "false\t0.3843\n",
      "true\t0.9046\n",
      "true\t0.7757\n",
      "true\t0.8831\n",
      "true\t0.6091\n",
      "true\t0.9049\n",
      "false\t0.0007\n",
      "false\t0.0963\n",
      "true\t0.7268\n",
      "false\t0.0097\n",
      "true\t0.5773\n",
      "true\t0.8198\n",
      "false\t0.0144\n",
      "false\t0.0148\n",
      "false\t0.0305\n",
      "false\t0.2804\n",
      "false\t0.0076\n",
      "false\t0.0008\n",
      "false\t0.0004\n",
      "false\t0.0034\n",
      "false\t0.0172\n",
      "false\t0.0237\n",
      "true\t0.8633\n",
      "false\t0.0034\n",
      "false\t0.0039\n",
      "true\t0.5556\n",
      "false\t0.0342\n",
      "false\t0.0253\n",
      "false\t0.0505\n",
      "false\t0.0000\n",
      "false\t0.1925\n",
      "false\t0.0010\n",
      "false\t0.0006\n",
      "false\t0.0080\n",
      "true\t0.7877\n",
      "false\t0.1963\n",
      "false\t0.0053\n",
      "true\t0.6398\n",
      "false\t0.2413\n",
      "false\t0.1329\n",
      "false\t0.2247\n",
      "true\t0.9610\n",
      "false\t0.0279\n",
      "false\t0.0000\n",
      "true\t0.8825\n",
      "false\t0.4717\n",
      "true\t0.6705\n",
      "true\t0.8630\n",
      "true\t0.8219\n",
      "true\t0.7805\n",
      "true\t0.8110\n",
      "true\t0.8986\n",
      "false\t0.1445\n",
      "false\t0.0495\n",
      "true\t0.9482\n",
      "true\t0.7543\n",
      "true\t0.8306\n",
      "true\t0.6843\n",
      "false\t0.0995\n",
      "true\t0.6335\n",
      "false\t0.0008\n",
      "false\t0.0215\n",
      "false\t0.0035\n",
      "false\t0.1813\n",
      "false\t0.0000\n",
      "false\t0.0643\n",
      "false\t0.0007\n",
      "false\t0.0654\n",
      "false\t0.0073\n",
      "false\t0.0008\n",
      "true\t0.5608\n",
      "false\t0.0006\n",
      "false\t0.0959\n",
      "false\t0.1794\n",
      "false\t0.0000\n",
      "false\t0.4980\n",
      "false\t0.0000\n",
      "false\t0.0253\n",
      "false\t0.0000\n",
      "false\t0.3078\n",
      "false\t0.0002\n",
      "false\t0.0102\n",
      "false\t0.0385\n",
      "false\t0.0000\n",
      "true\t0.5657\n",
      "false\t0.4976\n",
      "false\t0.4781\n",
      "false\t0.0508\n",
      "true\t0.9419\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0313\n",
      "false\t0.0002\n",
      "false\t0.0003\n",
      "false\t0.0001\n",
      "false\t0.0688\n",
      "false\t0.3958\n",
      "false\t0.1887\n",
      "false\t0.0277\n",
      "false\t0.3761\n",
      "true\t0.8108\n",
      "false\t0.0000\n",
      "false\t0.0089\n",
      "false\t0.0092\n",
      "false\t0.0042\n",
      "false\t0.0299\n",
      "false\t0.2012\n",
      "true\t0.8140\n",
      "true\t0.7464\n",
      "true\t0.6985\n",
      "false\t0.0004\n",
      "true\t0.5301\n",
      "true\t0.8216\n",
      "true\t0.7006\n",
      "false\t0.0004\n",
      "true\t0.7823\n",
      "false\t0.2387\n",
      "false\t0.3575\n",
      "true\t0.6364\n",
      "false\t0.0000\n",
      "false\t0.0014\n",
      "false\t0.0479\n",
      "false\t0.0413\n",
      "true\t0.8321\n",
      "false\t0.4098\n",
      "true\t0.8592\n",
      "false\t0.0137\n",
      "false\t0.0001\n",
      "false\t0.0004\n",
      "false\t0.3914\n",
      "false\t0.0215\n",
      "true\t0.6187\n",
      "false\t0.0002\n",
      "false\t0.0461\n",
      "false\t0.1634\n",
      "false\t0.0207\n",
      "false\t0.0456\n",
      "true\t0.7445\n",
      "false\t0.1311\n",
      "false\t0.0012\n",
      "true\t0.9704\n",
      "false\t0.1387\n",
      "false\t0.4856\n",
      "false\t0.0197\n",
      "false\t0.0000\n",
      "false\t0.0123\n",
      "false\t0.0139\n",
      "false\t0.0011\n",
      "false\t0.1740\n",
      "true\t0.6310\n",
      "false\t0.2275\n",
      "true\t0.9242\n",
      "false\t0.4374\n",
      "false\t0.0044\n",
      "false\t0.3197\n",
      "true\t0.8617\n",
      "false\t0.0449\n",
      "true\t0.9794\n",
      "false\t0.0000\n",
      "true\t0.8267\n",
      "true\t0.5670\n",
      "true\t0.6132\n",
      "false\t0.0018\n",
      "false\t0.0000\n",
      "false\t0.1684\n",
      "false\t0.0001\n",
      "false\t0.0207\n",
      "false\t0.4830\n",
      "false\t0.0071\n",
      "false\t0.0003\n",
      "false\t0.0000\n",
      "false\t0.0436\n",
      "false\t0.0000\n",
      "false\t0.0044\n",
      "false\t0.0031\n",
      "false\t0.0002\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0024\n",
      "false\t0.0039\n",
      "false\t0.2010\n",
      "false\t0.1495\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0002\n",
      "false\t0.0002\n",
      "true\t0.9293\n",
      "false\t0.3332\n",
      "false\t0.0896\n",
      "true\t0.7489\n",
      "false\t0.0272\n",
      "false\t0.1719\n",
      "true\t0.8159\n",
      "false\t0.0053\n",
      "true\t0.9310\n",
      "true\t0.8118\n",
      "true\t0.9354\n",
      "false\t0.0003\n",
      "false\t0.0000\n",
      "false\t0.0015\n",
      "true\t0.9473\n",
      "true\t0.9642\n",
      "true\t0.8779\n",
      "false\t0.2241\n",
      "false\t0.0007\n",
      "true\t0.5455\n",
      "false\t0.1772\n",
      "false\t0.4799\n",
      "false\t0.0001\n",
      "true\t0.8086\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.4702\n",
      "true\t0.5217\n",
      "false\t0.0852\n",
      "false\t0.0588\n",
      "true\t0.9736\n",
      "false\t0.0044\n",
      "false\t0.0011\n",
      "false\t0.4676\n",
      "false\t0.0894\n",
      "false\t0.0036\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0009\n",
      "false\t0.0001\n",
      "false\t0.0000\n",
      "false\t0.0020\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0034\n",
      "false\t0.0034\n",
      "false\t0.0171\n",
      "false\t0.0007\n",
      "false\t0.3459\n",
      "false\t0.0959\n",
      "true\t0.9249\n",
      "false\t0.0023\n",
      "false\t0.0175\n",
      "false\t0.4913\n",
      "true\t0.8343\n",
      "true\t0.5123\n",
      "false\t0.3342\n",
      "true\t0.7569\n",
      "false\t0.0002\n",
      "false\t0.0010\n",
      "false\t0.0000\n",
      "false\t0.0125\n",
      "false\t0.2710\n",
      "false\t0.0148\n",
      "false\t0.0037\n",
      "false\t0.0000\n",
      "false\t0.0053\n",
      "false\t0.0000\n",
      "false\t0.0118\n",
      "false\t0.0012\n",
      "false\t0.0000\n",
      "false\t0.0655\n",
      "false\t0.0008\n",
      "false\t0.0000\n",
      "false\t0.2679\n",
      "false\t0.0007\n",
      "false\t0.0001\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0003\n",
      "false\t0.0022\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.3691\n",
      "true\t0.6246\n",
      "true\t0.9522\n",
      "false\t0.1782\n",
      "true\t0.6035\n",
      "true\t0.6119\n",
      "false\t0.0052\n",
      "true\t0.9511\n",
      "true\t0.9212\n",
      "false\t0.2698\n",
      "false\t0.0037\n",
      "true\t0.8872\n",
      "true\t0.9312\n",
      "true\t0.5956\n",
      "false\t0.0442\n",
      "false\t0.2528\n",
      "false\t0.4760\n",
      "true\t0.9075\n",
      "true\t0.8793\n",
      "true\t0.9527\n",
      "false\t0.0454\n",
      "false\t0.0772\n",
      "false\t0.4490\n",
      "false\t0.0338\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0046\n",
      "false\t0.0000\n",
      "false\t0.0006\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0339\n",
      "false\t0.0002\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n",
      "false\t0.0000\n"
     ]
    }
   ],
   "source": [
    "for prediction in predictions:\n",
    "    if prediction >= 0.5:\n",
    "        print('true\\t' + \"{0:.4f}\".format(prediction[0]))\n",
    "    else:\n",
    "        print('false\\t' + \"{0:.4f}\".format(prediction[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  4.8min finished\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_recall_macro'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_precision_macro'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/leonid/Projects/prj-nlp-2019/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_f1_macro'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([19.88594723, 19.64271736, 20.15567207]),\n",
       " 'score_time': array([25.87473679, 25.69427466, 25.12932611]),\n",
       " 'test_recall_macro': array([0.5, 0.5, 0.5]),\n",
       " 'train_recall_macro': array([0.5, 0.5, 0.5]),\n",
       " 'test_precision_macro': array([0.32674298, 0.32669789, 0.32669789]),\n",
       " 'train_precision_macro': array([0.32669789, 0.32672044, 0.32672044]),\n",
       " 'test_f1_macro': array([0.39521712, 0.39518414, 0.39518414]),\n",
       " 'train_f1_macro': array([0.39518414, 0.39520063, 0.39520063])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf = DecisionTreeClassifier(max_depth=10)\n",
    "# 'test_f1_macro': array([0.56202534, 0.5483129 , 0.53597202])\n",
    "# 838\tNN\t01_NN\t\tF: 0.182\tPrec: 0.203\tRec: 0.166\t\tP-corr: -0.028\tF1: 0.367\tPrec: 0.240\tRec: 0.783\n",
    "\n",
    "# clf = RandomForestClassifier(max_depth=10, n_estimators=15, max_features=3)\n",
    "#''test_f1_macro': array([0.48649772, 0.48084575, 0.52552964])\n",
    "\n",
    "clf = SVC(gamma='auto', random_state=42)\n",
    "#  'test_f1_macro': array([0.39521712, 0.39518414, 0.39518414])\n",
    "# 838\tNN\t01_NN\t\tF: 0.151\tPrec: 0.250\tRec: 0.109\t\tP-corr: 0.201\tF1: 0.415\tPrec: 0.279\tRec: 0.806\n",
    "# difference of vectors\n",
    "# 838\tNN\t04_NN\t\tF: 0.522\tPrec: 0.497\tRec: 0.549\t\tP-corr: 0.415\tF1: 0.538\tPrec: 0.527\tRec: 0.549\n",
    "\n",
    "cross_validate(clf, train_features, train_labels, scoring=['recall_macro', 'precision_macro', 'f1_macro'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(gamma='auto', random_state=42, probability=True)\n",
    "clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\t0.3839\n",
      "false\t0.3206\n",
      "false\t0.3670\n",
      "false\t0.3829\n",
      "false\t0.3564\n",
      "false\t0.2975\n",
      "false\t0.3389\n",
      "false\t0.2964\n",
      "false\t0.3392\n",
      "false\t0.3992\n",
      "false\t0.4210\n",
      "false\t0.3984\n",
      "false\t0.3327\n",
      "false\t0.3818\n",
      "false\t0.4774\n",
      "false\t0.4899\n",
      "false\t0.4253\n",
      "false\t0.3766\n",
      "false\t0.3488\n",
      "false\t0.4098\n",
      "false\t0.4347\n",
      "false\t0.3949\n",
      "false\t0.3190\n",
      "false\t0.3168\n",
      "false\t0.3405\n",
      "false\t0.3622\n",
      "false\t0.3540\n",
      "false\t0.4107\n",
      "false\t0.3379\n",
      "false\t0.3192\n",
      "false\t0.3452\n",
      "false\t0.4009\n",
      "false\t0.4811\n",
      "false\t0.3903\n",
      "false\t0.4586\n",
      "false\t0.4216\n",
      "false\t0.4711\n",
      "false\t0.4272\n",
      "false\t0.3463\n",
      "false\t0.4287\n",
      "false\t0.3576\n",
      "false\t0.3676\n",
      "false\t0.2653\n",
      "false\t0.2458\n",
      "false\t0.3488\n",
      "false\t0.4280\n",
      "false\t0.3694\n",
      "false\t0.3883\n",
      "false\t0.3984\n",
      "false\t0.3107\n",
      "false\t0.4230\n",
      "false\t0.3141\n",
      "false\t0.3231\n",
      "false\t0.4565\n",
      "false\t0.3889\n",
      "false\t0.4139\n",
      "false\t0.3586\n",
      "false\t0.3804\n",
      "false\t0.2906\n",
      "false\t0.3274\n",
      "false\t0.3438\n",
      "false\t0.4547\n",
      "false\t0.2918\n",
      "false\t0.2828\n",
      "false\t0.2472\n",
      "false\t0.2553\n",
      "false\t0.2625\n",
      "false\t0.2329\n",
      "false\t0.2713\n",
      "false\t0.2506\n",
      "false\t0.2557\n",
      "false\t0.2965\n",
      "false\t0.1844\n",
      "false\t0.2363\n",
      "false\t0.1581\n",
      "false\t0.2037\n",
      "false\t0.2347\n",
      "false\t0.2675\n",
      "false\t0.2917\n",
      "false\t0.4398\n",
      "false\t0.4066\n",
      "false\t0.3791\n",
      "false\t0.2759\n",
      "false\t0.2941\n",
      "false\t0.3105\n",
      "false\t0.3667\n",
      "false\t0.2865\n",
      "false\t0.3805\n",
      "false\t0.2542\n",
      "false\t0.3898\n",
      "false\t0.2904\n",
      "false\t0.2595\n",
      "false\t0.2059\n",
      "false\t0.3605\n",
      "false\t0.3172\n",
      "false\t0.4388\n",
      "false\t0.3573\n",
      "false\t0.3934\n",
      "false\t0.3571\n",
      "false\t0.3338\n",
      "false\t0.3013\n",
      "false\t0.2750\n",
      "false\t0.4151\n",
      "false\t0.2036\n",
      "false\t0.1176\n",
      "false\t0.1515\n",
      "false\t0.1808\n",
      "false\t0.2950\n",
      "false\t0.1934\n",
      "false\t0.1932\n",
      "false\t0.4056\n",
      "false\t0.3347\n",
      "false\t0.4323\n",
      "false\t0.2834\n",
      "false\t0.3405\n",
      "false\t0.3475\n",
      "false\t0.2532\n",
      "false\t0.3887\n",
      "false\t0.2524\n",
      "false\t0.3184\n",
      "false\t0.4029\n",
      "false\t0.2910\n",
      "false\t0.3800\n",
      "false\t0.3887\n",
      "false\t0.2199\n",
      "false\t0.1838\n",
      "false\t0.2735\n",
      "false\t0.3475\n",
      "false\t0.2684\n",
      "false\t0.2174\n",
      "false\t0.2037\n",
      "false\t0.3411\n",
      "false\t0.3420\n",
      "false\t0.3332\n",
      "false\t0.3224\n",
      "false\t0.4201\n",
      "false\t0.5000\n",
      "false\t0.4330\n",
      "false\t0.3988\n",
      "false\t0.2525\n",
      "false\t0.2783\n",
      "false\t0.3083\n",
      "false\t0.3193\n",
      "false\t0.3051\n",
      "false\t0.3000\n",
      "false\t0.2645\n",
      "false\t0.2460\n",
      "false\t0.2324\n",
      "false\t0.2864\n",
      "false\t0.2770\n",
      "false\t0.2852\n",
      "false\t0.3588\n",
      "false\t0.3904\n",
      "false\t0.3340\n",
      "false\t0.4674\n",
      "false\t0.3348\n",
      "false\t0.2711\n",
      "false\t0.2769\n",
      "false\t0.3776\n",
      "false\t0.3049\n",
      "false\t0.3621\n",
      "false\t0.2069\n",
      "false\t0.3196\n",
      "false\t0.3315\n",
      "false\t0.3064\n",
      "false\t0.2947\n",
      "false\t0.2684\n",
      "false\t0.3747\n",
      "false\t0.4313\n",
      "false\t0.3326\n",
      "false\t0.3779\n",
      "false\t0.2723\n",
      "false\t0.3344\n",
      "false\t0.2534\n",
      "false\t0.3375\n",
      "false\t0.2536\n",
      "false\t0.1972\n",
      "false\t0.2579\n",
      "false\t0.1802\n",
      "false\t0.2031\n",
      "false\t0.3366\n",
      "false\t0.3191\n",
      "false\t0.3120\n",
      "false\t0.2705\n",
      "false\t0.3424\n",
      "false\t0.3728\n",
      "false\t0.4255\n",
      "false\t0.2990\n",
      "false\t0.3183\n",
      "false\t0.3365\n",
      "false\t0.3472\n",
      "false\t0.3761\n",
      "false\t0.3135\n",
      "false\t0.3360\n",
      "false\t0.2859\n",
      "false\t0.2820\n",
      "false\t0.2892\n",
      "false\t0.2992\n",
      "false\t0.4045\n",
      "false\t0.3208\n",
      "false\t0.3919\n",
      "false\t0.3378\n",
      "false\t0.2442\n",
      "false\t0.3443\n",
      "false\t0.2716\n",
      "false\t0.4334\n",
      "false\t0.4306\n",
      "false\t0.4466\n",
      "false\t0.3320\n",
      "false\t0.3453\n",
      "false\t0.2945\n",
      "false\t0.3629\n",
      "false\t0.4065\n",
      "false\t0.3600\n",
      "false\t0.3466\n",
      "false\t0.2409\n",
      "false\t0.2454\n",
      "false\t0.2353\n",
      "false\t0.3570\n",
      "false\t0.2196\n",
      "false\t0.2727\n",
      "false\t0.3677\n",
      "false\t0.4022\n",
      "false\t0.4473\n",
      "false\t0.4459\n",
      "false\t0.4542\n",
      "false\t0.3125\n",
      "false\t0.4335\n",
      "false\t0.3933\n",
      "false\t0.3587\n",
      "false\t0.3957\n",
      "true\t0.5129\n",
      "false\t0.3981\n",
      "false\t0.2997\n",
      "false\t0.3526\n",
      "false\t0.3359\n",
      "false\t0.3153\n",
      "false\t0.4125\n",
      "false\t0.3796\n",
      "false\t0.3655\n",
      "false\t0.4421\n",
      "false\t0.3951\n",
      "false\t0.2214\n",
      "false\t0.2523\n",
      "false\t0.3277\n",
      "false\t0.2571\n",
      "false\t0.2508\n",
      "false\t0.3327\n",
      "false\t0.2430\n",
      "false\t0.4089\n",
      "false\t0.4674\n",
      "false\t0.2950\n",
      "false\t0.3852\n",
      "false\t0.2500\n",
      "false\t0.1367\n",
      "false\t0.3311\n",
      "false\t0.3098\n",
      "false\t0.2966\n",
      "false\t0.2776\n",
      "false\t0.2229\n",
      "false\t0.1920\n",
      "false\t0.2926\n",
      "false\t0.3451\n",
      "false\t0.3399\n",
      "false\t0.3174\n",
      "false\t0.1842\n",
      "false\t0.3145\n",
      "false\t0.2707\n",
      "false\t0.1895\n",
      "false\t0.2525\n",
      "false\t0.3141\n",
      "false\t0.2578\n",
      "false\t0.1971\n",
      "false\t0.1479\n",
      "false\t0.1557\n",
      "false\t0.3031\n",
      "false\t0.2886\n",
      "false\t0.2844\n",
      "false\t0.2384\n",
      "false\t0.2449\n",
      "false\t0.2359\n",
      "false\t0.3674\n",
      "false\t0.3915\n",
      "false\t0.3705\n",
      "false\t0.1834\n",
      "false\t0.2296\n",
      "false\t0.1411\n",
      "false\t0.4300\n",
      "false\t0.3662\n",
      "false\t0.2320\n",
      "false\t0.1695\n",
      "false\t0.4495\n",
      "false\t0.2998\n",
      "false\t0.2588\n",
      "false\t0.3046\n",
      "false\t0.3307\n",
      "false\t0.4835\n",
      "false\t0.3622\n",
      "false\t0.3872\n",
      "false\t0.3749\n",
      "false\t0.3703\n",
      "false\t0.2678\n",
      "false\t0.4040\n",
      "false\t0.3501\n",
      "false\t0.2991\n",
      "false\t0.3233\n",
      "false\t0.3088\n",
      "false\t0.3688\n",
      "false\t0.3822\n",
      "false\t0.3051\n",
      "false\t0.3986\n",
      "false\t0.2429\n",
      "false\t0.2882\n",
      "false\t0.3325\n",
      "false\t0.3758\n",
      "false\t0.3676\n",
      "false\t0.4396\n",
      "false\t0.3485\n",
      "false\t0.3445\n",
      "false\t0.2526\n",
      "false\t0.2936\n",
      "false\t0.3124\n",
      "false\t0.3295\n",
      "false\t0.3717\n",
      "false\t0.2913\n",
      "false\t0.2471\n",
      "false\t0.3062\n",
      "false\t0.3301\n",
      "false\t0.3974\n",
      "false\t0.3505\n",
      "false\t0.3401\n",
      "false\t0.3109\n",
      "false\t0.3685\n",
      "false\t0.3496\n",
      "false\t0.4440\n",
      "false\t0.3561\n",
      "false\t0.4081\n",
      "false\t0.3582\n",
      "false\t0.3532\n",
      "false\t0.4137\n",
      "false\t0.3804\n",
      "false\t0.3470\n",
      "false\t0.2853\n",
      "false\t0.2400\n",
      "false\t0.4030\n",
      "false\t0.3815\n",
      "false\t0.3211\n",
      "false\t0.4392\n",
      "false\t0.2812\n",
      "false\t0.3366\n",
      "false\t0.3409\n",
      "false\t0.3304\n",
      "false\t0.3092\n",
      "false\t0.3211\n",
      "false\t0.3164\n",
      "false\t0.3759\n",
      "false\t0.3611\n",
      "false\t0.3186\n",
      "false\t0.3531\n",
      "false\t0.2580\n",
      "false\t0.3322\n",
      "false\t0.3860\n",
      "false\t0.4392\n",
      "false\t0.4051\n",
      "false\t0.3210\n",
      "false\t0.3731\n",
      "false\t0.1867\n",
      "false\t0.3863\n",
      "false\t0.4037\n",
      "false\t0.4287\n",
      "false\t0.3985\n",
      "false\t0.4679\n",
      "false\t0.3582\n",
      "false\t0.3797\n",
      "false\t0.3984\n",
      "false\t0.3233\n",
      "false\t0.3728\n",
      "false\t0.3827\n",
      "false\t0.3996\n",
      "false\t0.2832\n",
      "false\t0.3343\n",
      "false\t0.3551\n",
      "false\t0.3836\n",
      "false\t0.2972\n",
      "false\t0.4229\n",
      "false\t0.4188\n",
      "false\t0.3478\n",
      "false\t0.3751\n",
      "false\t0.3380\n",
      "false\t0.3944\n",
      "false\t0.3190\n",
      "false\t0.2879\n",
      "false\t0.4069\n",
      "false\t0.4436\n",
      "false\t0.3665\n",
      "false\t0.3080\n",
      "false\t0.3750\n",
      "false\t0.2732\n",
      "false\t0.3536\n",
      "false\t0.4046\n",
      "false\t0.4300\n",
      "false\t0.2985\n",
      "false\t0.2576\n",
      "false\t0.2528\n",
      "false\t0.4096\n",
      "false\t0.3400\n",
      "false\t0.3675\n",
      "false\t0.3474\n",
      "false\t0.3563\n",
      "false\t0.2798\n",
      "false\t0.2940\n",
      "false\t0.0625\n",
      "false\t0.1019\n",
      "false\t0.0353\n",
      "false\t0.0811\n",
      "true\t0.5201\n",
      "false\t0.3402\n",
      "false\t0.3100\n",
      "false\t0.3866\n",
      "false\t0.2746\n",
      "false\t0.3130\n",
      "false\t0.3683\n",
      "false\t0.2649\n",
      "false\t0.2216\n",
      "false\t0.3321\n",
      "false\t0.3307\n",
      "false\t0.3601\n",
      "false\t0.3351\n",
      "false\t0.3177\n",
      "false\t0.3192\n",
      "false\t0.4567\n",
      "false\t0.4537\n",
      "false\t0.3949\n",
      "false\t0.2855\n",
      "false\t0.4132\n",
      "false\t0.2905\n",
      "false\t0.2925\n",
      "false\t0.3528\n",
      "false\t0.2368\n",
      "false\t0.2987\n",
      "false\t0.3404\n",
      "false\t0.3421\n",
      "false\t0.2511\n",
      "false\t0.2322\n",
      "false\t0.4214\n",
      "false\t0.2717\n",
      "false\t0.2700\n",
      "false\t0.3923\n",
      "false\t0.2822\n",
      "false\t0.3473\n",
      "false\t0.4044\n",
      "false\t0.3760\n",
      "false\t0.3527\n",
      "false\t0.2220\n",
      "false\t0.3149\n",
      "false\t0.3457\n",
      "false\t0.2426\n",
      "false\t0.3338\n",
      "false\t0.3638\n",
      "false\t0.3332\n",
      "false\t0.3068\n",
      "false\t0.4224\n",
      "false\t0.3929\n",
      "false\t0.3642\n",
      "false\t0.3297\n",
      "false\t0.2687\n",
      "false\t0.3782\n",
      "false\t0.3590\n",
      "false\t0.3682\n",
      "false\t0.3084\n",
      "false\t0.3517\n",
      "false\t0.2641\n",
      "false\t0.2436\n",
      "false\t0.2924\n",
      "false\t0.2890\n",
      "false\t0.3468\n",
      "false\t0.2787\n",
      "false\t0.3953\n",
      "false\t0.3471\n",
      "false\t0.4406\n",
      "false\t0.3990\n",
      "false\t0.4626\n",
      "false\t0.3003\n",
      "false\t0.2603\n",
      "false\t0.2790\n",
      "false\t0.3581\n",
      "false\t0.3591\n",
      "false\t0.3013\n",
      "false\t0.2582\n",
      "false\t0.2794\n",
      "false\t0.3498\n",
      "false\t0.2562\n",
      "false\t0.2627\n",
      "false\t0.3092\n",
      "false\t0.3588\n",
      "false\t0.3849\n",
      "false\t0.3934\n",
      "false\t0.1580\n",
      "false\t0.1871\n",
      "false\t0.2971\n",
      "false\t0.3326\n",
      "false\t0.2085\n",
      "false\t0.4304\n",
      "false\t0.4023\n",
      "false\t0.2011\n",
      "false\t0.3152\n",
      "false\t0.2586\n",
      "false\t0.2676\n",
      "false\t0.3330\n",
      "false\t0.2872\n",
      "false\t0.3646\n",
      "false\t0.2880\n",
      "false\t0.3282\n",
      "false\t0.3358\n",
      "false\t0.2354\n",
      "false\t0.3294\n",
      "false\t0.3571\n",
      "false\t0.2998\n",
      "false\t0.2658\n",
      "false\t0.2793\n",
      "false\t0.3552\n",
      "false\t0.3372\n",
      "false\t0.4156\n",
      "false\t0.2430\n",
      "false\t0.2831\n",
      "false\t0.2676\n",
      "false\t0.2606\n",
      "false\t0.4160\n",
      "false\t0.4211\n",
      "false\t0.3767\n",
      "false\t0.3449\n",
      "false\t0.4302\n",
      "false\t0.4361\n",
      "false\t0.4721\n",
      "false\t0.3653\n",
      "false\t0.3216\n",
      "false\t0.2851\n",
      "false\t0.4064\n",
      "false\t0.3833\n",
      "false\t0.3850\n",
      "false\t0.3707\n",
      "false\t0.3861\n",
      "false\t0.3829\n",
      "false\t0.4164\n",
      "false\t0.4003\n",
      "false\t0.4020\n",
      "false\t0.3280\n",
      "false\t0.3583\n",
      "false\t0.4398\n",
      "false\t0.3644\n",
      "false\t0.2525\n",
      "false\t0.3961\n",
      "false\t0.3282\n",
      "false\t0.2928\n",
      "false\t0.3677\n",
      "false\t0.3685\n",
      "false\t0.3984\n",
      "false\t0.3855\n",
      "false\t0.3204\n",
      "false\t0.3705\n",
      "false\t0.3342\n",
      "false\t0.3308\n",
      "false\t0.3591\n",
      "false\t0.3394\n",
      "false\t0.3349\n",
      "false\t0.1957\n",
      "false\t0.3191\n",
      "false\t0.4395\n",
      "false\t0.3847\n",
      "false\t0.2870\n",
      "false\t0.4226\n",
      "false\t0.4582\n",
      "false\t0.2731\n",
      "false\t0.3418\n",
      "false\t0.3110\n",
      "false\t0.2864\n",
      "false\t0.3277\n",
      "false\t0.3628\n",
      "false\t0.2621\n",
      "false\t0.2410\n",
      "false\t0.3461\n",
      "false\t0.3874\n",
      "false\t0.2339\n",
      "false\t0.2885\n",
      "false\t0.4602\n",
      "false\t0.3475\n",
      "false\t0.2651\n",
      "false\t0.2791\n",
      "false\t0.3055\n",
      "false\t0.2938\n",
      "false\t0.5000\n",
      "true\t0.5146\n",
      "false\t0.4639\n",
      "false\t0.4785\n",
      "true\t0.5190\n",
      "false\t0.3508\n",
      "false\t0.4205\n",
      "false\t0.2568\n",
      "false\t0.3802\n",
      "false\t0.4182\n",
      "false\t0.3183\n",
      "false\t0.3172\n",
      "false\t0.4771\n",
      "false\t0.4594\n",
      "false\t0.4359\n",
      "false\t0.1818\n",
      "false\t0.1604\n",
      "false\t0.3798\n",
      "false\t0.2774\n",
      "false\t0.3049\n",
      "false\t0.3548\n",
      "false\t0.2248\n",
      "false\t0.3020\n",
      "false\t0.4011\n",
      "false\t0.4002\n",
      "false\t0.2818\n",
      "false\t0.3354\n",
      "false\t0.2463\n",
      "false\t0.4352\n",
      "false\t0.4047\n",
      "false\t0.2558\n",
      "true\t0.5344\n",
      "false\t0.2421\n",
      "false\t0.2914\n",
      "false\t0.3945\n",
      "false\t0.4124\n",
      "false\t0.3317\n",
      "false\t0.4016\n",
      "false\t0.4622\n",
      "false\t0.3581\n",
      "false\t0.4042\n",
      "false\t0.3367\n",
      "false\t0.3840\n",
      "false\t0.3882\n",
      "false\t0.3196\n",
      "false\t0.3795\n",
      "false\t0.4242\n",
      "false\t0.4013\n",
      "false\t0.3287\n",
      "false\t0.3000\n",
      "false\t0.3472\n",
      "false\t0.3582\n",
      "false\t0.2765\n",
      "false\t0.3373\n",
      "false\t0.2817\n",
      "false\t0.3137\n",
      "false\t0.3961\n",
      "false\t0.3034\n",
      "false\t0.3641\n",
      "false\t0.3156\n",
      "false\t0.3615\n",
      "false\t0.2274\n",
      "false\t0.2683\n",
      "false\t0.3666\n",
      "false\t0.3807\n",
      "false\t0.4802\n",
      "false\t0.3132\n",
      "false\t0.3995\n",
      "false\t0.4133\n",
      "false\t0.4467\n",
      "false\t0.3216\n",
      "false\t0.3211\n",
      "false\t0.2946\n",
      "false\t0.2782\n",
      "false\t0.2746\n",
      "false\t0.3491\n",
      "false\t0.2475\n",
      "false\t0.3678\n",
      "false\t0.1429\n",
      "false\t0.3326\n",
      "false\t0.3487\n",
      "false\t0.3912\n",
      "false\t0.3508\n",
      "false\t0.3426\n",
      "false\t0.2650\n",
      "false\t0.3792\n",
      "false\t0.3003\n",
      "false\t0.2867\n",
      "false\t0.3784\n",
      "false\t0.4331\n",
      "false\t0.3380\n",
      "false\t0.3965\n",
      "false\t0.4401\n",
      "false\t0.3285\n",
      "false\t0.3279\n",
      "false\t0.3035\n",
      "false\t0.3590\n",
      "false\t0.2674\n",
      "false\t0.2573\n",
      "false\t0.3159\n",
      "false\t0.2713\n",
      "false\t0.3821\n",
      "false\t0.4217\n",
      "false\t0.3719\n",
      "false\t0.3833\n",
      "false\t0.3663\n",
      "false\t0.4209\n",
      "false\t0.2878\n",
      "false\t0.3673\n",
      "false\t0.4081\n",
      "false\t0.3832\n",
      "false\t0.4148\n",
      "false\t0.3407\n",
      "false\t0.2710\n",
      "false\t0.4318\n",
      "false\t0.2879\n",
      "false\t0.1563\n",
      "false\t0.2716\n",
      "false\t0.3658\n",
      "false\t0.1702\n",
      "false\t0.3353\n",
      "false\t0.3607\n",
      "false\t0.3371\n",
      "false\t0.3460\n",
      "false\t0.2085\n",
      "false\t0.3649\n",
      "false\t0.1649\n",
      "false\t0.2440\n",
      "false\t0.1992\n",
      "false\t0.2144\n",
      "false\t0.4124\n",
      "false\t0.2821\n",
      "false\t0.2564\n",
      "false\t0.2401\n",
      "false\t0.3178\n",
      "false\t0.2107\n",
      "false\t0.2051\n",
      "false\t0.1958\n",
      "false\t0.1879\n",
      "false\t0.3090\n",
      "false\t0.3849\n",
      "false\t0.2474\n",
      "false\t0.2799\n",
      "false\t0.2885\n",
      "false\t0.3368\n",
      "false\t0.3142\n",
      "false\t0.3815\n",
      "false\t0.3221\n",
      "false\t0.3100\n",
      "false\t0.2075\n",
      "false\t0.2985\n",
      "false\t0.4055\n",
      "false\t0.2959\n",
      "false\t0.2980\n",
      "false\t0.3636\n",
      "false\t0.4508\n",
      "false\t0.2833\n",
      "false\t0.2715\n",
      "false\t0.2709\n",
      "false\t0.2991\n",
      "false\t0.3299\n",
      "false\t0.3463\n",
      "true\t0.6358\n",
      "false\t0.4649\n",
      "false\t0.4607\n",
      "false\t0.3547\n",
      "false\t0.2891\n",
      "false\t0.3832\n",
      "false\t0.3395\n",
      "false\t0.2021\n",
      "false\t0.4116\n",
      "false\t0.4398\n",
      "false\t0.3989\n",
      "false\t0.4097\n",
      "false\t0.2020\n",
      "false\t0.3161\n",
      "false\t0.2952\n",
      "false\t0.2874\n",
      "false\t0.3226\n",
      "false\t0.3976\n",
      "false\t0.4521\n",
      "false\t0.3785\n",
      "false\t0.2303\n",
      "false\t0.2358\n",
      "false\t0.4371\n",
      "false\t0.2014\n",
      "false\t0.3332\n",
      "false\t0.2819\n",
      "false\t0.2625\n",
      "false\t0.2729\n",
      "false\t0.3895\n",
      "false\t0.3591\n",
      "false\t0.2793\n",
      "false\t0.3375\n",
      "false\t0.2219\n",
      "false\t0.4520\n",
      "false\t0.3922\n",
      "false\t0.4257\n",
      "false\t0.3032\n",
      "false\t0.2707\n",
      "false\t0.2849\n",
      "false\t0.2438\n",
      "false\t0.3127\n",
      "false\t0.3026\n",
      "false\t0.3654\n",
      "false\t0.3235\n",
      "false\t0.3591\n",
      "false\t0.3584\n",
      "false\t0.2580\n",
      "false\t0.3628\n",
      "false\t0.3650\n",
      "false\t0.2591\n",
      "false\t0.3067\n",
      "false\t0.2872\n",
      "false\t0.3774\n",
      "false\t0.4472\n",
      "false\t0.3469\n",
      "false\t0.3295\n",
      "false\t0.3092\n",
      "false\t0.3062\n",
      "false\t0.2436\n",
      "false\t0.3391\n",
      "false\t0.2825\n",
      "false\t0.4019\n",
      "false\t0.3365\n",
      "false\t0.2434\n",
      "false\t0.3038\n",
      "false\t0.2020\n",
      "false\t0.2701\n",
      "false\t0.1593\n",
      "false\t0.2955\n",
      "false\t0.1797\n",
      "false\t0.1182\n",
      "false\t0.2274\n",
      "false\t0.2869\n",
      "false\t0.2784\n",
      "false\t0.2989\n",
      "false\t0.2698\n",
      "false\t0.3182\n",
      "false\t0.3133\n",
      "false\t0.1506\n",
      "false\t0.2424\n",
      "false\t0.4020\n",
      "false\t0.4612\n",
      "false\t0.3820\n",
      "false\t0.4096\n",
      "false\t0.4221\n",
      "false\t0.2953\n",
      "false\t0.2900\n",
      "false\t0.4230\n",
      "false\t0.3423\n",
      "false\t0.4412\n",
      "false\t0.4104\n",
      "false\t0.3603\n",
      "false\t0.2711\n",
      "false\t0.2541\n",
      "false\t0.3846\n",
      "false\t0.3968\n",
      "false\t0.4527\n",
      "false\t0.3945\n",
      "false\t0.3730\n",
      "false\t0.3279\n",
      "false\t0.3842\n",
      "false\t0.3180\n",
      "false\t0.4140\n",
      "false\t0.2516\n",
      "false\t0.4048\n",
      "false\t0.1568\n",
      "false\t0.1796\n",
      "false\t0.2457\n",
      "false\t0.1784\n",
      "false\t0.3157\n",
      "false\t0.3753\n",
      "false\t0.4811\n",
      "false\t0.3944\n",
      "false\t0.4388\n",
      "false\t0.4256\n",
      "false\t0.3014\n",
      "false\t0.4163\n",
      "false\t0.4074\n",
      "false\t0.2913\n",
      "false\t0.3403\n",
      "false\t0.2632\n",
      "false\t0.2275\n",
      "false\t0.4486\n",
      "false\t0.3100\n",
      "false\t0.3446\n",
      "false\t0.2886\n",
      "false\t0.3353\n",
      "false\t0.3338\n",
      "false\t0.3529\n",
      "false\t0.3279\n",
      "false\t0.3503\n",
      "false\t0.3143\n",
      "false\t0.3575\n",
      "false\t0.3936\n",
      "false\t0.3788\n",
      "false\t0.3426\n",
      "false\t0.3089\n",
      "false\t0.4099\n",
      "false\t0.3838\n",
      "false\t0.3544\n",
      "false\t0.4304\n",
      "false\t0.3460\n",
      "false\t0.2920\n",
      "false\t0.4254\n",
      "false\t0.3295\n",
      "false\t0.3137\n",
      "false\t0.3960\n",
      "false\t0.3630\n",
      "false\t0.3102\n",
      "false\t0.2082\n",
      "false\t0.2713\n",
      "false\t0.1679\n",
      "false\t0.3312\n",
      "false\t0.2380\n",
      "false\t0.2155\n",
      "false\t0.2720\n",
      "false\t0.3584\n",
      "false\t0.2815\n",
      "false\t0.3873\n",
      "false\t0.3166\n",
      "false\t0.3778\n",
      "false\t0.4031\n",
      "false\t0.3109\n",
      "false\t0.1699\n",
      "false\t0.2641\n",
      "false\t0.1701\n",
      "false\t0.0898\n",
      "false\t0.3595\n",
      "false\t0.3200\n",
      "false\t0.4312\n",
      "false\t0.2967\n",
      "false\t0.3329\n",
      "false\t0.3829\n",
      "false\t0.3028\n",
      "false\t0.3703\n",
      "false\t0.3542\n",
      "false\t0.3648\n",
      "false\t0.3801\n",
      "false\t0.3411\n",
      "false\t0.3766\n",
      "false\t0.3858\n",
      "false\t0.3477\n",
      "false\t0.3790\n",
      "false\t0.3837\n",
      "false\t0.4259\n",
      "false\t0.3430\n",
      "false\t0.3727\n",
      "false\t0.3368\n",
      "false\t0.3695\n",
      "false\t0.4049\n",
      "false\t0.2455\n",
      "false\t0.1594\n",
      "false\t0.1437\n",
      "false\t0.1121\n",
      "false\t0.2355\n",
      "false\t0.0945\n",
      "false\t0.2146\n",
      "false\t0.3113\n",
      "false\t0.1728\n",
      "false\t0.2288\n",
      "false\t0.1936\n",
      "false\t0.2635\n",
      "false\t0.2140\n",
      "false\t0.3475\n",
      "false\t0.2127\n",
      "false\t0.2520\n",
      "false\t0.2729\n",
      "false\t0.1138\n",
      "false\t0.1396\n",
      "false\t0.0743\n",
      "false\t0.1817\n",
      "false\t0.1170\n",
      "false\t0.1910\n",
      "false\t0.3606\n",
      "false\t0.2104\n",
      "false\t0.1333\n",
      "false\t0.1953\n",
      "false\t0.0886\n",
      "false\t0.0918\n",
      "false\t0.2471\n"
     ]
    }
   ],
   "source": [
    "for prediction in predictions:\n",
    "    label = np.argmax(prediction)\n",
    "    if prediction[1] > 0.5:\n",
    "        print('true\\t' + \"{0:.4f}\".format(prediction[1]))\n",
    "    else:\n",
    "        print('false\\t' + \"{0:.4f}\".format(prediction[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
