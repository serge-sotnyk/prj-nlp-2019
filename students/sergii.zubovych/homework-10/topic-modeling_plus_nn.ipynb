{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Спочатку завантажуємо word embeddings для української мови."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://lang.org.ua/static/downloads/models/news.lowercased.tokenized.word2vec.300d.bz2 --output news.lowercased.tokenized.word2vec.300d.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bunzip2 news.lowercased.tokenized.word2vec.300d.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.3 s, sys: 517 ms, total: 56.8 s\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "%time wv = KeyedVectors.load_word2vec_format('news.lowercased.tokenized.word2vec.300d', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('дієслово', 0.6502863764762878),\n",
       " ('слівце', 0.6484909653663635),\n",
       " ('словосполучення', 0.6456568241119385),\n",
       " ('гасло', 0.5913079977035522),\n",
       " ('слово**', 0.555127739906311),\n",
       " (\"прислів'я\", 0.5407627820968628),\n",
       " ('письмо', 0.5235773324966431),\n",
       " ('прізвище', 0.52119380235672),\n",
       " ('пророцтво', 0.5125285983085632),\n",
       " ('ремесло', 0.5058826804161072)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('слово')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потім розпаковуємо та завантажуємо дані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace 1551/Інші-Подяки.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip -q ../../../tasks/1551.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аварійний--травмонебезпечний-стан-утримання-об-єктів-благоустрою.txt\r\n",
      "Бажаючі-отримати--Картки-киянина--КК--.txt\r\n",
      "Будівництво-АЗС.txt\r\n",
      "Будівництво-в-нічний-час.txt\r\n",
      "Будівництво-дооблаштування-дитячого-майданчику.txt\r\n",
      "Будівництво--дооблаштування-спортивних-майданчиків.txt\r\n",
      "Будівництво-та-реконструкція-об-єктів-освіти.txt\r\n",
      "Взаємовідносини-з-сусідами.txt\r\n",
      "Вивезення--утилізація-твердих-та-негабаритних-відходів.txt\r\n",
      "Видалення-аварійних--пошкоджених-хворобами-дерев.txt\r\n",
      "Видача-розрахункових-книжок--квитанцій--довідок.txt\r\n",
      "Вилов-безпритульних-тварин.txt\r\n",
      "Вирізування--кронування--гілля-дерев.txt\r\n",
      "Виток-холодної-води-на-поверхню.txt\r\n",
      "Відновлення-благоустрою-після-вик--планових-аварійних-робіт-на-об-єктах-благоуст.txt\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls 1551 | head -n 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1551/Незадовільна-температура-ГВП.txt',\n",
       " '1551/Незадовільне-обслуговування-в-амбулаторно-поліклінічних-установах.txt',\n",
       " '1551/Відсутнє-електропостачання.txt',\n",
       " '1551/Порушення-правил-тиші--після-------.txt',\n",
       " '1551/Неякісне-ХВП.txt',\n",
       " '1551/Нанесення-дорожньої-розмітки.txt',\n",
       " '1551/Робота-циркуляційної-системи.txt',\n",
       " '1551/Встановлення-світлофора.txt',\n",
       " '1551/Завезення-піску-на-дитячий-майданчик.txt',\n",
       " '1551/Скошування-трави.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"1551/*\")\n",
    "\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "Document = namedtuple('Document', 'id topic_id tags content')\n",
    "\n",
    "def parse_tags(file):\n",
    "    return [tag for tag in os.path.basename(file.name)[:-4].split('-') if tag]\n",
    "\n",
    "def parse_topic_file(topic_id, filename):\n",
    "    documents = []    \n",
    "    with open(filename) as f:\n",
    "        tags = parse_tags(f)        \n",
    "        _id = None\n",
    "        idx = -1        \n",
    "        for line in f:            \n",
    "            if line.strip().isnumeric():\n",
    "                _id = int(line.strip())\n",
    "                documents.append(Document(_id, topic_id, tags, []))\n",
    "                idx +=1\n",
    "                continue\n",
    "            if not (_id is None) and line.strip():                \n",
    "                documents[idx].content.append(line.strip())                \n",
    "    \n",
    "    return [doc._replace(content = ''.join(doc.content)) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114348"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents = [doc for topic_id, file in enumerate(files) \\\n",
    "                 for doc in parse_topic_file(topic_id, file) if len(doc.content) > 0]\n",
    "\n",
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=2697865, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Недогрев горячей воды (вода нормальной температуры подавалась неделю с15 по 23, до этого была частичная подача горячей воды (пару часов вечером и ночью горячая), остальное время теплой), сейчас опять температура порядка 40 градусов. Эта ситуация продолжается на фоне постоянного недогрева батарей в квартире, ДУ 12 ККЕУ  МОУкраины  не реагирует на ситуацию.'),\n",
       " Document(id=3170827, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Из горячего крана течет холодная вода, в вечернее и утреннее время купаться нет возможности. Необходимо или пересчитывать тарифы или включать горячую воду.'),\n",
       " Document(id=3165270, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Я поживаю на 6 этаже 9и - этажного дома на протяжении долгого промежутка времени у нас в квартире из крана горячей воды, особенно утром и в первой половине дня течёт если не холодная вода, то вода чуть тёплая. По утрам для того чтобы совершить утренний туалет приходится долго спускать воду (и эта проблема у большей части жильцов нашего дома). В свете того, что с мая месяца у нас очень выросли тарифы на горячую воду, меня интересует вопрос - почему я должна платить деньги за некачественную услугу. Огромная просьба посодействовать в решении данной проблемы. Местные сантехники подтверждают, что проблемы с горячей водой не только в нашей квартире, но решить эту проблему они не могут, так как это от них не зависит.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер фільтруємо документи з українською мовою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d163aaebef743ae9d116af264e96d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=114348), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.detector import LangDetectException\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def memoize(filename, compute):  \n",
    "    \n",
    "    fullname = filename + '.can'\n",
    "    \n",
    "    if os.path.isfile(fullname):\n",
    "        with open(fullname, 'rb') as f:                        \n",
    "            return pickle.load(f)\n",
    "    \n",
    "    result = compute()\n",
    "    with open(fullname, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def is_uk(text):\n",
    "    \n",
    "    if len(text):\n",
    "        try:\n",
    "            return detect(text[:1024]) == 'uk'\n",
    "        except LangDetectException as e:\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "uk_documents = memoize('uk_documents', \n",
    "                       lambda: [doc for doc in tqdm_notebook(all_documents) if is_uk(doc.content)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дивимся на дані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "uk_doc_df = pandas.DataFrame([doc._replace(tags = \"-\".join(doc.tags)) for doc in uk_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3152784</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Відсутнітність горячого водопостачання належно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3143050</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Добрий вечір.Прошу розібратися з проблемою нев...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3142427</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>На моє звернення № Г-6623 відповідь написав ди...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3130991</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Доброго дня! Вже більше двох тижнів гаряче вод...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2405990</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>На звернення:Номер звернення:\\tГ-6478Зареєстро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3115494</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Звертаюсь до Вас стосовно вирішення питання, щ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3104107</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Доброго дня!!! Моє звернення від 02.12.14 #Г-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3091571</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Протягом останнього тижня гаряча вода недостат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2690156</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Прошу прийняти необхідні заходи по покращенню ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2748419</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>немає  температури гарячої води</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  topic_id                          tags  \\\n",
       "0  3152784         0  Незадовільна-температура-ГВП   \n",
       "1  3143050         0  Незадовільна-температура-ГВП   \n",
       "2  3142427         0  Незадовільна-температура-ГВП   \n",
       "3  3130991         0  Незадовільна-температура-ГВП   \n",
       "4  2405990         0  Незадовільна-температура-ГВП   \n",
       "5  3115494         0  Незадовільна-температура-ГВП   \n",
       "6  3104107         0  Незадовільна-температура-ГВП   \n",
       "7  3091571         0  Незадовільна-температура-ГВП   \n",
       "8  2690156         0  Незадовільна-температура-ГВП   \n",
       "9  2748419         0  Незадовільна-температура-ГВП   \n",
       "\n",
       "                                             content  \n",
       "0  Відсутнітність горячого водопостачання належно...  \n",
       "1  Добрий вечір.Прошу розібратися з проблемою нев...  \n",
       "2  На моє звернення № Г-6623 відповідь написав ди...  \n",
       "3  Доброго дня! Вже більше двох тижнів гаряче вод...  \n",
       "4  На звернення:Номер звернення:\\tГ-6478Зареєстро...  \n",
       "5  Звертаюсь до Вас стосовно вирішення питання, щ...  \n",
       "6  Доброго дня!!! Моє звернення від 02.12.14 #Г-1...  \n",
       "7  Протягом останнього тижня гаряча вода недостат...  \n",
       "8  Прошу прийняти необхідні заходи по покращенню ...  \n",
       "9                    немає  температури гарячої води  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.182900e+04</td>\n",
       "      <td>61829.000000</td>\n",
       "      <td>61829</td>\n",
       "      <td>61829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188</td>\n",
       "      <td>56061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Відсутність-ГВП</td>\n",
       "      <td>Відсутнє гаряче водопостачання</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6564</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.159625e+06</td>\n",
       "      <td>105.551731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.084360e+07</td>\n",
       "      <td>55.922291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.841555e+06</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.083712e+06</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.245460e+06</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.013102e+09</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id      topic_id             tags  \\\n",
       "count   6.182900e+04  61829.000000            61829   \n",
       "unique           NaN           NaN              188   \n",
       "top              NaN           NaN  Відсутність-ГВП   \n",
       "freq             NaN           NaN             6564   \n",
       "mean    3.159625e+06    105.551731              NaN   \n",
       "std     1.084360e+07     55.922291              NaN   \n",
       "min     1.000000e+01      0.000000              NaN   \n",
       "25%     2.841555e+06     58.000000              NaN   \n",
       "50%     3.083712e+06    121.000000              NaN   \n",
       "75%     3.245460e+06    150.000000              NaN   \n",
       "max     2.013102e+09    187.000000              NaN   \n",
       "\n",
       "                               content  \n",
       "count                            61829  \n",
       "unique                           56061  \n",
       "top     Відсутнє гаряче водопостачання  \n",
       "freq                                46  \n",
       "mean                               NaN  \n",
       "std                                NaN  \n",
       "min                                NaN  \n",
       "25%                                NaN  \n",
       "50%                                NaN  \n",
       "75%                                NaN  \n",
       "max                                NaN  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <th>Відсутність-ГВП</th>\n",
       "      <td>6564</td>\n",
       "      <td>6564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <th>Укладання-та-ремонт-асфальтного-покриття</th>\n",
       "      <td>3628</td>\n",
       "      <td>3628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>Відсутність-опалення</th>\n",
       "      <td>3142</td>\n",
       "      <td>3142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <th>Перевірка-дозвільної-документації-демонтаж-кіосків-ларків</th>\n",
       "      <td>2199</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <th>Прибирання-та-санітарний-стан-територій</th>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <th>Технічний-стан-проїжджих-частин-вулиць-та-тротуарів</th>\n",
       "      <td>1303</td>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <th>Відновлення-благоустрою-після-вик-планових-аварійних-робіт-на-об-єктах-благоуст</th>\n",
       "      <td>1289</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <th>Відсутність-освітлення-у-під-їзді-за-відсутності-несправності-лампочок</th>\n",
       "      <td>1256</td>\n",
       "      <td>1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>Не-працює-пасажирський-ліфт</th>\n",
       "      <td>1220</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <th>Ремонт-під-їзду</th>\n",
       "      <td>1198</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>Незадовільна-температура-ГВП</th>\n",
       "      <td>1116</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <th>Перерахунок-та-нарахування-плати-за-інші-види-житлово-комунальних-послуг</th>\n",
       "      <td>1097</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <th>Про-розгляд-звернень-громадян</th>\n",
       "      <td>993</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <th>Відсутність-опалення-по-стояку</th>\n",
       "      <td>989</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <th>ГЛ-Несанкціонована-торгівля</th>\n",
       "      <td>826</td>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <th>Прибирання-приміщень</th>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <th>Відсутнє-ХВП</th>\n",
       "      <td>785</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <th>Освітлення-в-приміщенні-й-при-вході-в-нього</th>\n",
       "      <td>743</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <th>Інші-технічні-недоліки-стану-ліфту</th>\n",
       "      <td>695</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <th>Ремонт-дахів</th>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <th>Перевірка-наявності-дозволів-на-виконання-будівельних-робіт</th>\n",
       "      <td>646</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <th>Незадовільна-температура-опалення</th>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>Будівництво-дооблаштування-дитячого-майданчику</th>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <th>Утримання-підвалів-колясочних-технічних-поверхів</th>\n",
       "      <td>617</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <th>Відсутність-освітлення-на-опорних-стовпах-за-відсутності-несправності-лампочок</th>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <th>Питання-освітлення-на-опорних-стовпах</th>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <th>Встановлення-та-експлуатація-лічильників-на-водопостачання</th>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>Робота-світлофора</th>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <th>Стихійне-сміттєзвалище</th>\n",
       "      <td>477</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>Робота-циркуляційної-системи</th>\n",
       "      <td>473</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <th>Технічне-обслуговування-систем-тепло-водопостачання-та-водовідведення-і-зливов</th>\n",
       "      <td>458</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <th>Демонтаж-рекламних-конструкцій-і-вивісок</th>\n",
       "      <td>458</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <th>Скління-та-ремонт-вікон-на-сходових-клітинах</th>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>Встановлення-лічильників-на-опалення</th>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>Вологе-прибирання-приміщень</th>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <th>Незадовільний-вивіз-сміття-з-контейнерів-та-урн-для-сміття</th>\n",
       "      <td>383</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <th>Встановлення-та-експлуатація-дорожніх-знаків</th>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Нанесення-дорожньої-розмітки</th>\n",
       "      <td>353</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <th>Паркування-авто-у-місцях-загального-користування</th>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <th>Встановлення-сміттєвих-контейнерів-та-урн-для-сміття</th>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <th>Видалення-аварійних-пошкоджених-хворобами-дерев</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <th>Контроль-за-станом-рекламних-засобів</th>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>Перевірка-дозвільної-документації-демонтаж-літніх-майданчиків-кафе-ресторанів</th>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <th>Інші-Подяки</th>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <th>Встановлення-огородження-зеленої-зони</th>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <th>Знищення-омели-амброзії-та-рослин-паразитів</th>\n",
       "      <td>283</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>Аварійний-травмонебезпечний-стан-утримання-об-єктів-благоустрою</th>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <th>Не-працює-вантажний-ліфт</th>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <th>Встановлення-сигнальних-стовпчиків-бар-єрних-огороджень-бордюрів</th>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <th>Ремонт-і-обслуговування-сміттєпроводів-та-сміттєзбірників-в-приміщенні</th>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               id  content\n",
       "topic_id tags                                                             \n",
       "138      Відсутність-ГВП                                     6564     6564\n",
       "180      Укладання-та-ремонт-асфальтного-покриття            3628     3628\n",
       "27       Відсутність-опалення                                3142     3142\n",
       "127      Перевірка-дозвільної-документації-демонтаж-кіос...  2199     2199\n",
       "67       Прибирання-та-санітарний-стан-територій             2005     2005\n",
       "79       Технічний-стан-проїжджих-частин-вулиць-та-троту...  1303     1303\n",
       "155      Відновлення-благоустрою-після-вик-планових-авар...  1289     1289\n",
       "121      Відсутність-освітлення-у-під-їзді-за-відсутност...  1256     1256\n",
       "58       Не-працює-пасажирський-ліфт                         1220     1220\n",
       "183      Ремонт-під-їзду                                     1198     1198\n",
       "0        Незадовільна-температура-ГВП                        1116     1116\n",
       "101      Перерахунок-та-нарахування-плати-за-інші-види-ж...  1097     1097\n",
       "143      Про-розгляд-звернень-громадян                        993      993\n",
       "88       Відсутність-опалення-по-стояку                       989      989\n",
       "173      ГЛ-Несанкціонована-торгівля                          826      826\n",
       "161      Прибирання-приміщень                                 800      800\n",
       "176      Відсутнє-ХВП                                         785      785\n",
       "171      Освітлення-в-приміщенні-й-при-вході-в-нього          743      743\n",
       "148      Інші-технічні-недоліки-стану-ліфту                   695      695\n",
       "178      Ремонт-дахів                                         671      671\n",
       "22       Перевірка-наявності-дозволів-на-виконання-будів...   646      646\n",
       "174      Незадовільна-температура-опалення                    635      635\n",
       "68       Будівництво-дооблаштування-дитячого-майданчику       633      633\n",
       "181      Утримання-підвалів-колясочних-технічних-поверхів     617      617\n",
       "164      Відсутність-освітлення-на-опорних-стовпах-за-ві...   538      538\n",
       "109      Питання-освітлення-на-опорних-стовпах                516      516\n",
       "154      Встановлення-та-експлуатація-лічильників-на-вод...   481      481\n",
       "10       Робота-світлофора                                    480      480\n",
       "41       Стихійне-сміттєзвалище                               477      477\n",
       "6        Робота-циркуляційної-системи                         473      473\n",
       "131      Технічне-обслуговування-систем-тепло-водопостач...   458      458\n",
       "114      Демонтаж-рекламних-конструкцій-і-вивісок             458      458\n",
       "116      Скління-та-ремонт-вікон-на-сходових-клітинах         431      431\n",
       "33       Встановлення-лічильників-на-опалення                 430      430\n",
       "16       Вологе-прибирання-приміщень                          407      407\n",
       "40       Незадовільний-вивіз-сміття-з-контейнерів-та-урн...   383      383\n",
       "57       Встановлення-та-експлуатація-дорожніх-знаків         359      359\n",
       "5        Нанесення-дорожньої-розмітки                         353      353\n",
       "69       Паркування-авто-у-місцях-загального-користування     351      351\n",
       "78       Встановлення-сміттєвих-контейнерів-та-урн-для-с...   348      348\n",
       "136      Видалення-аварійних-пошкоджених-хворобами-дерев      321      321\n",
       "62       Контроль-за-станом-рекламних-засобів                 318      318\n",
       "32       Перевірка-дозвільної-документації-демонтаж-літн...   316      316\n",
       "168      Інші-Подяки                                          307      307\n",
       "105      Встановлення-огородження-зеленої-зони                289      289\n",
       "74       Знищення-омели-амброзії-та-рослин-паразитів          283      283\n",
       "14       Аварійний-травмонебезпечний-стан-утримання-об-є...   280      280\n",
       "145      Не-працює-вантажний-ліфт                             275      275\n",
       "128      Встановлення-сигнальних-стовпчиків-бар-єрних-ог...   273      273\n",
       "112      Ремонт-і-обслуговування-сміттєпроводів-та-смітт...   265      265"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.groupby(['topic_id', 'tags']).count().sort_values(['id'], ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виділяємо лейбли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61818"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic_labels = np.array([doc.topic_id for doc in uk_documents])\n",
    "len(topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "І розбиваємо дані на тренувальні і тестові."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_documents, test_documents, train_topic_labels, test_topic_labels = \\\n",
    "    train_test_split(uk_documents, topic_labels, random_state = 26, test_size = 0.3, stratify = topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43272\n",
      "43272\n"
     ]
    }
   ],
   "source": [
    "print(len(train_documents))\n",
    "print(len(train_topic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18546\n",
      "18546\n"
     ]
    }
   ],
   "source": [
    "print(len(test_documents))\n",
    "print(len(test_topic_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Будуємо бейзлайн, знаходимо суму векторів слів по кожному документу і використовуємо kNN на знайденних векторах. Для порівняння векторів застосовуємо cosine similarity. Перед знаходженням суми векторів, документ токенізується та видаляються stop words. Знайдені вектори нормалізуються, в такому випадку eclidean distance для kNN має той самий ефект що й cosine distance, при цьому алгоритм дозволяє використовувати більш ефективні структури данних, такі як, наприклад, k-d tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize_uk\n",
    "import string\n",
    "import html\n",
    "import re\n",
    "\n",
    "with open('uk_stop_words.txt') as f:\n",
    "    STOP_WORDS = f.read().split()\n",
    "    \n",
    "EXT_PUNCTUATION = \"”...«»№\"\n",
    "\n",
    "def contain_numbers(s):\n",
    "    return bool(re.search(r'\\d', s))\n",
    "\n",
    "def non_stop_word(word):\n",
    "    return not (word in string.punctuation or word in EXT_PUNCTUATION \\\n",
    "                or word in STOP_WORDS or contain_numbers(word) or len(word) < 2)\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if non_stop_word(token.lower())]\n",
    "\n",
    "def tokenize_doc(doc):\n",
    "    return [word.lower() for word in \\\n",
    "            remove_stop_words(tokenize_uk.tokenize_words(html.unescape(doc.content)))]\n",
    "\n",
    "def normalize_vec(x):\n",
    "    m = np.max(x)\n",
    "    if m > 0.0:\n",
    "        return x/np.sqrt(np.dot(x,x))\n",
    "    return x\n",
    "    \n",
    "def doc_to_sum_vec(doc):\n",
    "    words = tokenize_doc(doc)    \n",
    "    vec = np.zeros(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec += wv[word]\n",
    "        except KeyError as e:            \n",
    "            pass\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рахуємо вектори для тренувальних і тестових документів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3588d664134bfbae947fc7e2392e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_doc_sum_vecs = np.array([doc_to_sum_vec(doc) for doc in tqdm_notebook(train_documents)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999cfa4d7833461a87f9fd284b9144ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_doc_sum_vecs = np.array([doc_to_sum_vec(doc) for doc in tqdm_notebook(test_documents)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN+sum vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренуємо модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels):\n",
    "        self.train_vectors = train_vectors\n",
    "        self.train_labels = train_labels\n",
    "        self.test_vectors = test_vectors\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.fit(self.train_vectors, self.train_labels)\n",
    "        self.topics_predicted = self.model.predict(self.test_vectors)\n",
    "        \n",
    "    def test(self):\n",
    "        self.test_report = classification_report(self.test_labels, self.topics_predicted)\n",
    "        print(classification_report(self.test_labels, self.topics_predicted))  \n",
    "\n",
    "class KnnModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels, n = 1):\n",
    "        super().__init__(np.array([normalize_vec(doc) for doc in train_vectors]),\\\n",
    "                       train_labels,\\\n",
    "                       np.array([normalize_vec(doc) for doc in test_vectors]),\\\n",
    "                       test_labels)                \n",
    "        self.model = KNeighborsClassifier(n_neighbors = n, algorithm='kd_tree', metric = 'euclidean', n_jobs = 6)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KnnModel(train_doc_sum_vecs, train_topic_labels, test_doc_sum_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 14s, sys: 282 ms, total: 11min 15s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%time knn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.55      0.50       335\n",
      "           1       0.77      0.80      0.79        41\n",
      "           2       0.55      0.30      0.39        79\n",
      "           3       0.38      0.28      0.32        39\n",
      "           4       0.88      0.32      0.47        22\n",
      "           5       0.42      0.30      0.35       106\n",
      "           6       0.22      0.32      0.26       142\n",
      "           7       0.25      0.32      0.28        62\n",
      "           8       0.81      0.43      0.57        30\n",
      "           9       0.40      0.36      0.38        22\n",
      "          10       0.57      0.53      0.55       144\n",
      "          11       0.21      0.17      0.19        29\n",
      "          12       0.44      0.36      0.40        22\n",
      "          13       0.33      0.31      0.32        45\n",
      "          14       0.23      0.18      0.20        84\n",
      "          15       0.40      0.25      0.31        16\n",
      "          16       0.51      0.48      0.49       122\n",
      "          17       0.25      0.19      0.21        16\n",
      "          18       0.29      0.28      0.29        53\n",
      "          19       0.36      0.26      0.30        19\n",
      "          20       0.83      0.31      0.45        16\n",
      "          21       0.21      0.25      0.23        24\n",
      "          22       0.46      0.43      0.45       194\n",
      "          23       0.81      0.49      0.61        51\n",
      "          24       0.46      0.40      0.43        47\n",
      "          25       0.29      0.32      0.31        28\n",
      "          26       0.50      0.24      0.32        25\n",
      "          27       0.55      0.63      0.59       943\n",
      "          28       0.31      0.30      0.31        56\n",
      "          29       0.25      0.33      0.29        63\n",
      "          30       0.23      0.16      0.19        44\n",
      "          31       0.25      0.04      0.07        25\n",
      "          32       0.53      0.55      0.54        95\n",
      "          33       0.54      0.59      0.56       129\n",
      "          34       0.40      0.47      0.43        45\n",
      "          35       0.21      0.20      0.21        35\n",
      "          36       0.24      0.22      0.23        50\n",
      "          37       0.33      0.23      0.27        40\n",
      "          38       0.53      0.41      0.46        39\n",
      "          39       0.41      0.34      0.37        44\n",
      "          40       0.55      0.57      0.56       115\n",
      "          41       0.41      0.36      0.38       143\n",
      "          42       0.57      0.65      0.61        62\n",
      "          43       0.30      0.15      0.20        41\n",
      "          44       0.19      0.21      0.20        29\n",
      "          45       0.32      0.25      0.28        71\n",
      "          46       0.51      0.43      0.47        56\n",
      "          47       0.22      0.19      0.21        21\n",
      "          48       0.11      0.12      0.12        16\n",
      "          49       0.42      0.26      0.32        19\n",
      "          50       0.36      0.20      0.26        44\n",
      "          51       0.44      0.20      0.28        20\n",
      "          52       0.13      0.22      0.16        27\n",
      "          53       0.44      0.47      0.45        53\n",
      "          54       0.34      0.56      0.43        18\n",
      "          55       0.17      0.23      0.20        26\n",
      "          56       0.56      0.32      0.41        28\n",
      "          57       0.32      0.38      0.35       108\n",
      "          58       0.55      0.52      0.54       366\n",
      "          59       0.23      0.15      0.18        74\n",
      "          60       0.30      0.29      0.30        34\n",
      "          61       0.23      0.29      0.25        24\n",
      "          62       0.45      0.39      0.42        95\n",
      "          63       0.17      0.11      0.13        18\n",
      "          64       0.67      0.54      0.60        37\n",
      "          65       0.62      0.33      0.43        15\n",
      "          66       0.28      0.30      0.29        33\n",
      "          67       0.51      0.58      0.54       602\n",
      "          68       0.49      0.61      0.54       190\n",
      "          69       0.27      0.31      0.29       105\n",
      "          70       0.55      0.44      0.49        36\n",
      "          71       0.43      0.27      0.33        48\n",
      "          72       0.27      0.18      0.22        22\n",
      "          73       0.23      0.27      0.25        33\n",
      "          74       0.79      0.76      0.78        85\n",
      "          75       0.52      0.53      0.52        30\n",
      "          76       0.61      0.46      0.52        50\n",
      "          77       0.32      0.21      0.25        43\n",
      "          78       0.41      0.37      0.39       104\n",
      "          79       0.39      0.42      0.40       391\n",
      "          80       0.20      0.06      0.09        17\n",
      "          81       0.67      0.64      0.65        28\n",
      "          82       0.24      0.30      0.27        20\n",
      "          83       0.23      0.12      0.16        24\n",
      "          84       0.37      0.42      0.39        53\n",
      "          85       0.57      0.35      0.43        23\n",
      "          86       0.40      0.23      0.29        74\n",
      "          87       0.24      0.35      0.29        34\n",
      "          88       0.40      0.38      0.39       297\n",
      "          89       0.55      0.45      0.49        40\n",
      "          90       0.30      0.19      0.23        16\n",
      "          91       0.33      0.26      0.29        39\n",
      "          92       0.32      0.35      0.33        26\n",
      "          93       0.81      0.59      0.68        22\n",
      "          94       0.29      0.30      0.29        20\n",
      "          95       0.25      0.07      0.11        42\n",
      "          96       0.27      0.14      0.19        21\n",
      "          97       0.65      0.46      0.54        24\n",
      "          98       0.35      0.36      0.35        25\n",
      "          99       0.50      0.41      0.45        34\n",
      "         100       0.22      0.40      0.29        55\n",
      "         101       0.50      0.57      0.53       329\n",
      "         102       0.67      0.43      0.52        75\n",
      "         103       0.46      0.32      0.37        19\n",
      "         104       0.81      0.58      0.68        38\n",
      "         105       0.20      0.15      0.17        87\n",
      "         106       0.16      0.30      0.21        47\n",
      "         107       0.69      0.28      0.40        32\n",
      "         108       0.78      0.67      0.72        21\n",
      "         109       0.38      0.37      0.37       155\n",
      "         110       0.28      0.30      0.29        66\n",
      "         111       0.50      0.65      0.57        20\n",
      "         112       0.19      0.19      0.19        80\n",
      "         113       0.38      0.45      0.41        56\n",
      "         114       0.43      0.42      0.43       137\n",
      "         115       0.25      0.18      0.21        17\n",
      "         116       0.46      0.49      0.47       129\n",
      "         117       0.20      0.15      0.17        78\n",
      "         118       0.30      0.28      0.29        39\n",
      "         119       0.33      0.30      0.31        47\n",
      "         120       0.26      0.43      0.32        14\n",
      "         121       0.48      0.47      0.48       377\n",
      "         122       0.56      0.61      0.58        38\n",
      "         123       0.24      0.33      0.28        78\n",
      "         124       0.19      0.16      0.17        25\n",
      "         125       0.34      0.40      0.37        73\n",
      "         126       0.79      0.75      0.77        20\n",
      "         127       0.53      0.52      0.53       660\n",
      "         128       0.20      0.29      0.24        82\n",
      "         129       0.53      0.50      0.51        20\n",
      "         130       0.12      0.06      0.08        18\n",
      "         131       0.23      0.25      0.24       137\n",
      "         132       0.26      0.23      0.24        43\n",
      "         133       0.35      0.24      0.28        55\n",
      "         134       0.52      0.44      0.47        64\n",
      "         135       0.34      0.28      0.31        57\n",
      "         136       0.44      0.36      0.40        96\n",
      "         137       0.31      0.21      0.25        61\n",
      "         138       0.69      0.73      0.71      1969\n",
      "         139       0.44      0.35      0.39        54\n",
      "         140       0.25      0.19      0.21        16\n",
      "         141       0.38      0.40      0.39        25\n",
      "         142       0.18      0.24      0.21        17\n",
      "         143       0.37      0.35      0.36       298\n",
      "         144       0.14      0.10      0.11        41\n",
      "         145       0.46      0.52      0.49        83\n",
      "         146       0.56      0.40      0.47        25\n",
      "         147       0.30      0.12      0.17        25\n",
      "         148       0.34      0.39      0.36       209\n",
      "         149       0.46      0.40      0.43        15\n",
      "         150       0.30      0.36      0.33        61\n",
      "         151       0.27      0.30      0.29        20\n",
      "         152       0.42      0.25      0.31        20\n",
      "         153       0.47      0.24      0.31        34\n",
      "         154       0.52      0.52      0.52       144\n",
      "         155       0.42      0.43      0.42       387\n",
      "         156       0.24      0.21      0.23        56\n",
      "         157       0.18      0.12      0.15        24\n",
      "         158       0.40      0.37      0.39        51\n",
      "         159       0.57      0.35      0.43        23\n",
      "         160       0.27      0.27      0.27        44\n",
      "         161       0.48      0.43      0.45       240\n",
      "         162       0.44      0.26      0.33        27\n",
      "         163       0.29      0.26      0.28        19\n",
      "         164       0.33      0.34      0.34       161\n",
      "         165       0.56      0.30      0.39        66\n",
      "         166       0.40      0.33      0.36        70\n",
      "         167       0.55      0.43      0.48        61\n",
      "         168       0.46      0.17      0.25        92\n",
      "         169       0.39      0.53      0.45        34\n",
      "         170       0.19      0.16      0.18        37\n",
      "         171       0.36      0.39      0.37       223\n",
      "         172       0.20      0.08      0.12        24\n",
      "         173       0.62      0.58      0.60       248\n",
      "         174       0.45      0.46      0.45       191\n",
      "         175       0.20      0.21      0.20        53\n",
      "         176       0.42      0.32      0.36       236\n",
      "         177       0.32      0.28      0.30        39\n",
      "         178       0.37      0.48      0.42       201\n",
      "         179       0.12      0.16      0.14        31\n",
      "         180       0.51      0.57      0.54      1088\n",
      "         181       0.37      0.36      0.37       185\n",
      "         182       0.30      0.25      0.27        28\n",
      "         183       0.41      0.45      0.43       359\n",
      "         184       0.52      0.68      0.59        19\n",
      "         185       0.16      0.14      0.15        35\n",
      "         186       0.47      0.23      0.31        39\n",
      "         187       0.42      0.40      0.41        25\n",
      "\n",
      "   micro avg       0.46      0.46      0.46     18549\n",
      "   macro avg       0.40      0.35      0.36     18549\n",
      "weighted avg       0.46      0.46      0.46     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imrovements\n",
    "\n",
    "Намагаємося покращити результат. Спочатку будемо використовувати логістичну регресію, потім проробимо все те саме але з векторами Doc2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logreg+sum vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class LogregModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels, iters = 3000):\n",
    "        super().__init__(train_vectors, train_labels, test_vectors, test_labels)\n",
    "        self.model = LogisticRegression(random_state=26, n_jobs = 6, solver=\"lbfgs\", \\\n",
    "                                        multi_class=\"multinomial\", max_iter = iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogregModel(train_doc_sum_vecs, train_topic_labels, test_doc_sum_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 407 ms, sys: 308 ms, total: 715 ms\n",
      "Wall time: 27min 32s\n"
     ]
    }
   ],
   "source": [
    "%time logreg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84        28\n",
      "           1       0.34      0.20      0.25        87\n",
      "           2       0.47      0.52      0.49        83\n",
      "           3       0.71      0.62      0.67        40\n",
      "           4       0.44      0.37      0.40       185\n",
      "           5       0.43      0.48      0.45        25\n",
      "           6       0.62      0.64      0.63        50\n",
      "           7       0.22      0.30      0.25        44\n",
      "           8       0.29      0.27      0.28        22\n",
      "           9       0.85      0.77      0.81        22\n",
      "          10       0.06      0.08      0.07        25\n",
      "          11       0.64      0.45      0.53        20\n",
      "          12       0.51      0.64      0.57        53\n",
      "          13       0.19      0.31      0.24        16\n",
      "          14       0.18      0.25      0.21        24\n",
      "          15       0.44      0.37      0.40        19\n",
      "          16       0.46      0.49      0.48       137\n",
      "          17       0.18      0.22      0.20        18\n",
      "          18       0.38      0.34      0.36       105\n",
      "          19       0.16      0.26      0.20        19\n",
      "          20       0.24      0.36      0.29        33\n",
      "          21       0.31      0.38      0.34        48\n",
      "          22       0.26      0.24      0.25        45\n",
      "          23       0.42      0.57      0.48        23\n",
      "          24       0.52      0.51      0.51        96\n",
      "          25       0.07      0.06      0.07        47\n",
      "          26       0.57      0.47      0.51        43\n",
      "          27       0.17      0.21      0.19        29\n",
      "          28       0.69      0.48      0.56        23\n",
      "          29       0.65      0.57      0.61        30\n",
      "          30       0.66      0.66      0.66      1089\n",
      "          31       0.53      0.58      0.55       377\n",
      "          32       0.31      0.31      0.31        74\n",
      "          33       0.46      0.43      0.45       104\n",
      "          34       0.42      0.34      0.38        32\n",
      "          35       0.25      0.41      0.31        56\n",
      "          36       0.08      0.10      0.09        40\n",
      "          37       0.27      0.31      0.29        55\n",
      "          38       0.23      0.22      0.22        74\n",
      "          39       0.32      0.41      0.36        27\n",
      "          40       0.37      0.49      0.42        47\n",
      "          41       0.30      0.44      0.35        39\n",
      "          42       0.74      0.61      0.67       249\n",
      "          43       0.48      0.50      0.49        24\n",
      "          44       0.38      0.56      0.45        57\n",
      "          45       0.68      0.59      0.63       201\n",
      "          46       0.20      0.31      0.24        29\n",
      "          47       0.58      0.63      0.61       106\n",
      "          48       0.26      0.33      0.29        52\n",
      "          49       0.35      0.38      0.36        16\n",
      "          50       0.62      0.72      0.67        36\n",
      "          51       0.62      0.70      0.65       145\n",
      "          52       0.07      0.06      0.07        16\n",
      "          53       0.19      0.25      0.22        20\n",
      "          54       0.53      0.37      0.43       391\n",
      "          55       0.37      0.27      0.31       224\n",
      "          56       0.58      0.53      0.56       360\n",
      "          57       0.72      0.72      0.72        85\n",
      "          58       0.25      0.23      0.24        13\n",
      "          59       0.18      0.24      0.20        55\n",
      "          60       0.45      0.56      0.50        70\n",
      "          61       0.14      0.22      0.17        18\n",
      "          62       0.37      0.46      0.41        28\n",
      "          63       0.53      0.59      0.56        56\n",
      "          64       0.40      0.52      0.45        62\n",
      "          65       0.45      0.59      0.51        79\n",
      "          66       0.07      0.12      0.09        25\n",
      "          67       0.39      0.60      0.47        65\n",
      "          68       0.21      0.38      0.27        42\n",
      "          69       0.49      0.44      0.46       143\n",
      "          70       0.48      0.63      0.55       122\n",
      "          71       0.12      0.17      0.14        35\n",
      "          72       0.41      0.37      0.39       161\n",
      "          73       0.40      0.52      0.45        33\n",
      "          74       0.81      0.67      0.73        39\n",
      "          75       0.70      0.63      0.66       658\n",
      "          76       0.26      0.33      0.29        57\n",
      "          77       0.58      0.55      0.57        38\n",
      "          78       0.29      0.29      0.29        14\n",
      "          79       0.38      0.40      0.39        95\n",
      "          80       0.11      0.20      0.15        20\n",
      "          81       0.12      0.09      0.10        78\n",
      "          82       0.26      0.30      0.28        20\n",
      "          83       0.21      0.33      0.26        39\n",
      "          84       0.25      0.28      0.27        53\n",
      "          85       0.58      0.60      0.59        25\n",
      "          86       0.44      0.56      0.49        61\n",
      "          87       0.16      0.23      0.19        44\n",
      "          88       0.60      0.49      0.54       328\n",
      "          89       0.44      0.37      0.40        19\n",
      "          90       0.27      0.29      0.28        34\n",
      "          91       0.20      0.19      0.20        83\n",
      "          92       0.34      0.39      0.36        64\n",
      "          93       0.28      0.44      0.34        25\n",
      "          94       0.09      0.13      0.11        30\n",
      "          95       0.57      0.53      0.55       129\n",
      "          96       0.30      0.21      0.25       142\n",
      "          97       0.79      0.62      0.70        37\n",
      "          98       0.65      0.54      0.59        24\n",
      "          99       0.32      0.39      0.35        70\n",
      "         100       0.27      0.20      0.23       298\n",
      "         101       0.35      0.47      0.40        17\n",
      "         102       0.89      0.85      0.87        20\n",
      "         103       0.62      0.58      0.60       191\n",
      "         104       0.44      0.22      0.30        18\n",
      "         105       0.23      0.35      0.28        23\n",
      "         106       0.20      0.26      0.23        34\n",
      "         107       0.44      0.37      0.40       209\n",
      "         108       0.43      0.38      0.40        16\n",
      "         109       0.41      0.35      0.38        26\n",
      "         110       0.59      0.43      0.50       297\n",
      "         111       0.30      0.36      0.33        25\n",
      "         112       0.50      0.40      0.44        20\n",
      "         113       0.18      0.23      0.20        53\n",
      "         114       0.18      0.22      0.20        41\n",
      "         115       0.67      0.32      0.43        25\n",
      "         116       0.14      0.24      0.17        25\n",
      "         117       0.29      0.32      0.30        19\n",
      "         118       0.56      0.67      0.61        75\n",
      "         119       0.26      0.17      0.20       137\n",
      "         120       0.38      0.34      0.35       155\n",
      "         121       0.39      0.63      0.48        54\n",
      "         122       0.52      0.52      0.52        61\n",
      "         123       0.82      0.71      0.76        38\n",
      "         124       0.65      0.57      0.61        61\n",
      "         125       0.29      0.40      0.34        25\n",
      "         126       0.15      0.31      0.20        16\n",
      "         127       0.53      0.48      0.50       194\n",
      "         128       0.08      0.15      0.10        26\n",
      "         129       0.59      0.63      0.61       144\n",
      "         130       0.62      0.56      0.59       334\n",
      "         131       0.57      0.64      0.60       116\n",
      "         132       0.29      0.38      0.33        53\n",
      "         133       0.36      0.38      0.37        79\n",
      "         134       0.64      0.62      0.63       365\n",
      "         135       0.18      0.34      0.24        35\n",
      "         136       0.52      0.67      0.58        95\n",
      "         137       0.32      0.44      0.37        41\n",
      "         138       0.16      0.26      0.20        34\n",
      "         139       0.27      0.40      0.32        20\n",
      "         140       0.67      0.29      0.40        21\n",
      "         141       0.56      0.74      0.64        19\n",
      "         142       0.25      0.41      0.31        34\n",
      "         143       0.25      0.35      0.29        37\n",
      "         144       0.71      0.74      0.72       945\n",
      "         145       0.69      0.67      0.68        30\n",
      "         146       0.34      0.44      0.38        45\n",
      "         147       0.62      0.33      0.43        15\n",
      "         148       0.42      0.52      0.46        61\n",
      "         149       0.31      0.30      0.31        92\n",
      "         150       0.82      0.76      0.78        41\n",
      "         151       0.14      0.19      0.16        21\n",
      "         152       0.52      0.48      0.50       240\n",
      "         153       0.57      0.57      0.57        47\n",
      "         154       0.31      0.31      0.31        16\n",
      "         155       0.22      0.22      0.22        23\n",
      "         156       0.58      0.44      0.50       236\n",
      "         157       0.54      0.62      0.58        34\n",
      "         158       0.78      0.78      0.78      1969\n",
      "         159       0.54      0.50      0.52       108\n",
      "         160       0.37      0.35      0.36        63\n",
      "         161       0.38      0.35      0.36        66\n",
      "         162       0.82      0.50      0.62        28\n",
      "         163       0.05      0.04      0.04        84\n",
      "         164       0.21      0.28      0.24        39\n",
      "         165       0.17      0.21      0.19        42\n",
      "         166       0.29      0.29      0.29        17\n",
      "         167       0.59      0.47      0.52       190\n",
      "         168       0.18      0.18      0.18        17\n",
      "         169       0.23      0.38      0.29        39\n",
      "         170       0.60      0.27      0.37        22\n",
      "         171       0.37      0.47      0.41        45\n",
      "         172       0.79      0.52      0.63        21\n",
      "         173       0.27      0.36      0.31        50\n",
      "         174       0.24      0.32      0.27        73\n",
      "         175       0.60      0.62      0.61       130\n",
      "         176       0.19      0.29      0.23        24\n",
      "         177       0.22      0.35      0.27        31\n",
      "         178       0.82      0.71      0.76        51\n",
      "         179       0.17      0.21      0.19        56\n",
      "         180       0.38      0.45      0.41        20\n",
      "         181       0.52      0.44      0.47        39\n",
      "         182       0.60      0.56      0.58       600\n",
      "         183       0.62      0.46      0.53       387\n",
      "         184       0.36      0.33      0.34        79\n",
      "         185       0.37      0.64      0.47        22\n",
      "         186       0.22      0.34      0.27        44\n",
      "         187       0.62      0.48      0.54        27\n",
      "\n",
      "   micro avg       0.52      0.52      0.52     18552\n",
      "   macro avg       0.41      0.42      0.41     18552\n",
      "weighted avg       0.53      0.52      0.52     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Є невелике покращення в якості."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN+Doc2Vec\n",
    "\n",
    "Переходимо до Doc2Vec. Для цього використовуємо gensim. Спочатку конвертуємо наші документи в модель gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "def to_tagged_doc(doc):\n",
    "    words = tokenize_doc(doc)\n",
    "    return TaggedDocument(words, [doc.topic_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['заявляю', 'чергове', 'втручання', 'діяльність', 'інформаційної', 'системи', 'колцентру', 'метою', 'викривлення', 'відомостей', 'стосовно', 'результатів', 'вирішення', 'поданих', 'звернень', 'даний', 'відмітку', 'виконано', 'наводжу', 'витяг', 'березня', 'перегляд', 'інтерактивній', 'картівідсутність', 'гвп', 'відповідальний', 'кп', 'печерська', 'брама', 'мазурчак', 'олександр', 'володимирович', 'дата', 'контролю', 'березня', 'статус', 'виконано', 'відповідаю', 'дійсності', 'оскільки', 'годин', 'офіційно', 'отриманий', 'електронний', 'запит', 'виклав', 'текст', 'протилежного', 'змісту', 'наводжу', 'заявника', 'квітня', 'статус', 'виконано', 'квартира', 'розташована', 'му', 'під’їзді', 'відношення', 'перекриття', 'стояка', 'го', 'заміна', 'вентиля', 'сусідів', 'потребує', 'перекриття', 'водопостачання', 'будинку', 'викличе', 'появу', 'трубах', 'будинку', 'стільки', 'бруду', 'зливався', 'понад', 'хвилини', 'хвилину', 'вигаданих', 'нормативів', 'кму', 'скарги', 'взагалі', 'подавались', 'заради', 'отримання', 'пустих', 'роз’яснень', 'усунення', 'проблеми', 'рахунок', 'винуватців', 'поборами', 'див', 'договір', 'викликає', 'подив', 'подане', 'звернення', 'зареєстроване', 'оскільки', 'начебто', 'другу', 'добу', 'перевіряється', 'оператором', '???', 'наведене', 'криміналом', 'корупційними', 'діяннями', '!!!'], tags=[0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tagged_doc(uk_documents[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35c0edb995548939ae5287fddc6a8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_tagged_docs = [to_tagged_doc(doc) for doc in tqdm_notebook(train_documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb612684da72496d83d50b652a2dd500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_tagged_docs = [to_tagged_doc(doc) for doc in tqdm_notebook(test_documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потім тренуємо PV-DBOW модель. Розмір вектору документа 300, як і в моделі з embeddins, яку ми використовували в бейзлайні."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "dbow_model = Doc2Vec(dm=0, vector_size=300, min_count=5, window=10, workers=6, epochs=120)\n",
    "\n",
    "dbow_model.build_vocab(train_tagged_docs + test_tagged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 12s, sys: 45.7 s, total: 12min 58s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%time dbow_model.train(train_tagged_docs, total_examples=dbow_model.corpus_count, epochs=dbow_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Збираємо вектори документів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8171c55ac98406790a7b649a338bb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_doc_vecs = np.array([dbow_model.infer_vector(doc.words) for doc in tqdm_notebook(train_tagged_docs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb30f0558bc437684a37f50dc2e55a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_doc_vecs = np.array([dbow_model.infer_vector(doc.words) for doc in tqdm_notebook(test_tagged_docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знову намагаємося застосувати kNN та логістичну регресію на отриманних векторах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KnnModel(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 29s, sys: 3.79 s, total: 12min 32s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%time knn2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82        28\n",
      "           1       0.37      0.36      0.36        87\n",
      "           2       0.40      0.65      0.50        83\n",
      "           3       0.69      0.78      0.73        40\n",
      "           4       0.51      0.35      0.42       185\n",
      "           5       0.42      0.56      0.48        25\n",
      "           6       0.53      0.62      0.57        50\n",
      "           7       0.52      0.57      0.54        44\n",
      "           8       0.42      0.23      0.29        22\n",
      "           9       0.83      0.86      0.84        22\n",
      "          10       0.33      0.16      0.22        25\n",
      "          11       0.85      0.55      0.67        20\n",
      "          12       0.51      0.55      0.53        53\n",
      "          13       0.27      0.38      0.32        16\n",
      "          14       0.35      0.25      0.29        24\n",
      "          15       0.70      0.37      0.48        19\n",
      "          16       0.49      0.59      0.54       137\n",
      "          17       0.33      0.50      0.40        18\n",
      "          18       0.34      0.33      0.34       105\n",
      "          19       0.44      0.58      0.50        19\n",
      "          20       0.55      0.33      0.42        33\n",
      "          21       0.58      0.44      0.50        48\n",
      "          22       0.70      0.58      0.63        45\n",
      "          23       0.57      0.70      0.63        23\n",
      "          24       0.57      0.64      0.60        96\n",
      "          25       0.32      0.26      0.29        47\n",
      "          26       0.49      0.79      0.60        43\n",
      "          27       0.42      0.52      0.46        29\n",
      "          28       0.59      0.43      0.50        23\n",
      "          29       0.37      0.83      0.51        30\n",
      "          30       0.65      0.69      0.67      1089\n",
      "          31       0.57      0.67      0.61       377\n",
      "          32       0.51      0.34      0.41        74\n",
      "          33       0.62      0.46      0.53       104\n",
      "          34       0.83      0.47      0.60        32\n",
      "          35       0.58      0.66      0.62        56\n",
      "          36       0.54      0.17      0.26        40\n",
      "          37       0.48      0.42      0.45        55\n",
      "          38       0.40      0.36      0.38        74\n",
      "          39       0.50      0.37      0.43        27\n",
      "          40       0.37      0.55      0.44        47\n",
      "          41       0.63      0.56      0.59        39\n",
      "          42       0.66      0.68      0.67       249\n",
      "          43       0.45      0.62      0.53        24\n",
      "          44       0.58      0.54      0.56        57\n",
      "          45       0.72      0.72      0.72       201\n",
      "          46       0.60      0.21      0.31        29\n",
      "          47       0.49      0.69      0.57       106\n",
      "          48       0.49      0.50      0.50        52\n",
      "          49       0.41      0.56      0.47        16\n",
      "          50       0.87      0.75      0.81        36\n",
      "          51       0.83      0.87      0.85       145\n",
      "          52       0.18      0.12      0.15        16\n",
      "          53       0.38      0.30      0.33        20\n",
      "          54       0.47      0.46      0.47       391\n",
      "          55       0.42      0.47      0.45       224\n",
      "          56       0.63      0.53      0.58       360\n",
      "          57       0.79      0.87      0.83        85\n",
      "          58       0.19      0.46      0.27        13\n",
      "          59       0.57      0.38      0.46        55\n",
      "          60       0.57      0.34      0.43        70\n",
      "          61       0.25      0.22      0.24        18\n",
      "          62       0.60      0.64      0.62        28\n",
      "          63       0.75      0.79      0.77        56\n",
      "          64       0.59      0.47      0.52        62\n",
      "          65       0.48      0.52      0.50        79\n",
      "          66       0.11      0.20      0.14        25\n",
      "          67       0.56      0.54      0.55        65\n",
      "          68       0.25      0.36      0.30        42\n",
      "          69       0.46      0.53      0.49       143\n",
      "          70       0.52      0.68      0.59       122\n",
      "          71       0.33      0.29      0.31        35\n",
      "          72       0.55      0.47      0.51       161\n",
      "          73       0.49      0.55      0.51        33\n",
      "          74       0.61      0.56      0.59        39\n",
      "          75       0.75      0.70      0.73       658\n",
      "          76       0.44      0.35      0.39        57\n",
      "          77       0.64      0.66      0.65        38\n",
      "          78       0.50      0.50      0.50        14\n",
      "          79       0.44      0.41      0.42        95\n",
      "          80       0.11      0.25      0.15        20\n",
      "          81       0.41      0.23      0.30        78\n",
      "          82       0.58      0.55      0.56        20\n",
      "          83       0.32      0.31      0.32        39\n",
      "          84       0.51      0.42      0.46        53\n",
      "          85       0.50      0.64      0.56        25\n",
      "          86       0.60      0.75      0.67        61\n",
      "          87       0.31      0.25      0.28        44\n",
      "          88       0.69      0.53      0.60       328\n",
      "          89       0.58      0.37      0.45        19\n",
      "          90       0.33      0.24      0.28        34\n",
      "          91       0.48      0.42      0.45        83\n",
      "          92       0.52      0.48      0.50        64\n",
      "          93       0.52      0.56      0.54        25\n",
      "          94       0.24      0.20      0.22        30\n",
      "          95       0.62      0.62      0.62       129\n",
      "          96       0.42      0.29      0.34       142\n",
      "          97       0.61      0.84      0.70        37\n",
      "          98       0.50      0.54      0.52        24\n",
      "          99       0.45      0.50      0.47        70\n",
      "         100       0.57      0.34      0.42       298\n",
      "         101       0.55      0.35      0.43        17\n",
      "         102       0.61      0.95      0.75        20\n",
      "         103       0.64      0.55      0.59       191\n",
      "         104       0.54      0.39      0.45        18\n",
      "         105       0.52      0.48      0.50        23\n",
      "         106       0.42      0.65      0.51        34\n",
      "         107       0.49      0.31      0.38       209\n",
      "         108       0.24      0.25      0.24        16\n",
      "         109       0.45      0.50      0.47        26\n",
      "         110       0.56      0.37      0.45       297\n",
      "         111       0.35      0.36      0.35        25\n",
      "         112       0.57      0.65      0.60        20\n",
      "         113       0.44      0.23      0.30        53\n",
      "         114       0.23      0.32      0.27        41\n",
      "         115       0.50      0.32      0.39        25\n",
      "         116       0.43      0.60      0.50        25\n",
      "         117       0.38      0.47      0.42        19\n",
      "         118       0.86      0.96      0.91        75\n",
      "         119       0.40      0.23      0.29       137\n",
      "         120       0.52      0.48      0.50       155\n",
      "         121       0.46      0.61      0.53        54\n",
      "         122       0.57      0.62      0.59        61\n",
      "         123       0.70      0.87      0.78        38\n",
      "         124       0.72      0.84      0.77        61\n",
      "         125       0.43      0.24      0.31        25\n",
      "         126       0.20      0.12      0.15        16\n",
      "         127       0.72      0.51      0.60       194\n",
      "         128       0.21      0.15      0.18        26\n",
      "         129       0.68      0.71      0.69       144\n",
      "         130       0.53      0.64      0.58       334\n",
      "         131       0.52      0.75      0.61       116\n",
      "         132       0.36      0.32      0.34        53\n",
      "         133       0.60      0.52      0.56        79\n",
      "         134       0.65      0.77      0.70       365\n",
      "         135       0.75      0.34      0.47        35\n",
      "         136       0.74      0.82      0.78        95\n",
      "         137       0.50      0.63      0.56        41\n",
      "         138       0.42      0.29      0.34        34\n",
      "         139       0.69      0.45      0.55        20\n",
      "         140       0.74      0.67      0.70        21\n",
      "         141       0.50      0.84      0.63        19\n",
      "         142       0.30      0.32      0.31        34\n",
      "         143       0.56      0.62      0.59        37\n",
      "         144       0.65      0.81      0.72       945\n",
      "         145       0.68      0.90      0.77        30\n",
      "         146       0.62      0.71      0.66        45\n",
      "         147       0.60      0.40      0.48        15\n",
      "         148       0.58      0.34      0.43        61\n",
      "         149       0.23      0.21      0.22        92\n",
      "         150       0.74      0.85      0.80        41\n",
      "         151       0.18      0.24      0.20        21\n",
      "         152       0.58      0.57      0.58       240\n",
      "         153       0.54      0.55      0.55        47\n",
      "         154       0.56      0.31      0.40        16\n",
      "         155       0.27      0.26      0.27        23\n",
      "         156       0.60      0.61      0.60       236\n",
      "         157       0.51      0.76      0.61        34\n",
      "         158       0.78      0.84      0.81      1969\n",
      "         159       0.64      0.60      0.62       108\n",
      "         160       0.59      0.37      0.45        63\n",
      "         161       0.54      0.30      0.39        66\n",
      "         162       0.87      0.71      0.78        28\n",
      "         163       0.35      0.15      0.21        84\n",
      "         164       0.47      0.38      0.42        39\n",
      "         165       0.15      0.21      0.18        42\n",
      "         166       0.35      0.41      0.38        17\n",
      "         167       0.55      0.52      0.53       190\n",
      "         168       0.42      0.29      0.34        17\n",
      "         169       0.67      0.26      0.37        39\n",
      "         170       0.60      0.55      0.57        22\n",
      "         171       0.45      0.49      0.47        45\n",
      "         172       0.50      0.52      0.51        21\n",
      "         173       0.44      0.32      0.37        50\n",
      "         174       0.47      0.27      0.34        73\n",
      "         175       0.71      0.65      0.68       130\n",
      "         176       0.57      0.33      0.42        24\n",
      "         177       0.31      0.35      0.33        31\n",
      "         178       0.62      0.88      0.73        51\n",
      "         179       0.27      0.27      0.27        56\n",
      "         180       0.89      0.40      0.55        20\n",
      "         181       0.43      0.49      0.46        39\n",
      "         182       0.55      0.68      0.61       600\n",
      "         183       0.65      0.49      0.55       387\n",
      "         184       0.30      0.32      0.31        79\n",
      "         185       0.44      0.82      0.57        22\n",
      "         186       0.44      0.27      0.34        44\n",
      "         187       0.47      0.33      0.39        27\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     18552\n",
      "   macro avg       0.51      0.50      0.49     18552\n",
      "weighted avg       0.59      0.59      0.58     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn2.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logreg+Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = LogregModel(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 423 ms, total: 827 ms\n",
      "Wall time: 5min 5s\n"
     ]
    }
   ],
   "source": [
    "%time logreg2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.50      0.64        28\n",
      "           1       0.32      0.33      0.33        87\n",
      "           2       0.55      0.57      0.56        83\n",
      "           3       0.83      0.62      0.71        40\n",
      "           4       0.44      0.39      0.41       185\n",
      "           5       0.64      0.56      0.60        25\n",
      "           6       0.55      0.58      0.56        50\n",
      "           7       0.48      0.45      0.47        44\n",
      "           8       0.56      0.23      0.32        22\n",
      "           9       0.94      0.77      0.85        22\n",
      "          10       0.44      0.16      0.24        25\n",
      "          11       0.65      0.55      0.59        20\n",
      "          12       0.54      0.51      0.52        53\n",
      "          13       0.50      0.19      0.27        16\n",
      "          14       0.64      0.29      0.40        24\n",
      "          15       0.80      0.42      0.55        19\n",
      "          16       0.54      0.55      0.54       137\n",
      "          17       0.50      0.50      0.50        18\n",
      "          18       0.36      0.42      0.39       105\n",
      "          19       0.57      0.42      0.48        19\n",
      "          20       0.43      0.30      0.36        33\n",
      "          21       0.65      0.54      0.59        48\n",
      "          22       0.56      0.40      0.47        45\n",
      "          23       0.71      0.43      0.54        23\n",
      "          24       0.64      0.61      0.63        96\n",
      "          25       0.32      0.23      0.27        47\n",
      "          26       0.70      0.72      0.71        43\n",
      "          27       0.49      0.59      0.53        29\n",
      "          28       0.75      0.39      0.51        23\n",
      "          29       0.62      0.70      0.66        30\n",
      "          30       0.63      0.74      0.68      1089\n",
      "          31       0.58      0.62      0.59       377\n",
      "          32       0.50      0.45      0.47        74\n",
      "          33       0.59      0.49      0.54       104\n",
      "          34       0.82      0.44      0.57        32\n",
      "          35       0.65      0.57      0.61        56\n",
      "          36       0.35      0.17      0.23        40\n",
      "          37       0.50      0.36      0.42        55\n",
      "          38       0.50      0.49      0.49        74\n",
      "          39       0.62      0.37      0.47        27\n",
      "          40       0.50      0.60      0.54        47\n",
      "          41       0.59      0.56      0.58        39\n",
      "          42       0.71      0.64      0.67       249\n",
      "          43       0.41      0.62      0.49        24\n",
      "          44       0.62      0.46      0.53        57\n",
      "          45       0.70      0.73      0.71       201\n",
      "          46       0.83      0.17      0.29        29\n",
      "          47       0.70      0.61      0.65       106\n",
      "          48       0.40      0.40      0.40        52\n",
      "          49       0.53      0.56      0.55        16\n",
      "          50       0.83      0.53      0.64        36\n",
      "          51       0.83      0.88      0.85       145\n",
      "          52       0.29      0.12      0.17        16\n",
      "          53       0.38      0.25      0.30        20\n",
      "          54       0.47      0.49      0.48       391\n",
      "          55       0.38      0.42      0.40       224\n",
      "          56       0.54      0.61      0.57       360\n",
      "          57       0.88      0.84      0.86        85\n",
      "          58       0.50      0.15      0.24        13\n",
      "          59       0.44      0.38      0.41        55\n",
      "          60       0.52      0.44      0.48        70\n",
      "          61       0.14      0.11      0.12        18\n",
      "          62       0.60      0.54      0.57        28\n",
      "          63       0.95      0.70      0.80        56\n",
      "          64       0.78      0.45      0.57        62\n",
      "          65       0.60      0.61      0.60        79\n",
      "          66       0.17      0.16      0.17        25\n",
      "          67       0.64      0.57      0.60        65\n",
      "          68       0.28      0.31      0.30        42\n",
      "          69       0.47      0.52      0.49       143\n",
      "          70       0.57      0.70      0.62       122\n",
      "          71       0.35      0.20      0.25        35\n",
      "          72       0.49      0.50      0.49       161\n",
      "          73       0.57      0.52      0.54        33\n",
      "          74       0.73      0.69      0.71        39\n",
      "          75       0.67      0.78      0.72       658\n",
      "          76       0.40      0.44      0.42        57\n",
      "          77       0.82      0.61      0.70        38\n",
      "          78       0.71      0.36      0.48        14\n",
      "          79       0.51      0.53      0.52        95\n",
      "          80       0.12      0.15      0.14        20\n",
      "          81       0.32      0.17      0.22        78\n",
      "          82       0.65      0.55      0.59        20\n",
      "          83       0.38      0.28      0.32        39\n",
      "          84       0.44      0.21      0.28        53\n",
      "          85       0.67      0.40      0.50        25\n",
      "          86       0.65      0.77      0.71        61\n",
      "          87       0.30      0.25      0.27        44\n",
      "          88       0.52      0.66      0.58       328\n",
      "          89       0.67      0.32      0.43        19\n",
      "          90       0.54      0.21      0.30        34\n",
      "          91       0.43      0.43      0.43        83\n",
      "          92       0.50      0.52      0.51        64\n",
      "          93       0.60      0.48      0.53        25\n",
      "          94       0.26      0.17      0.20        30\n",
      "          95       0.68      0.61      0.64       129\n",
      "          96       0.35      0.37      0.36       142\n",
      "          97       0.60      0.81      0.69        37\n",
      "          98       0.50      0.46      0.48        24\n",
      "          99       0.48      0.39      0.43        70\n",
      "         100       0.29      0.44      0.35       298\n",
      "         101       0.67      0.24      0.35        17\n",
      "         102       0.89      0.80      0.84        20\n",
      "         103       0.66      0.66      0.66       191\n",
      "         104       1.00      0.44      0.62        18\n",
      "         105       0.56      0.39      0.46        23\n",
      "         106       0.51      0.62      0.56        34\n",
      "         107       0.47      0.52      0.49       209\n",
      "         108       0.17      0.06      0.09        16\n",
      "         109       0.56      0.38      0.45        26\n",
      "         110       0.51      0.51      0.51       297\n",
      "         111       0.47      0.28      0.35        25\n",
      "         112       0.71      0.60      0.65        20\n",
      "         113       0.37      0.21      0.27        53\n",
      "         114       0.20      0.22      0.21        41\n",
      "         115       0.73      0.32      0.44        25\n",
      "         116       0.50      0.40      0.44        25\n",
      "         117       0.53      0.53      0.53        19\n",
      "         118       0.90      0.95      0.92        75\n",
      "         119       0.31      0.26      0.28       137\n",
      "         120       0.49      0.55      0.52       155\n",
      "         121       0.49      0.70      0.58        54\n",
      "         122       0.55      0.61      0.58        61\n",
      "         123       0.94      0.82      0.87        38\n",
      "         124       0.76      0.74      0.75        61\n",
      "         125       0.58      0.28      0.38        25\n",
      "         126       0.50      0.12      0.20        16\n",
      "         127       0.67      0.56      0.61       194\n",
      "         128       0.44      0.15      0.23        26\n",
      "         129       0.68      0.67      0.68       144\n",
      "         130       0.55      0.58      0.56       334\n",
      "         131       0.60      0.66      0.63       116\n",
      "         132       0.44      0.38      0.41        53\n",
      "         133       0.59      0.63      0.61        79\n",
      "         134       0.72      0.73      0.72       365\n",
      "         135       0.69      0.31      0.43        35\n",
      "         136       0.76      0.84      0.80        95\n",
      "         137       0.55      0.56      0.55        41\n",
      "         138       0.88      0.21      0.33        34\n",
      "         139       0.83      0.25      0.38        20\n",
      "         140       0.68      0.62      0.65        21\n",
      "         141       0.71      0.53      0.61        19\n",
      "         142       0.33      0.38      0.35        34\n",
      "         143       0.78      0.68      0.72        37\n",
      "         144       0.69      0.71      0.70       945\n",
      "         145       0.74      0.87      0.80        30\n",
      "         146       0.74      0.56      0.63        45\n",
      "         147       0.62      0.33      0.43        15\n",
      "         148       0.65      0.46      0.54        61\n",
      "         149       0.24      0.35      0.28        92\n",
      "         150       0.93      0.68      0.79        41\n",
      "         151       0.46      0.29      0.35        21\n",
      "         152       0.59      0.56      0.57       240\n",
      "         153       0.55      0.57      0.56        47\n",
      "         154       0.83      0.31      0.45        16\n",
      "         155       0.29      0.17      0.22        23\n",
      "         156       0.63      0.53      0.58       236\n",
      "         157       0.62      0.82      0.71        34\n",
      "         158       0.77      0.86      0.81      1969\n",
      "         159       0.58      0.64      0.61       108\n",
      "         160       0.51      0.40      0.45        63\n",
      "         161       0.59      0.33      0.43        66\n",
      "         162       0.93      0.46      0.62        28\n",
      "         163       0.33      0.21      0.26        84\n",
      "         164       0.43      0.41      0.42        39\n",
      "         165       0.14      0.24      0.18        42\n",
      "         166       0.42      0.29      0.34        17\n",
      "         167       0.50      0.50      0.50       190\n",
      "         168       0.50      0.24      0.32        17\n",
      "         169       0.37      0.26      0.30        39\n",
      "         170       0.57      0.18      0.28        22\n",
      "         171       0.57      0.56      0.56        45\n",
      "         172       0.78      0.33      0.47        21\n",
      "         173       0.39      0.30      0.34        50\n",
      "         174       0.47      0.29      0.36        73\n",
      "         175       0.75      0.61      0.67       130\n",
      "         176       0.35      0.25      0.29        24\n",
      "         177       0.32      0.32      0.32        31\n",
      "         178       0.93      0.80      0.86        51\n",
      "         179       0.49      0.41      0.45        56\n",
      "         180       0.82      0.45      0.58        20\n",
      "         181       0.46      0.31      0.37        39\n",
      "         182       0.63      0.66      0.64       600\n",
      "         183       0.55      0.58      0.56       387\n",
      "         184       0.44      0.46      0.45        79\n",
      "         185       0.41      0.68      0.51        22\n",
      "         186       0.50      0.27      0.35        44\n",
      "         187       0.86      0.22      0.35        27\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     18552\n",
      "   macro avg       0.57      0.47      0.50     18552\n",
      "weighted avg       0.59      0.59      0.59     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg2.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо що Doc2Vec дав значне покращення у якості."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Approach\n",
    "\n",
    "Спробуємо використати нейронні мережі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN+Doc2Vec\n",
    "\n",
    "Починаємо з Feed Forward мережі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' #CNN doesn't work on my PC on GPU due to libraries incompatibility, comment to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "class NnModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels):\n",
    "        super().__init__(train_vectors, \n",
    "                         to_categorical(train_labels), \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "    \n",
    "    def train(self, epochs = 10):\n",
    "        self.history = self.model.fit(self.train_vectors, self.train_labels,\n",
    "                                      epochs = epochs, batch_size=128, \n",
    "                                      verbose=1, validation_split=0.1)\n",
    "        self.topics_predicted = np.argmax(self.model.predict(self.test_vectors), axis=-1)\n",
    "    \n",
    "\n",
    "class FeedForwardNN(NnModel):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels,\n",
    "                 input_size, hidden_size):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)               \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hidden_size, activation='relu', input_shape=(input_size,)))        \n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 188)               192700    \n",
      "=================================================================\n",
      "Total params: 500,924\n",
      "Trainable params: 500,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_model = FeedForwardNN(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels, 300, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38952 samples, validate on 4328 samples\n",
      "Epoch 1/20\n",
      "38952/38952 [==============================] - 2s 61us/step - loss: 3.2943 - acc: 0.3832 - val_loss: 2.0738 - val_acc: 0.6250\n",
      "Epoch 2/20\n",
      "38952/38952 [==============================] - 2s 54us/step - loss: 1.4696 - acc: 0.7314 - val_loss: 1.0555 - val_acc: 0.7962\n",
      "Epoch 3/20\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 0.8142 - acc: 0.8426 - val_loss: 0.6825 - val_acc: 0.8646\n",
      "Epoch 4/20\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 0.5574 - acc: 0.8852 - val_loss: 0.5254 - val_acc: 0.8879\n",
      "Epoch 5/20\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 0.4344 - acc: 0.9039 - val_loss: 0.4395 - val_acc: 0.8974\n",
      "Epoch 6/20\n",
      "38952/38952 [==============================] - 2s 50us/step - loss: 0.3629 - acc: 0.9173 - val_loss: 0.3870 - val_acc: 0.9009\n",
      "Epoch 7/20\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 0.3165 - acc: 0.9261 - val_loss: 0.3518 - val_acc: 0.9060\n",
      "Epoch 8/20\n",
      "38952/38952 [==============================] - 2s 57us/step - loss: 0.2826 - acc: 0.9319 - val_loss: 0.3306 - val_acc: 0.9087\n",
      "Epoch 9/20\n",
      "38952/38952 [==============================] - 2s 57us/step - loss: 0.2577 - acc: 0.9358 - val_loss: 0.3139 - val_acc: 0.9104\n",
      "Epoch 10/20\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 0.2378 - acc: 0.9393 - val_loss: 0.3004 - val_acc: 0.9131\n",
      "Epoch 11/20\n",
      "38952/38952 [==============================] - 2s 55us/step - loss: 0.2224 - acc: 0.9431 - val_loss: 0.2859 - val_acc: 0.9136\n",
      "Epoch 12/20\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 0.2084 - acc: 0.9453 - val_loss: 0.2805 - val_acc: 0.9129\n",
      "Epoch 13/20\n",
      "38952/38952 [==============================] - 2s 56us/step - loss: 0.1964 - acc: 0.9483 - val_loss: 0.2757 - val_acc: 0.9140\n",
      "Epoch 14/20\n",
      "38952/38952 [==============================] - 2s 56us/step - loss: 0.1864 - acc: 0.9502 - val_loss: 0.2688 - val_acc: 0.9150\n",
      "Epoch 15/20\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 0.1779 - acc: 0.9531 - val_loss: 0.2677 - val_acc: 0.9175\n",
      "Epoch 16/20\n",
      "38952/38952 [==============================] - 2s 53us/step - loss: 0.1702 - acc: 0.9546 - val_loss: 0.2619 - val_acc: 0.9127\n",
      "Epoch 17/20\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 0.1629 - acc: 0.9554 - val_loss: 0.2565 - val_acc: 0.9171\n",
      "Epoch 18/20\n",
      "38952/38952 [==============================] - 2s 55us/step - loss: 0.1561 - acc: 0.9571 - val_loss: 0.2568 - val_acc: 0.9152\n",
      "Epoch 19/20\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 0.1501 - acc: 0.9599 - val_loss: 0.2509 - val_acc: 0.9152\n",
      "Epoch 20/20\n",
      "38952/38952 [==============================] - 2s 50us/step - loss: 0.1450 - acc: 0.9610 - val_loss: 0.2535 - val_acc: 0.9180\n"
     ]
    }
   ],
   "source": [
    "ff_model.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.59      0.58       335\n",
      "           1       0.88      0.73      0.80        41\n",
      "           2       0.55      0.47      0.51        79\n",
      "           3       0.52      0.56      0.54        39\n",
      "           4       1.00      0.41      0.58        22\n",
      "           5       0.57      0.69      0.62       106\n",
      "           6       0.39      0.34      0.36       142\n",
      "           7       0.65      0.69      0.67        62\n",
      "           8       0.64      0.77      0.70        30\n",
      "           9       0.80      0.55      0.65        22\n",
      "          10       0.85      0.82      0.83       144\n",
      "          11       0.62      0.17      0.27        29\n",
      "          12       1.00      0.36      0.53        22\n",
      "          13       0.62      0.51      0.56        45\n",
      "          14       0.45      0.18      0.26        84\n",
      "          15       0.78      0.44      0.56        16\n",
      "          16       0.53      0.62      0.57       122\n",
      "          17       1.00      0.38      0.55        16\n",
      "          18       0.37      0.32      0.34        53\n",
      "          19       0.45      0.26      0.33        19\n",
      "          20       1.00      0.25      0.40        16\n",
      "          21       0.73      0.33      0.46        24\n",
      "          22       0.57      0.70      0.63       194\n",
      "          23       0.89      0.80      0.85        51\n",
      "          24       0.61      0.53      0.57        47\n",
      "          25       0.88      0.54      0.67        28\n",
      "          26       0.45      0.36      0.40        25\n",
      "          27       0.69      0.77      0.73       943\n",
      "          28       0.68      0.73      0.71        56\n",
      "          29       0.43      0.49      0.46        63\n",
      "          30       0.29      0.27      0.28        44\n",
      "          31       0.08      0.04      0.05        25\n",
      "          32       0.83      0.75      0.78        95\n",
      "          33       0.85      0.77      0.80       129\n",
      "          34       0.66      0.64      0.65        45\n",
      "          35       0.50      0.20      0.29        35\n",
      "          36       0.36      0.40      0.38        50\n",
      "          37       0.47      0.20      0.28        40\n",
      "          38       0.68      0.72      0.70        39\n",
      "          39       0.66      0.57      0.61        44\n",
      "          40       0.62      0.70      0.66       115\n",
      "          41       0.66      0.55      0.60       143\n",
      "          42       0.54      0.73      0.62        62\n",
      "          43       0.64      0.78      0.70        41\n",
      "          44       0.56      0.34      0.43        29\n",
      "          45       0.51      0.52      0.51        71\n",
      "          46       0.82      0.66      0.73        56\n",
      "          47       0.46      0.52      0.49        21\n",
      "          48       0.00      0.00      0.00        16\n",
      "          49       0.67      0.63      0.65        19\n",
      "          50       0.69      0.66      0.67        44\n",
      "          51       0.82      0.45      0.58        20\n",
      "          52       0.56      0.19      0.28        27\n",
      "          53       0.59      0.66      0.62        53\n",
      "          54       0.40      0.22      0.29        18\n",
      "          55       0.45      0.19      0.27        26\n",
      "          56       0.95      0.68      0.79        28\n",
      "          57       0.55      0.63      0.59       108\n",
      "          58       0.65      0.81      0.72       366\n",
      "          59       0.48      0.41      0.44        74\n",
      "          60       0.63      0.35      0.45        34\n",
      "          61       0.47      0.38      0.42        24\n",
      "          62       0.60      0.42      0.49        95\n",
      "          63       0.57      0.22      0.32        18\n",
      "          64       0.73      0.81      0.77        37\n",
      "          65       0.75      0.40      0.52        15\n",
      "          66       0.50      0.33      0.40        33\n",
      "          67       0.68      0.70      0.69       602\n",
      "          68       0.66      0.67      0.67       190\n",
      "          69       0.39      0.45      0.41       105\n",
      "          70       0.83      0.67      0.74        36\n",
      "          71       0.63      0.40      0.49        48\n",
      "          72       0.35      0.32      0.33        22\n",
      "          73       0.64      0.55      0.59        33\n",
      "          74       0.84      0.91      0.87        85\n",
      "          75       0.78      0.60      0.68        30\n",
      "          76       0.73      0.72      0.73        50\n",
      "          77       0.68      0.74      0.71        43\n",
      "          78       0.65      0.65      0.65       104\n",
      "          79       0.44      0.46      0.45       391\n",
      "          80       1.00      0.18      0.30        17\n",
      "          81       0.83      0.54      0.65        28\n",
      "          82       0.36      0.45      0.40        20\n",
      "          83       0.45      0.21      0.29        24\n",
      "          84       0.45      0.38      0.41        53\n",
      "          85       0.75      0.39      0.51        23\n",
      "          86       0.56      0.53      0.54        74\n",
      "          87       0.53      0.29      0.38        34\n",
      "          88       0.56      0.53      0.54       297\n",
      "          89       0.66      0.78      0.71        40\n",
      "          90       0.50      0.19      0.27        16\n",
      "          91       0.42      0.33      0.37        39\n",
      "          92       0.35      0.46      0.40        26\n",
      "          93       0.95      0.91      0.93        22\n",
      "          94       0.27      0.30      0.29        20\n",
      "          95       0.25      0.14      0.18        42\n",
      "          96       0.80      0.38      0.52        21\n",
      "          97       0.42      0.75      0.54        24\n",
      "          98       0.53      0.40      0.45        25\n",
      "          99       0.67      0.47      0.55        34\n",
      "         100       0.58      0.51      0.54        55\n",
      "         101       0.54      0.64      0.59       329\n",
      "         102       0.92      0.89      0.91        75\n",
      "         103       0.72      0.68      0.70        19\n",
      "         104       0.97      0.82      0.89        38\n",
      "         105       0.35      0.33      0.34        87\n",
      "         106       0.32      0.19      0.24        47\n",
      "         107       0.72      0.41      0.52        32\n",
      "         108       0.71      0.48      0.57        21\n",
      "         109       0.54      0.46      0.50       155\n",
      "         110       0.45      0.21      0.29        66\n",
      "         111       1.00      0.75      0.86        20\n",
      "         112       0.70      0.69      0.69        80\n",
      "         113       0.67      0.46      0.55        56\n",
      "         114       0.50      0.62      0.55       137\n",
      "         115       0.25      0.12      0.16        17\n",
      "         116       0.73      0.77      0.75       129\n",
      "         117       0.45      0.26      0.33        78\n",
      "         118       0.62      0.21      0.31        39\n",
      "         119       0.67      0.43      0.52        47\n",
      "         120       0.71      0.36      0.48        14\n",
      "         121       0.57      0.69      0.63       377\n",
      "         122       0.81      0.68      0.74        38\n",
      "         123       0.45      0.49      0.47        78\n",
      "         124       0.46      0.24      0.32        25\n",
      "         125       0.52      0.30      0.38        73\n",
      "         126       0.78      0.70      0.74        20\n",
      "         127       0.68      0.85      0.75       660\n",
      "         128       0.43      0.24      0.31        82\n",
      "         129       1.00      0.35      0.52        20\n",
      "         130       1.00      0.11      0.20        18\n",
      "         131       0.30      0.33      0.32       137\n",
      "         132       0.31      0.23      0.27        43\n",
      "         133       0.56      0.33      0.41        55\n",
      "         134       0.70      0.47      0.56        64\n",
      "         135       0.51      0.46      0.48        57\n",
      "         136       0.69      0.74      0.71        96\n",
      "         137       0.68      0.80      0.74        61\n",
      "         138       0.80      0.84      0.82      1969\n",
      "         139       0.57      0.67      0.62        54\n",
      "         140       1.00      0.12      0.22        16\n",
      "         141       0.48      0.52      0.50        25\n",
      "         142       0.57      0.24      0.33        17\n",
      "         143       0.35      0.44      0.39       298\n",
      "         144       0.24      0.22      0.23        41\n",
      "         145       0.72      0.61      0.66        83\n",
      "         146       0.72      0.52      0.60        25\n",
      "         147       0.36      0.36      0.36        25\n",
      "         148       0.46      0.53      0.49       209\n",
      "         149       0.70      0.47      0.56        15\n",
      "         150       0.65      0.49      0.56        61\n",
      "         151       0.67      0.40      0.50        20\n",
      "         152       0.57      0.40      0.47        20\n",
      "         153       0.37      0.32      0.34        34\n",
      "         154       0.80      0.76      0.78       144\n",
      "         155       0.55      0.61      0.58       387\n",
      "         156       0.54      0.46      0.50        56\n",
      "         157       0.38      0.25      0.30        24\n",
      "         158       0.61      0.33      0.43        51\n",
      "         159       0.82      0.39      0.53        23\n",
      "         160       0.74      0.32      0.44        44\n",
      "         161       0.57      0.59      0.58       240\n",
      "         162       1.00      0.33      0.50        27\n",
      "         163       1.00      0.26      0.42        19\n",
      "         164       0.51      0.56      0.53       161\n",
      "         165       0.55      0.62      0.58        66\n",
      "         166       0.44      0.51      0.47        70\n",
      "         167       0.76      0.74      0.75        61\n",
      "         168       0.28      0.45      0.34        92\n",
      "         169       0.50      0.71      0.59        34\n",
      "         170       0.61      0.54      0.57        37\n",
      "         171       0.42      0.40      0.41       223\n",
      "         172       0.12      0.12      0.12        24\n",
      "         173       0.71      0.71      0.71       248\n",
      "         174       0.60      0.58      0.59       191\n",
      "         175       0.50      0.17      0.25        53\n",
      "         176       0.59      0.63      0.61       236\n",
      "         177       0.40      0.26      0.31        39\n",
      "         178       0.84      0.69      0.76       201\n",
      "         179       0.38      0.16      0.23        31\n",
      "         180       0.64      0.74      0.68      1088\n",
      "         181       0.57      0.47      0.52       185\n",
      "         182       0.39      0.46      0.43        28\n",
      "         183       0.56      0.69      0.62       359\n",
      "         184       0.69      0.58      0.63        19\n",
      "         185       0.36      0.14      0.20        35\n",
      "         186       0.50      0.49      0.49        39\n",
      "         187       0.58      0.56      0.57        25\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     18549\n",
      "   macro avg       0.60      0.49      0.52     18549\n",
      "weighted avg       0.62      0.62      0.61     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ff_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поки що це найкращий результат згідно F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN+sum vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 188)               192700    \n",
      "=================================================================\n",
      "Total params: 500,924\n",
      "Trainable params: 500,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_sum_model = FeedForwardNN(train_doc_sum_vecs, train_topic_labels, \n",
    "                             test_doc_sum_vecs, test_topic_labels, 300, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38952 samples, validate on 4328 samples\n",
      "Epoch 1/25\n",
      "38952/38952 [==============================] - 3s 66us/step - loss: 6.4830 - acc: 0.2602 - val_loss: 4.9022 - val_acc: 0.3487\n",
      "Epoch 2/25\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 4.2437 - acc: 0.3973 - val_loss: 3.9712 - val_acc: 0.4136\n",
      "Epoch 3/25\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 3.3941 - acc: 0.4670 - val_loss: 3.5272 - val_acc: 0.4441\n",
      "Epoch 4/25\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 2.8743 - acc: 0.5196 - val_loss: 3.2926 - val_acc: 0.4670\n",
      "Epoch 5/25\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 2.5304 - acc: 0.5637 - val_loss: 3.1585 - val_acc: 0.4852\n",
      "Epoch 6/25\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 2.2664 - acc: 0.6015 - val_loss: 3.1114 - val_acc: 0.4889\n",
      "Epoch 7/25\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 2.0649 - acc: 0.6341 - val_loss: 3.0131 - val_acc: 0.5116\n",
      "Epoch 8/25\n",
      "38952/38952 [==============================] - 2s 53us/step - loss: 1.8967 - acc: 0.6629 - val_loss: 2.9514 - val_acc: 0.5183\n",
      "Epoch 9/25\n",
      "38952/38952 [==============================] - 2s 53us/step - loss: 1.7579 - acc: 0.6892 - val_loss: 2.9348 - val_acc: 0.5215\n",
      "Epoch 10/25\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 1.6384 - acc: 0.7149 - val_loss: 2.9202 - val_acc: 0.5213\n",
      "Epoch 11/25\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 1.5304 - acc: 0.7366 - val_loss: 2.8669 - val_acc: 0.5360\n",
      "Epoch 12/25\n",
      "38952/38952 [==============================] - 2s 51us/step - loss: 1.4427 - acc: 0.7580 - val_loss: 2.8850 - val_acc: 0.5400\n",
      "Epoch 13/25\n",
      "38952/38952 [==============================] - 2s 55us/step - loss: 1.3604 - acc: 0.7759 - val_loss: 2.9160 - val_acc: 0.5356\n",
      "Epoch 14/25\n",
      "38952/38952 [==============================] - 2s 57us/step - loss: 1.2922 - acc: 0.7917 - val_loss: 2.8815 - val_acc: 0.5400\n",
      "Epoch 15/25\n",
      "38952/38952 [==============================] - 2s 55us/step - loss: 1.2290 - acc: 0.8096 - val_loss: 2.8978 - val_acc: 0.5444\n",
      "Epoch 16/25\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 1.1765 - acc: 0.8223 - val_loss: 2.8983 - val_acc: 0.5451\n",
      "Epoch 17/25\n",
      "38952/38952 [==============================] - 2s 53us/step - loss: 1.1275 - acc: 0.8369 - val_loss: 2.8946 - val_acc: 0.5527\n",
      "Epoch 18/25\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 1.0841 - acc: 0.8494 - val_loss: 2.9154 - val_acc: 0.5566\n",
      "Epoch 19/25\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 1.0467 - acc: 0.8601 - val_loss: 2.9296 - val_acc: 0.5548\n",
      "Epoch 20/25\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 1.0128 - acc: 0.8709 - val_loss: 2.9542 - val_acc: 0.5506\n",
      "Epoch 21/25\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 0.9813 - acc: 0.8800 - val_loss: 2.9672 - val_acc: 0.5585\n",
      "Epoch 22/25\n",
      "38952/38952 [==============================] - 2s 52us/step - loss: 0.9539 - acc: 0.8880 - val_loss: 2.9706 - val_acc: 0.5605\n",
      "Epoch 23/25\n",
      "38952/38952 [==============================] - 2s 55us/step - loss: 0.9268 - acc: 0.8954 - val_loss: 3.0270 - val_acc: 0.5631\n",
      "Epoch 24/25\n",
      "38952/38952 [==============================] - 2s 53us/step - loss: 0.9075 - acc: 0.9016 - val_loss: 3.0136 - val_acc: 0.5675\n",
      "Epoch 25/25\n",
      "38952/38952 [==============================] - 2s 54us/step - loss: 0.8856 - acc: 0.9073 - val_loss: 3.0916 - val_acc: 0.5578\n"
     ]
    }
   ],
   "source": [
    "ff_sum_model.train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.62      0.60       335\n",
      "           1       0.00      0.00      0.00        41\n",
      "           2       0.55      0.52      0.53        79\n",
      "           3       0.45      0.36      0.40        39\n",
      "           4       0.00      0.00      0.00        22\n",
      "           5       0.60      0.47      0.53       106\n",
      "           6       0.31      0.22      0.26       142\n",
      "           7       0.48      0.50      0.49        62\n",
      "           8       0.76      0.43      0.55        30\n",
      "           9       0.82      0.41      0.55        22\n",
      "          10       0.73      0.75      0.74       144\n",
      "          11       0.33      0.21      0.26        29\n",
      "          12       0.00      0.00      0.00        22\n",
      "          13       0.52      0.38      0.44        45\n",
      "          14       0.14      0.13      0.14        84\n",
      "          15       0.58      0.44      0.50        16\n",
      "          16       0.60      0.53      0.57       122\n",
      "          17       0.50      0.31      0.38        16\n",
      "          18       0.29      0.23      0.25        53\n",
      "          19       0.44      0.21      0.29        19\n",
      "          20       0.00      0.00      0.00        16\n",
      "          21       0.11      0.04      0.06        24\n",
      "          22       0.61      0.63      0.62       194\n",
      "          23       0.90      0.75      0.82        51\n",
      "          24       0.58      0.55      0.57        47\n",
      "          25       0.35      0.25      0.29        28\n",
      "          26       0.00      0.00      0.00        25\n",
      "          27       0.63      0.85      0.72       943\n",
      "          28       0.45      0.45      0.45        56\n",
      "          29       0.52      0.52      0.52        63\n",
      "          30       0.32      0.14      0.19        44\n",
      "          31       0.08      0.04      0.05        25\n",
      "          32       0.68      0.58      0.63        95\n",
      "          33       0.61      0.82      0.70       129\n",
      "          34       0.64      0.56      0.60        45\n",
      "          35       0.29      0.23      0.25        35\n",
      "          36       0.43      0.20      0.27        50\n",
      "          37       0.23      0.25      0.24        40\n",
      "          38       0.61      0.59      0.60        39\n",
      "          39       0.45      0.43      0.44        44\n",
      "          40       0.68      0.61      0.64       115\n",
      "          41       0.56      0.45      0.50       143\n",
      "          42       0.60      0.66      0.63        62\n",
      "          43       0.50      0.37      0.42        41\n",
      "          44       0.44      0.28      0.34        29\n",
      "          45       0.54      0.49      0.51        71\n",
      "          46       0.68      0.46      0.55        56\n",
      "          47       0.23      0.14      0.18        21\n",
      "          48       0.00      0.00      0.00        16\n",
      "          49       0.00      0.00      0.00        19\n",
      "          50       0.41      0.25      0.31        44\n",
      "          51       0.50      0.50      0.50        20\n",
      "          52       0.45      0.19      0.26        27\n",
      "          53       0.62      0.53      0.57        53\n",
      "          54       0.50      0.11      0.18        18\n",
      "          55       0.00      0.00      0.00        26\n",
      "          56       1.00      0.57      0.73        28\n",
      "          57       0.62      0.69      0.65       108\n",
      "          58       0.64      0.69      0.66       366\n",
      "          59       0.30      0.32      0.31        74\n",
      "          60       0.00      0.00      0.00        34\n",
      "          61       0.00      0.00      0.00        24\n",
      "          62       0.45      0.43      0.44        95\n",
      "          63       0.44      0.22      0.30        18\n",
      "          64       0.82      0.73      0.77        37\n",
      "          65       0.00      0.00      0.00        15\n",
      "          66       0.47      0.45      0.46        33\n",
      "          67       0.59      0.71      0.65       602\n",
      "          68       0.59      0.69      0.64       190\n",
      "          69       0.26      0.33      0.29       105\n",
      "          70       0.69      0.61      0.65        36\n",
      "          71       0.43      0.31      0.36        48\n",
      "          72       0.30      0.14      0.19        22\n",
      "          73       0.58      0.42      0.49        33\n",
      "          74       0.76      0.74      0.75        85\n",
      "          75       0.80      0.53      0.64        30\n",
      "          76       0.73      0.38      0.50        50\n",
      "          77       0.48      0.30      0.37        43\n",
      "          78       0.62      0.55      0.58       104\n",
      "          79       0.42      0.58      0.49       391\n",
      "          80       0.27      0.18      0.21        17\n",
      "          81       0.00      0.00      0.00        28\n",
      "          82       0.24      0.30      0.27        20\n",
      "          83       0.23      0.12      0.16        24\n",
      "          84       0.40      0.51      0.45        53\n",
      "          85       0.58      0.30      0.40        23\n",
      "          86       0.26      0.14      0.18        74\n",
      "          87       0.00      0.00      0.00        34\n",
      "          88       0.57      0.34      0.42       297\n",
      "          89       0.75      0.53      0.62        40\n",
      "          90       0.36      0.25      0.30        16\n",
      "          91       0.33      0.36      0.34        39\n",
      "          92       0.27      0.35      0.31        26\n",
      "          93       0.86      0.86      0.86        22\n",
      "          94       0.27      0.20      0.23        20\n",
      "          95       0.35      0.26      0.30        42\n",
      "          96       0.54      0.33      0.41        21\n",
      "          97       0.72      0.54      0.62        24\n",
      "          98       0.41      0.28      0.33        25\n",
      "          99       0.48      0.41      0.44        34\n",
      "         100       0.38      0.35      0.36        55\n",
      "         101       0.57      0.59      0.58       329\n",
      "         102       0.75      0.57      0.65        75\n",
      "         103       0.29      0.37      0.33        19\n",
      "         104       0.79      0.68      0.73        38\n",
      "         105       0.33      0.29      0.31        87\n",
      "         106       0.28      0.36      0.31        47\n",
      "         107       0.00      0.00      0.00        32\n",
      "         108       0.00      0.00      0.00        21\n",
      "         109       0.43      0.34      0.38       155\n",
      "         110       0.38      0.36      0.37        66\n",
      "         111       0.00      0.00      0.00        20\n",
      "         112       0.43      0.33      0.37        80\n",
      "         113       0.62      0.38      0.47        56\n",
      "         114       0.55      0.47      0.51       137\n",
      "         115       0.15      0.12      0.13        17\n",
      "         116       0.64      0.74      0.68       129\n",
      "         117       0.16      0.26      0.20        78\n",
      "         118       0.34      0.26      0.29        39\n",
      "         119       0.60      0.45      0.51        47\n",
      "         120       0.29      0.14      0.19        14\n",
      "         121       0.51      0.55      0.53       377\n",
      "         122       0.52      0.58      0.55        38\n",
      "         123       0.46      0.33      0.39        78\n",
      "         124       0.29      0.28      0.29        25\n",
      "         125       0.40      0.45      0.43        73\n",
      "         126       1.00      0.70      0.82        20\n",
      "         127       0.65      0.72      0.69       660\n",
      "         128       0.28      0.23      0.25        82\n",
      "         129       0.00      0.00      0.00        20\n",
      "         130       0.12      0.06      0.08        18\n",
      "         131       0.29      0.24      0.26       137\n",
      "         132       0.26      0.19      0.22        43\n",
      "         133       0.43      0.35      0.38        55\n",
      "         134       0.49      0.47      0.48        64\n",
      "         135       0.28      0.33      0.31        57\n",
      "         136       0.64      0.49      0.55        96\n",
      "         137       0.48      0.56      0.52        61\n",
      "         138       0.77      0.86      0.81      1969\n",
      "         139       0.51      0.59      0.55        54\n",
      "         140       0.00      0.00      0.00        16\n",
      "         141       0.28      0.20      0.23        25\n",
      "         142       0.50      0.24      0.32        17\n",
      "         143       0.31      0.40      0.35       298\n",
      "         144       0.23      0.27      0.25        41\n",
      "         145       0.55      0.60      0.57        83\n",
      "         146       0.62      0.60      0.61        25\n",
      "         147       0.26      0.20      0.23        25\n",
      "         148       0.38      0.43      0.40       209\n",
      "         149       0.00      0.00      0.00        15\n",
      "         150       0.46      0.62      0.53        61\n",
      "         151       0.33      0.35      0.34        20\n",
      "         152       0.36      0.20      0.26        20\n",
      "         153       0.33      0.24      0.28        34\n",
      "         154       0.66      0.65      0.66       144\n",
      "         155       0.57      0.55      0.56       387\n",
      "         156       0.27      0.30      0.29        56\n",
      "         157       0.45      0.21      0.29        24\n",
      "         158       0.61      0.37      0.46        51\n",
      "         159       0.77      0.43      0.56        23\n",
      "         160       0.28      0.32      0.30        44\n",
      "         161       0.47      0.62      0.53       240\n",
      "         162       0.00      0.00      0.00        27\n",
      "         163       0.60      0.32      0.41        19\n",
      "         164       0.45      0.49      0.47       161\n",
      "         165       0.56      0.48      0.52        66\n",
      "         166       0.52      0.53      0.52        70\n",
      "         167       0.78      0.70      0.74        61\n",
      "         168       0.40      0.34      0.36        92\n",
      "         169       0.69      0.71      0.70        34\n",
      "         170       0.42      0.27      0.33        37\n",
      "         171       0.36      0.43      0.39       223\n",
      "         172       0.00      0.00      0.00        24\n",
      "         173       0.66      0.77      0.71       248\n",
      "         174       0.52      0.48      0.50       191\n",
      "         175       0.27      0.28      0.28        53\n",
      "         176       0.64      0.48      0.55       236\n",
      "         177       0.36      0.38      0.37        39\n",
      "         178       0.58      0.69      0.63       201\n",
      "         179       0.36      0.16      0.22        31\n",
      "         180       0.62      0.69      0.65      1088\n",
      "         181       0.42      0.48      0.44       185\n",
      "         182       0.50      0.32      0.39        28\n",
      "         183       0.53      0.64      0.58       359\n",
      "         184       0.65      0.58      0.61        19\n",
      "         185       0.00      0.00      0.00        35\n",
      "         186       0.41      0.23      0.30        39\n",
      "         187       0.00      0.00      0.00        25\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     18549\n",
      "   macro avg       0.43      0.37      0.39     18549\n",
      "weighted avg       0.54      0.56      0.54     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ff_sum_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приблизно як логістична регресія на сумі векторів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тренуванням LSTM мережі, спочатку векторизуємо дані. Кожний документ представимо вектором цілих чисел де кожне слово замінимо його цілочисленною відповідністю. Для цього спочатку готуємо словник таких відповідностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134538"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "vocab = Dictionary([doc.words for doc in (train_tagged_docs + test_tagged_docs)])\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відкидаємо слова з дуже великою або дуже малою частотою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48519\n"
     ]
    }
   ],
   "source": [
    "vocab.filter_extremes(no_below = 3, no_above = 0.9, keep_n = 50000)\n",
    "\n",
    "MAX_WORDS_NUM = len(vocab) + 1\n",
    "\n",
    "print(MAX_WORDS_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готуємо вектори документів для тренувальних і тестових данних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict([(i, token)for token, i in vocab.token2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(words):\n",
    "    return [i + 1 for i in vocab.doc2idx(words)]\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [id2token[i - 1] for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43287"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences = [text_to_sequence(doc.words) for doc in train_tagged_docs]\n",
    "\n",
    "len(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 3, 23, 18, 1, 32, 26, 7, 11, 18, 21, 19, 10, 28, 33, 4, 5, 16, 20, 22, 18, 30, 29, 2, 0, 9, 12, 0, 8, 27, 14, 31, 6, 15, 25, 15, 12, 18, 4, 17, 13]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['добрий', 'прошу', 'надати', 'роз’яснення', 'приводу', 'підвищення', 'рази', 'ціни', 'оплату', 'послуг', 'дитячого', 'садочка', 'оболонському', 'києва', 'ціни', 'районах', 'підвищені', 'оболонський', 'район', 'мав', 'нахабність', 'зробити', 'харчування', 'дітей', 'нічим', 'відрізняється', 'садочків', 'районів', 'дітей', 'годують', 'червоною', 'дають', 'свіжі', 'фрукти', 'повинні', 'давати', 'вихователі', 'більшу', 'зарплату', 'підвищення', 'оплати', 'отримувати', 'харчування', 'дітей', 'покращується', 'давали', 'пів', 'дають', 'синього', 'кольору', 'понеділок', 'макарони', 'наступний', 'прошу', 'розібратися', 'даній', 'ситуації']\n"
     ]
    }
   ],
   "source": [
    "print(sequence_to_text(train_sequences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18552"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences = [text_to_sequence(doc.words) for doc in test_tagged_docs]\n",
    "\n",
    "len(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дивимось розподіл векторів по розміру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length: 39.9\n",
      "Max Length: 235\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [len(s) for s in (train_sequences + test_sequences)]\n",
    "print(\"Average Length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"Max Length: %d\" % max(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(seq_lens, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встановолюємо максимального вектору та вирівнюємо відносно цього розміру тренувальні і тестові данні."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43287, 235)\n",
      "(18552, 235)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQ_LEN = 235\n",
    "\n",
    "train_padded_seqs = pad_sequences(train_sequences, maxlen = MAX_SEQ_LEN)\n",
    "test_padded_seqs = pad_sequences(test_sequences, maxlen = MAX_SEQ_LEN)\n",
    "\n",
    "print(train_padded_seqs.shape)\n",
    "print(test_padded_seqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будуємо та тренуємо модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, Input, SpatialDropout1D, Dropout\n",
    "\n",
    "class LstmModel(NnModel):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels,\n",
    "                 embedding_dim, memory_units):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(MAX_WORDS_NUM, embedding_dim, input_length = MAX_SEQ_LEN))\n",
    "        self.model.add(Dropout(0.2))        \n",
    "        self.model.add(LSTM(memory_units, dropout=0.2, recurrent_dropout=0.2))        \n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=RMSprop(lr=1e-3),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 235, 100)          4855300   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 235, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 188)               18988     \n",
      "=================================================================\n",
      "Total params: 4,954,688\n",
      "Trainable params: 4,954,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LstmModel(train_padded_seqs, train_topic_labels, test_padded_seqs, test_topic_labels, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38952 samples, validate on 4328 samples\n",
      "Epoch 1/30\n",
      "38952/38952 [==============================] - 92s 2ms/step - loss: 4.1753 - acc: 0.1576 - val_loss: 3.7373 - val_acc: 0.2380\n",
      "Epoch 2/30\n",
      "38952/38952 [==============================] - 90s 2ms/step - loss: 3.3994 - acc: 0.2684 - val_loss: 3.0775 - val_acc: 0.3161\n",
      "Epoch 3/30\n",
      "38952/38952 [==============================] - 89s 2ms/step - loss: 2.9337 - acc: 0.3233 - val_loss: 2.7251 - val_acc: 0.3634\n",
      "Epoch 4/30\n",
      "38952/38952 [==============================] - 92s 2ms/step - loss: 2.5980 - acc: 0.3754 - val_loss: 2.4697 - val_acc: 0.4101\n",
      "Epoch 5/30\n",
      "38952/38952 [==============================] - 91s 2ms/step - loss: 2.3235 - acc: 0.4271 - val_loss: 2.2899 - val_acc: 0.4466\n",
      "Epoch 6/30\n",
      "38952/38952 [==============================] - 91s 2ms/step - loss: 2.0915 - acc: 0.4723 - val_loss: 2.1276 - val_acc: 0.4778\n",
      "Epoch 7/30\n",
      "38952/38952 [==============================] - 90s 2ms/step - loss: 1.8945 - acc: 0.5152 - val_loss: 2.0026 - val_acc: 0.5039\n",
      "Epoch 8/30\n",
      "38952/38952 [==============================] - 93s 2ms/step - loss: 1.7245 - acc: 0.5481 - val_loss: 1.9163 - val_acc: 0.5326\n",
      "Epoch 9/30\n",
      "38952/38952 [==============================] - 88s 2ms/step - loss: 1.5717 - acc: 0.5854 - val_loss: 1.8354 - val_acc: 0.5430\n",
      "Epoch 10/30\n",
      "38952/38952 [==============================] - 88s 2ms/step - loss: 1.4446 - acc: 0.6151 - val_loss: 1.7833 - val_acc: 0.5524\n",
      "Epoch 11/30\n",
      "38952/38952 [==============================] - 89s 2ms/step - loss: 1.3233 - acc: 0.6437 - val_loss: 1.7359 - val_acc: 0.5656\n",
      "Epoch 12/30\n",
      "38952/38952 [==============================] - 88s 2ms/step - loss: 1.2227 - acc: 0.6700 - val_loss: 1.7316 - val_acc: 0.5652\n",
      "Epoch 13/30\n",
      "38952/38952 [==============================] - 88s 2ms/step - loss: 1.1383 - acc: 0.6915 - val_loss: 1.6910 - val_acc: 0.5832\n",
      "Epoch 14/30\n",
      "38952/38952 [==============================] - 89s 2ms/step - loss: 1.0514 - acc: 0.7159 - val_loss: 1.6856 - val_acc: 0.5871\n",
      "Epoch 15/30\n",
      "38952/38952 [==============================] - 86s 2ms/step - loss: 0.9709 - acc: 0.7349 - val_loss: 1.6737 - val_acc: 0.5922\n",
      "Epoch 16/30\n",
      "38952/38952 [==============================] - 86s 2ms/step - loss: 0.9024 - acc: 0.7544 - val_loss: 1.6750 - val_acc: 0.5915\n",
      "Epoch 17/30\n",
      "38952/38952 [==============================] - 87s 2ms/step - loss: 0.8340 - acc: 0.7727 - val_loss: 1.6741 - val_acc: 0.5991\n",
      "Epoch 18/30\n",
      "38952/38952 [==============================] - 84s 2ms/step - loss: 0.7736 - acc: 0.7901 - val_loss: 1.6398 - val_acc: 0.6058\n",
      "Epoch 19/30\n",
      "38952/38952 [==============================] - 83s 2ms/step - loss: 0.7175 - acc: 0.8050 - val_loss: 1.6709 - val_acc: 0.6000\n",
      "Epoch 20/30\n",
      "38952/38952 [==============================] - 83s 2ms/step - loss: 0.6675 - acc: 0.8178 - val_loss: 1.6821 - val_acc: 0.6095\n",
      "Epoch 21/30\n",
      "38952/38952 [==============================] - 82s 2ms/step - loss: 0.6189 - acc: 0.8335 - val_loss: 1.6952 - val_acc: 0.6019\n",
      "Epoch 22/30\n",
      "38952/38952 [==============================] - 82s 2ms/step - loss: 0.5819 - acc: 0.8432 - val_loss: 1.6802 - val_acc: 0.6144\n",
      "Epoch 23/30\n",
      "38952/38952 [==============================] - 89s 2ms/step - loss: 0.5403 - acc: 0.8555 - val_loss: 1.7010 - val_acc: 0.6139\n",
      "Epoch 24/30\n",
      "38952/38952 [==============================] - 84s 2ms/step - loss: 0.5046 - acc: 0.8644 - val_loss: 1.7179 - val_acc: 0.6171\n",
      "Epoch 25/30\n",
      "38952/38952 [==============================] - 84s 2ms/step - loss: 0.4697 - acc: 0.8751 - val_loss: 1.7036 - val_acc: 0.6213\n",
      "Epoch 26/30\n",
      "38952/38952 [==============================] - 84s 2ms/step - loss: 0.4412 - acc: 0.8830 - val_loss: 1.7260 - val_acc: 0.6128\n",
      "Epoch 27/30\n",
      "38952/38952 [==============================] - 87s 2ms/step - loss: 0.4084 - acc: 0.8917 - val_loss: 1.7597 - val_acc: 0.6148\n",
      "Epoch 28/30\n",
      "38952/38952 [==============================] - 84s 2ms/step - loss: 0.3831 - acc: 0.8977 - val_loss: 1.7556 - val_acc: 0.6158\n",
      "Epoch 29/30\n",
      "38952/38952 [==============================] - 85s 2ms/step - loss: 0.3629 - acc: 0.9026 - val_loss: 1.7678 - val_acc: 0.6169\n",
      "Epoch 30/30\n",
      "38952/38952 [==============================] - 84s 2ms/step - loss: 0.3381 - acc: 0.9111 - val_loss: 1.7805 - val_acc: 0.6181\n"
     ]
    }
   ],
   "source": [
    "lstm_model.train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63       335\n",
      "           1       0.69      0.59      0.63        41\n",
      "           2       0.57      0.57      0.57        79\n",
      "           3       0.34      0.46      0.39        39\n",
      "           4       0.45      0.23      0.30        22\n",
      "           5       0.70      0.72      0.71       106\n",
      "           6       0.36      0.30      0.33       142\n",
      "           7       0.67      0.61      0.64        62\n",
      "           8       0.74      0.57      0.64        30\n",
      "           9       0.63      0.55      0.59        22\n",
      "          10       0.86      0.80      0.83       144\n",
      "          11       0.28      0.17      0.21        29\n",
      "          12       0.65      0.68      0.67        22\n",
      "          13       0.46      0.40      0.43        45\n",
      "          14       0.25      0.17      0.20        84\n",
      "          15       0.64      0.56      0.60        16\n",
      "          16       0.76      0.61      0.67       122\n",
      "          17       0.50      0.31      0.38        16\n",
      "          18       0.44      0.42      0.43        53\n",
      "          19       0.75      0.16      0.26        19\n",
      "          20       0.18      0.12      0.15        16\n",
      "          21       0.47      0.29      0.36        24\n",
      "          22       0.65      0.63      0.64       194\n",
      "          23       0.91      0.80      0.85        51\n",
      "          24       0.59      0.55      0.57        47\n",
      "          25       0.44      0.57      0.50        28\n",
      "          26       0.32      0.28      0.30        25\n",
      "          27       0.77      0.78      0.77       943\n",
      "          28       0.76      0.70      0.73        56\n",
      "          29       0.42      0.41      0.42        63\n",
      "          30       0.23      0.20      0.22        44\n",
      "          31       0.13      0.12      0.12        25\n",
      "          32       0.73      0.72      0.72        95\n",
      "          33       0.74      0.82      0.78       129\n",
      "          34       0.56      0.49      0.52        45\n",
      "          35       0.17      0.20      0.18        35\n",
      "          36       0.38      0.22      0.28        50\n",
      "          37       0.15      0.20      0.17        40\n",
      "          38       0.69      0.51      0.59        39\n",
      "          39       0.54      0.57      0.56        44\n",
      "          40       0.60      0.69      0.64       115\n",
      "          41       0.54      0.55      0.55       143\n",
      "          42       0.59      0.69      0.64        62\n",
      "          43       0.56      0.49      0.52        41\n",
      "          44       0.41      0.38      0.39        29\n",
      "          45       0.53      0.51      0.52        71\n",
      "          46       0.70      0.66      0.68        56\n",
      "          47       0.41      0.43      0.42        21\n",
      "          48       0.33      0.12      0.18        16\n",
      "          49       0.35      0.37      0.36        19\n",
      "          50       0.39      0.52      0.45        44\n",
      "          51       0.23      0.15      0.18        20\n",
      "          52       0.50      0.19      0.27        27\n",
      "          53       0.44      0.68      0.54        53\n",
      "          54       0.50      0.06      0.10        18\n",
      "          55       0.17      0.19      0.18        26\n",
      "          56       0.68      0.68      0.68        28\n",
      "          57       0.68      0.66      0.67       108\n",
      "          58       0.77      0.74      0.76       366\n",
      "          59       0.47      0.34      0.39        74\n",
      "          60       0.44      0.12      0.19        34\n",
      "          61       0.50      0.21      0.29        24\n",
      "          62       0.52      0.48      0.50        95\n",
      "          63       0.22      0.11      0.15        18\n",
      "          64       0.76      0.76      0.76        37\n",
      "          65       0.45      0.33      0.38        15\n",
      "          66       0.43      0.45      0.44        33\n",
      "          67       0.65      0.71      0.68       602\n",
      "          68       0.63      0.73      0.68       190\n",
      "          69       0.41      0.37      0.39       105\n",
      "          70       0.80      0.67      0.73        36\n",
      "          71       0.40      0.35      0.38        48\n",
      "          72       0.42      0.23      0.29        22\n",
      "          73       0.48      0.48      0.48        33\n",
      "          74       0.95      0.88      0.91        85\n",
      "          75       0.63      0.63      0.63        30\n",
      "          76       0.85      0.46      0.60        50\n",
      "          77       0.66      0.53      0.59        43\n",
      "          78       0.57      0.59      0.58       104\n",
      "          79       0.53      0.49      0.51       391\n",
      "          80       0.07      0.06      0.06        17\n",
      "          81       0.94      0.57      0.71        28\n",
      "          82       0.36      0.25      0.29        20\n",
      "          83       0.07      0.04      0.05        24\n",
      "          84       0.39      0.34      0.36        53\n",
      "          85       0.39      0.30      0.34        23\n",
      "          86       0.35      0.38      0.36        74\n",
      "          87       0.37      0.32      0.34        34\n",
      "          88       0.58      0.63      0.60       297\n",
      "          89       0.50      0.38      0.43        40\n",
      "          90       0.00      0.00      0.00        16\n",
      "          91       0.47      0.36      0.41        39\n",
      "          92       0.50      0.27      0.35        26\n",
      "          93       1.00      0.91      0.95        22\n",
      "          94       0.16      0.40      0.23        20\n",
      "          95       0.21      0.14      0.17        42\n",
      "          96       0.55      0.57      0.56        21\n",
      "          97       0.63      0.71      0.67        24\n",
      "          98       0.18      0.12      0.14        25\n",
      "          99       0.42      0.50      0.46        34\n",
      "         100       0.44      0.64      0.52        55\n",
      "         101       0.67      0.67      0.67       329\n",
      "         102       0.90      0.87      0.88        75\n",
      "         103       0.26      0.26      0.26        19\n",
      "         104       0.97      0.76      0.85        38\n",
      "         105       0.33      0.45      0.38        87\n",
      "         106       0.25      0.21      0.23        47\n",
      "         107       0.29      0.25      0.27        32\n",
      "         108       0.46      0.62      0.53        21\n",
      "         109       0.49      0.56      0.52       155\n",
      "         110       0.29      0.27      0.28        66\n",
      "         111       0.60      0.75      0.67        20\n",
      "         112       0.62      0.65      0.63        80\n",
      "         113       0.68      0.70      0.69        56\n",
      "         114       0.59      0.56      0.57       137\n",
      "         115       0.10      0.06      0.07        17\n",
      "         116       0.77      0.77      0.77       129\n",
      "         117       0.19      0.18      0.19        78\n",
      "         118       0.27      0.33      0.30        39\n",
      "         119       0.62      0.60      0.61        47\n",
      "         120       0.27      0.29      0.28        14\n",
      "         121       0.60      0.60      0.60       377\n",
      "         122       0.62      0.55      0.58        38\n",
      "         123       0.39      0.44      0.41        78\n",
      "         124       0.44      0.16      0.24        25\n",
      "         125       0.39      0.36      0.37        73\n",
      "         126       0.56      0.70      0.62        20\n",
      "         127       0.81      0.85      0.83       660\n",
      "         128       0.39      0.27      0.32        82\n",
      "         129       0.26      0.30      0.28        20\n",
      "         130       0.31      0.22      0.26        18\n",
      "         131       0.32      0.36      0.34       137\n",
      "         132       0.32      0.28      0.30        43\n",
      "         133       0.42      0.35      0.38        55\n",
      "         134       0.66      0.55      0.60        64\n",
      "         135       0.39      0.33      0.36        57\n",
      "         136       0.71      0.69      0.70        96\n",
      "         137       0.81      0.69      0.74        61\n",
      "         138       0.86      0.89      0.88      1969\n",
      "         139       0.61      0.78      0.68        54\n",
      "         140       0.62      0.31      0.42        16\n",
      "         141       0.30      0.36      0.33        25\n",
      "         142       0.29      0.24      0.26        17\n",
      "         143       0.41      0.36      0.39       298\n",
      "         144       0.16      0.17      0.17        41\n",
      "         145       0.71      0.72      0.72        83\n",
      "         146       0.65      0.52      0.58        25\n",
      "         147       0.12      0.08      0.10        25\n",
      "         148       0.48      0.56      0.52       209\n",
      "         149       0.40      0.27      0.32        15\n",
      "         150       0.46      0.44      0.45        61\n",
      "         151       0.17      0.20      0.18        20\n",
      "         152       0.21      0.25      0.23        20\n",
      "         153       0.31      0.44      0.36        34\n",
      "         154       0.85      0.76      0.81       144\n",
      "         155       0.61      0.65      0.63       387\n",
      "         156       0.34      0.32      0.33        56\n",
      "         157       0.31      0.17      0.22        24\n",
      "         158       0.49      0.45      0.47        51\n",
      "         159       0.43      0.43      0.43        23\n",
      "         160       0.25      0.30      0.27        44\n",
      "         161       0.62      0.72      0.67       240\n",
      "         162       0.26      0.33      0.29        27\n",
      "         163       0.50      0.37      0.42        19\n",
      "         164       0.47      0.45      0.46       161\n",
      "         165       0.59      0.55      0.57        66\n",
      "         166       0.45      0.56      0.50        70\n",
      "         167       0.82      0.80      0.81        61\n",
      "         168       0.28      0.28      0.28        92\n",
      "         169       0.61      0.74      0.67        34\n",
      "         170       0.54      0.54      0.54        37\n",
      "         171       0.44      0.50      0.47       223\n",
      "         172       0.25      0.17      0.20        24\n",
      "         173       0.74      0.85      0.79       248\n",
      "         174       0.65      0.58      0.61       191\n",
      "         175       0.31      0.28      0.29        53\n",
      "         176       0.77      0.75      0.76       236\n",
      "         177       0.64      0.41      0.50        39\n",
      "         178       0.78      0.78      0.78       201\n",
      "         179       0.50      0.45      0.47        31\n",
      "         180       0.70      0.76      0.73      1088\n",
      "         181       0.56      0.54      0.55       185\n",
      "         182       0.48      0.39      0.43        28\n",
      "         183       0.66      0.69      0.68       359\n",
      "         184       0.63      0.63      0.63        19\n",
      "         185       0.18      0.31      0.23        35\n",
      "         186       0.62      0.41      0.49        39\n",
      "         187       0.52      0.48      0.50        25\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     18549\n",
      "   macro avg       0.50      0.46      0.47     18549\n",
      "weighted avg       0.62      0.62      0.62     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Якість приблизно така я у Logreg+Doc2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "Спробуємо застосувати згорточну мережу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "class CnnModel(NnModel):\n",
    "     def __init__(self, train_vectors, train_labels, test_vectors, test_labels, \n",
    "                  embedding_dim, channels_num, conv_window):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(MAX_WORDS_NUM, embedding_dim, input_length = MAX_SEQ_LEN))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Conv1D(channels_num, conv_window, activation='relu'))\n",
    "        self.model.add(MaxPooling1D(conv_window))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(1024, activation='relu'))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=RMSprop(lr=1e-3),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 235, 100)          4855300   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 235, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 231, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5888)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1024)              6030336   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 188)               192700    \n",
      "=================================================================\n",
      "Total params: 11,142,464\n",
      "Trainable params: 11,142,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CnnModel(train_padded_seqs, train_topic_labels, test_padded_seqs, test_topic_labels, 100, 128, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38952 samples, validate on 4328 samples\n",
      "Epoch 1/30\n",
      "38952/38952 [==============================] - 69s 2ms/step - loss: 3.6320 - acc: 0.2328 - val_loss: 2.8530 - val_acc: 0.3415\n",
      "Epoch 2/30\n",
      "38952/38952 [==============================] - 69s 2ms/step - loss: 2.4023 - acc: 0.4127 - val_loss: 2.2241 - val_acc: 0.4630\n",
      "Epoch 3/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 1.8068 - acc: 0.5257 - val_loss: 2.0443 - val_acc: 0.5072\n",
      "Epoch 4/30\n",
      "38952/38952 [==============================] - 69s 2ms/step - loss: 1.4328 - acc: 0.6060 - val_loss: 2.0490 - val_acc: 0.5206\n",
      "Epoch 5/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 1.1630 - acc: 0.6743 - val_loss: 2.1007 - val_acc: 0.5354\n",
      "Epoch 6/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.9518 - acc: 0.7299 - val_loss: 2.1749 - val_acc: 0.5566\n",
      "Epoch 7/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.7770 - acc: 0.7795 - val_loss: 2.3155 - val_acc: 0.5534\n",
      "Epoch 8/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.6384 - acc: 0.8188 - val_loss: 2.5143 - val_acc: 0.5555\n",
      "Epoch 9/30\n",
      "38952/38952 [==============================] - 70s 2ms/step - loss: 0.5258 - acc: 0.8521 - val_loss: 2.6793 - val_acc: 0.5421\n",
      "Epoch 10/30\n",
      "38952/38952 [==============================] - 70s 2ms/step - loss: 0.4371 - acc: 0.8788 - val_loss: 2.9167 - val_acc: 0.5524\n",
      "Epoch 11/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.3650 - acc: 0.8992 - val_loss: 2.9388 - val_acc: 0.5601\n",
      "Epoch 12/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.3176 - acc: 0.9146 - val_loss: 3.1379 - val_acc: 0.5444\n",
      "Epoch 13/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.2754 - acc: 0.9275 - val_loss: 3.3533 - val_acc: 0.5323\n",
      "Epoch 14/30\n",
      "38952/38952 [==============================] - 69s 2ms/step - loss: 0.2434 - acc: 0.9352 - val_loss: 3.3753 - val_acc: 0.5490\n",
      "Epoch 15/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.2198 - acc: 0.9433 - val_loss: 3.4819 - val_acc: 0.5580\n",
      "Epoch 16/30\n",
      "38952/38952 [==============================] - 72s 2ms/step - loss: 0.1988 - acc: 0.9488 - val_loss: 3.5954 - val_acc: 0.5645\n",
      "Epoch 17/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.1840 - acc: 0.9533 - val_loss: 3.7380 - val_acc: 0.5467\n",
      "Epoch 18/30\n",
      "38952/38952 [==============================] - 75s 2ms/step - loss: 0.1731 - acc: 0.9569 - val_loss: 3.8484 - val_acc: 0.5437\n",
      "Epoch 19/30\n",
      "38952/38952 [==============================] - 69s 2ms/step - loss: 0.1630 - acc: 0.9609 - val_loss: 3.7919 - val_acc: 0.5591\n",
      "Epoch 20/30\n",
      "38952/38952 [==============================] - 70s 2ms/step - loss: 0.1532 - acc: 0.9646 - val_loss: 4.0630 - val_acc: 0.5363\n",
      "Epoch 21/30\n",
      "38952/38952 [==============================] - 72s 2ms/step - loss: 0.1532 - acc: 0.9650 - val_loss: 3.9109 - val_acc: 0.5494\n",
      "Epoch 22/30\n",
      "38952/38952 [==============================] - 69s 2ms/step - loss: 0.1398 - acc: 0.9682 - val_loss: 4.1283 - val_acc: 0.5379\n",
      "Epoch 23/30\n",
      "38952/38952 [==============================] - 69s 2ms/step - loss: 0.1380 - acc: 0.9688 - val_loss: 4.0911 - val_acc: 0.5504\n",
      "Epoch 24/30\n",
      "38952/38952 [==============================] - 67s 2ms/step - loss: 0.1344 - acc: 0.9696 - val_loss: 4.2097 - val_acc: 0.5319\n",
      "Epoch 25/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.1302 - acc: 0.9708 - val_loss: 4.1674 - val_acc: 0.5471\n",
      "Epoch 26/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.1234 - acc: 0.9729 - val_loss: 4.1793 - val_acc: 0.5464\n",
      "Epoch 27/30\n",
      "38952/38952 [==============================] - 69s 2ms/step - loss: 0.1293 - acc: 0.9721 - val_loss: 4.1944 - val_acc: 0.5411\n",
      "Epoch 28/30\n",
      "38952/38952 [==============================] - 67s 2ms/step - loss: 0.1202 - acc: 0.9747 - val_loss: 4.3959 - val_acc: 0.5356\n",
      "Epoch 29/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.1220 - acc: 0.9747 - val_loss: 4.4272 - val_acc: 0.5367\n",
      "Epoch 30/30\n",
      "38952/38952 [==============================] - 68s 2ms/step - loss: 0.1200 - acc: 0.9748 - val_loss: 4.4988 - val_acc: 0.5377\n"
     ]
    }
   ],
   "source": [
    "cnn_model.train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.69      0.58       335\n",
      "           1       0.59      0.56      0.57        41\n",
      "           2       0.52      0.41      0.45        79\n",
      "           3       0.39      0.36      0.37        39\n",
      "           4       0.38      0.23      0.29        22\n",
      "           5       0.55      0.61      0.58       106\n",
      "           6       0.28      0.37      0.31       142\n",
      "           7       0.33      0.61      0.43        62\n",
      "           8       0.60      0.30      0.40        30\n",
      "           9       0.50      0.36      0.42        22\n",
      "          10       0.71      0.76      0.74       144\n",
      "          11       0.25      0.14      0.18        29\n",
      "          12       0.47      0.36      0.41        22\n",
      "          13       0.37      0.53      0.44        45\n",
      "          14       0.13      0.13      0.13        84\n",
      "          15       0.35      0.56      0.43        16\n",
      "          16       0.71      0.57      0.63       122\n",
      "          17       0.25      0.12      0.17        16\n",
      "          18       0.29      0.34      0.31        53\n",
      "          19       0.33      0.16      0.21        19\n",
      "          20       0.08      0.12      0.10        16\n",
      "          21       0.41      0.29      0.34        24\n",
      "          22       0.59      0.42      0.49       194\n",
      "          23       0.76      0.73      0.74        51\n",
      "          24       0.44      0.43      0.43        47\n",
      "          25       0.39      0.39      0.39        28\n",
      "          26       0.40      0.16      0.23        25\n",
      "          27       0.73      0.68      0.71       943\n",
      "          28       0.46      0.50      0.48        56\n",
      "          29       0.26      0.29      0.27        63\n",
      "          30       0.19      0.18      0.18        44\n",
      "          31       0.00      0.00      0.00        25\n",
      "          32       0.67      0.61      0.64        95\n",
      "          33       0.77      0.71      0.74       129\n",
      "          34       0.43      0.44      0.43        45\n",
      "          35       0.13      0.23      0.17        35\n",
      "          36       0.29      0.26      0.27        50\n",
      "          37       0.23      0.20      0.21        40\n",
      "          38       0.67      0.56      0.61        39\n",
      "          39       0.40      0.43      0.42        44\n",
      "          40       0.69      0.62      0.65       115\n",
      "          41       0.59      0.48      0.53       143\n",
      "          42       0.57      0.63      0.60        62\n",
      "          43       0.25      0.39      0.30        41\n",
      "          44       0.23      0.24      0.23        29\n",
      "          45       0.35      0.39      0.37        71\n",
      "          46       0.49      0.45      0.47        56\n",
      "          47       0.31      0.24      0.27        21\n",
      "          48       0.14      0.19      0.16        16\n",
      "          49       0.37      0.37      0.37        19\n",
      "          50       0.75      0.41      0.53        44\n",
      "          51       0.14      0.20      0.17        20\n",
      "          52       0.14      0.19      0.16        27\n",
      "          53       0.53      0.47      0.50        53\n",
      "          54       0.29      0.28      0.29        18\n",
      "          55       0.22      0.19      0.20        26\n",
      "          56       0.35      0.57      0.43        28\n",
      "          57       0.61      0.60      0.60       108\n",
      "          58       0.82      0.60      0.70       366\n",
      "          59       0.41      0.19      0.26        74\n",
      "          60       0.16      0.21      0.18        34\n",
      "          61       0.37      0.29      0.33        24\n",
      "          62       0.41      0.41      0.41        95\n",
      "          63       0.18      0.11      0.14        18\n",
      "          64       0.54      0.70      0.61        37\n",
      "          65       0.10      0.33      0.16        15\n",
      "          66       0.33      0.21      0.26        33\n",
      "          67       0.61      0.65      0.63       602\n",
      "          68       0.63      0.51      0.57       190\n",
      "          69       0.30      0.25      0.27       105\n",
      "          70       0.43      0.56      0.48        36\n",
      "          71       0.41      0.42      0.41        48\n",
      "          72       0.21      0.23      0.22        22\n",
      "          73       0.43      0.36      0.39        33\n",
      "          74       0.76      0.84      0.79        85\n",
      "          75       0.58      0.50      0.54        30\n",
      "          76       0.57      0.46      0.51        50\n",
      "          77       0.63      0.60      0.62        43\n",
      "          78       0.41      0.60      0.48       104\n",
      "          79       0.38      0.50      0.43       391\n",
      "          80       0.00      0.00      0.00        17\n",
      "          81       0.76      0.68      0.72        28\n",
      "          82       0.15      0.30      0.20        20\n",
      "          83       0.15      0.17      0.16        24\n",
      "          84       0.21      0.45      0.29        53\n",
      "          85       0.50      0.39      0.44        23\n",
      "          86       0.23      0.23      0.23        74\n",
      "          87       0.29      0.41      0.34        34\n",
      "          88       0.57      0.48      0.52       297\n",
      "          89       0.62      0.25      0.36        40\n",
      "          90       0.06      0.12      0.08        16\n",
      "          91       0.31      0.23      0.26        39\n",
      "          92       0.21      0.23      0.22        26\n",
      "          93       0.68      0.68      0.68        22\n",
      "          94       0.05      0.10      0.06        20\n",
      "          95       0.17      0.14      0.15        42\n",
      "          96       0.56      0.43      0.49        21\n",
      "          97       0.39      0.50      0.44        24\n",
      "          98       0.20      0.32      0.25        25\n",
      "          99       0.36      0.56      0.44        34\n",
      "         100       0.23      0.47      0.31        55\n",
      "         101       0.64      0.52      0.57       329\n",
      "         102       0.76      0.81      0.79        75\n",
      "         103       0.22      0.37      0.27        19\n",
      "         104       0.49      0.68      0.57        38\n",
      "         105       0.28      0.37      0.32        87\n",
      "         106       0.11      0.21      0.15        47\n",
      "         107       0.31      0.16      0.21        32\n",
      "         108       0.24      0.57      0.34        21\n",
      "         109       0.38      0.54      0.44       155\n",
      "         110       0.21      0.32      0.26        66\n",
      "         111       0.46      0.65      0.54        20\n",
      "         112       0.33      0.47      0.39        80\n",
      "         113       0.37      0.61      0.46        56\n",
      "         114       0.47      0.58      0.52       137\n",
      "         115       0.05      0.06      0.05        17\n",
      "         116       0.58      0.67      0.62       129\n",
      "         117       0.09      0.15      0.11        78\n",
      "         118       0.43      0.23      0.30        39\n",
      "         119       0.54      0.43      0.48        47\n",
      "         120       0.50      0.36      0.42        14\n",
      "         121       0.66      0.57      0.61       377\n",
      "         122       0.73      0.58      0.65        38\n",
      "         123       0.37      0.28      0.32        78\n",
      "         124       0.15      0.12      0.13        25\n",
      "         125       0.19      0.30      0.23        73\n",
      "         126       0.46      0.30      0.36        20\n",
      "         127       0.84      0.75      0.79       660\n",
      "         128       0.26      0.29      0.28        82\n",
      "         129       0.30      0.15      0.20        20\n",
      "         130       0.29      0.22      0.25        18\n",
      "         131       0.31      0.25      0.28       137\n",
      "         132       0.22      0.28      0.25        43\n",
      "         133       0.47      0.27      0.34        55\n",
      "         134       0.40      0.53      0.46        64\n",
      "         135       0.14      0.33      0.20        57\n",
      "         136       0.51      0.45      0.48        96\n",
      "         137       0.42      0.59      0.49        61\n",
      "         138       0.86      0.79      0.82      1969\n",
      "         139       0.59      0.61      0.60        54\n",
      "         140       0.33      0.25      0.29        16\n",
      "         141       0.23      0.44      0.30        25\n",
      "         142       0.12      0.24      0.16        17\n",
      "         143       0.33      0.26      0.29       298\n",
      "         144       0.09      0.29      0.14        41\n",
      "         145       0.72      0.60      0.66        83\n",
      "         146       0.50      0.48      0.49        25\n",
      "         147       0.05      0.08      0.06        25\n",
      "         148       0.44      0.68      0.53       209\n",
      "         149       0.21      0.33      0.26        15\n",
      "         150       0.33      0.34      0.34        61\n",
      "         151       0.19      0.40      0.26        20\n",
      "         152       0.20      0.20      0.20        20\n",
      "         153       0.22      0.38      0.28        34\n",
      "         154       0.81      0.66      0.73       144\n",
      "         155       0.61      0.47      0.53       387\n",
      "         156       0.41      0.23      0.30        56\n",
      "         157       0.12      0.12      0.12        24\n",
      "         158       0.29      0.39      0.34        51\n",
      "         159       0.33      0.35      0.34        23\n",
      "         160       0.33      0.27      0.30        44\n",
      "         161       0.66      0.50      0.57       240\n",
      "         162       0.14      0.19      0.16        27\n",
      "         163       0.38      0.26      0.31        19\n",
      "         164       0.43      0.31      0.36       161\n",
      "         165       0.38      0.42      0.40        66\n",
      "         166       0.30      0.53      0.39        70\n",
      "         167       0.80      0.59      0.68        61\n",
      "         168       0.25      0.26      0.26        92\n",
      "         169       0.40      0.65      0.49        34\n",
      "         170       0.36      0.43      0.40        37\n",
      "         171       0.48      0.48      0.48       223\n",
      "         172       0.06      0.08      0.07        24\n",
      "         173       0.78      0.69      0.73       248\n",
      "         174       0.63      0.52      0.57       191\n",
      "         175       0.28      0.23      0.25        53\n",
      "         176       0.70      0.60      0.65       236\n",
      "         177       0.18      0.44      0.25        39\n",
      "         178       0.79      0.67      0.72       201\n",
      "         179       0.38      0.32      0.35        31\n",
      "         180       0.73      0.56      0.64      1088\n",
      "         181       0.36      0.43      0.39       185\n",
      "         182       0.35      0.39      0.37        28\n",
      "         183       0.67      0.47      0.55       359\n",
      "         184       0.47      0.47      0.47        19\n",
      "         185       0.19      0.20      0.19        35\n",
      "         186       0.52      0.36      0.42        39\n",
      "         187       0.48      0.40      0.43        25\n",
      "\n",
      "   micro avg       0.53      0.53      0.53     18549\n",
      "   macro avg       0.40      0.40      0.39     18549\n",
      "weighted avg       0.57      0.53      0.54     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision (macro avg)</th>\n",
       "      <th>recall (macro avg)</th>\n",
       "      <th>f1 (macro avg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN+vectors sum</td>\n",
       "      <td>0.460025</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logreg+vectors sum</td>\n",
       "      <td>0.521106</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kNN+doc2vec</td>\n",
       "      <td>0.592808</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg+doc2vec</td>\n",
       "      <td>0.598954</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FNN+vectors sum</td>\n",
       "      <td>0.557820</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FNN+doc2vec</td>\n",
       "      <td>0.616260</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.623699</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.531835</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  accuracy precision (macro avg) recall (macro avg)  \\\n",
       "0     kNN+vectors sum  0.460025                  0.40               0.35   \n",
       "1  logreg+vectors sum  0.521106                  0.42               0.42   \n",
       "2         kNN+doc2vec  0.592808                  0.51               0.50   \n",
       "3      logreg+doc2vec  0.598954                  0.58               0.47   \n",
       "4     FNN+vectors sum  0.557820                  0.43               0.37   \n",
       "5         FNN+doc2vec  0.616260                  0.60               0.49   \n",
       "6                LSTM  0.623699                  0.50               0.46   \n",
       "7                 CNN  0.531835                  0.40               0.40   \n",
       "\n",
       "  f1 (macro avg)  \n",
       "0           0.36  \n",
       "1           0.41  \n",
       "2           0.49  \n",
       "3           0.50  \n",
       "4           0.39  \n",
       "5           0.52  \n",
       "6           0.47  \n",
       "7           0.39  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(model):\n",
    "    return accuracy_score(model.test_labels, model.topics_predicted)\n",
    "\n",
    "def extract_metrics(model, group = 'macro avg'): \n",
    "    metrics = [line.strip() for line in  model.test_report.split('\\n') if group in line][0]\n",
    "    return [metric.strip() for metric in metrics[len(group)-1:].split()][1:-1]\n",
    "\n",
    "pandas.DataFrame({\n",
    "    'model': ['kNN+vectors sum',\n",
    "              'logreg+vectors sum',\n",
    "              'kNN+doc2vec',\n",
    "              'logreg+doc2vec',\n",
    "              'FNN+vectors sum',\n",
    "              'FNN+doc2vec',\n",
    "              'LSTM',\n",
    "              'CNN'],\n",
    "    'accuracy': [accuracy(knn),\n",
    "                 accuracy(logreg),\n",
    "                 accuracy(knn2),\n",
    "                 accuracy(logreg2),\n",
    "                 accuracy(ff_sum_model),\n",
    "                 accuracy(ff_model),\n",
    "                 accuracy(lstm_model),\n",
    "                 accuracy(cnn_model)],\n",
    "    'precision (macro avg)': [extract_metrics(knn)[0],\n",
    "                              extract_metrics(logreg)[0],\n",
    "                              extract_metrics(knn2)[0],\n",
    "                              extract_metrics(logreg2)[0],\n",
    "                              extract_metrics(ff_sum_model)[0],\n",
    "                              extract_metrics(ff_model)[0],\n",
    "                              extract_metrics(lstm_model)[0],\n",
    "                              extract_metrics(cnn_model)[0]],\n",
    "    'recall (macro avg)': [extract_metrics(knn)[1],\n",
    "                              extract_metrics(logreg)[1],\n",
    "                              extract_metrics(knn2)[1],\n",
    "                              extract_metrics(logreg2)[1],\n",
    "                              extract_metrics(ff_sum_model)[1],\n",
    "                              extract_metrics(ff_model)[1],\n",
    "                              extract_metrics(lstm_model)[1],\n",
    "                              extract_metrics(cnn_model)[1]],\n",
    "    'f1 (macro avg)': [extract_metrics(knn)[2],\n",
    "                              extract_metrics(logreg)[2],\n",
    "                              extract_metrics(knn2)[2],\n",
    "                              extract_metrics(logreg2)[2],\n",
    "                              extract_metrics(ff_sum_model)[2],\n",
    "                              extract_metrics(ff_model)[2],\n",
    "                              extract_metrics(lstm_model)[2],\n",
    "                              extract_metrics(cnn_model)[2]]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо що вдалося покращити якість у порівнянні з бейзлайном більш ніж на 10% згідно F1. Логістична регресія в порівнянні з kNN у всіх випадках працювала краще. Вектори документів також дали покращення у всіх випадках. Найкращий результат дало застосування feed forward мережі в комбінації с Doc2Vec, дуже близький результат дала LSTM мережа. Цікаво що аналогічні результати мают згорточна мереже і логістична мережа на сумі векторів. Виглядає так что згортка має той самий єфект що й сумма ембедінгів."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
