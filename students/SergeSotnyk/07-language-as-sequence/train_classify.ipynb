{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/nyt2000-sents.jsonl'\n",
    "train_fn = filename.replace('.jsonl', '.train.jsonl')\n",
    "dev_fn = filename.replace('.jsonl', '.dev.jsonl')\n",
    "\n",
    "test_fn = 'data/run-on-test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "from jsonlines import jsonlines\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load spacy...\n",
      "...Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Load spacy...\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#nlp.remove_pipe('ner')\n",
    "#nlp.remove_pipe('tagger')\n",
    "print(\"...Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_x_y_jsonl(filename, max_num=1000000):\n",
    "    buffer = []\n",
    "    with jsonlines.open(filename) as reader:\n",
    "        for sentence in reader:\n",
    "            buffer.append(sentence)\n",
    "            if len(buffer)>max_num:\n",
    "                break\n",
    "    x, y = [], []\n",
    "    for sent in buffer:\n",
    "        x.append([x for x, y in sent])\n",
    "        y.append([y for x, y in sent])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared train and dev datasets are too long for my notebook, so I use reduced ones (the third parameter in read_x_y_jsonl function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'was', 'told', 'that', 'John', 'mixed', 'his', 'last', 'drink', 'last', 'year', '.'], ['With', 'the', 'theme', 'of', '\"', 'Leave', 'No', 'Child', 'Behind', ',', '\"', 'and', 'people', 'of', 'all', 'races', 'turning', 'the', 'convention', 'stage', 'into', 'Mr.', 'Bush', \"'s\", 'Neighborhood', ',', 'and', 'lines', 'like', '\"', 'We', 'Shall', 'Overcome', '\"', 'in', 'his', 'speech', ',', 'Mr.', 'Bush', 'left', 'moderates', 'feeling', 'as', 'if', 'he', 'had', 'unleashed', 'the', 'party', \"'s\", 'inner', 'Connecticut', 'Yankee', '.']]\n",
      "[[False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]]\n"
     ]
    }
   ],
   "source": [
    "x_train_tokens, y_train = read_x_y_jsonl(train_fn, 50000)\n",
    "print(x_train_tokens[:2])\n",
    "print(y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_tokens, y_dev = read_x_y_jsonl(dev_fn, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_x_y_json(filename):\n",
    "    with open(filename, 'rt', encoding='utf-8') as f:\n",
    "        js = json.loads(f.read())\n",
    "    tokens, flags = [], []\n",
    "    for sent in js:\n",
    "        ts, fs = [], []\n",
    "        for t, f in sent:\n",
    "            ts.append(t)\n",
    "            fs.append(f)\n",
    "        tokens.append(ts)\n",
    "        flags.append(fs)\n",
    "    return tokens, flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens, y_test = read_x_y_json(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'think', 'the', 'magnitude', 'of', 'a', 'benefit', 'and', 'error', 'rates', 'that', 'were', 'chosen', 'were', 'reasonable', 'They', 'were', 'standard', 'from', 'our', 'learning', '.'], ['Economists', 'on', 'both', 'the', 'left', 'and', 'right', 'broadly', 'agree', 'that', 'the', 'need', 'for', 'stimulative', 'government', 'spending', 'is', 'necessary', 'to', 'prevent', 'a', 'further', 'collapse', 'of', 'the', 'global', 'economic', 'system', '-', 'just', 'as', 'the', 'New', 'Deal', 'and', 'the', 'deficit', 'spending', 'of', 'World', 'War', 'II', 'restored', 'the', 'health', 'of', 'the', 'global', 'economy', 'in', 'the', 'last', 'century', '.']]\n",
      "[[False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test_tokens[:2])\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_colloc_bigrams():\n",
    "    bi_name = 'data/bigrams.jsonl'\n",
    "    with jsonlines.open(bi_name, 'r') as r:\n",
    "        res = {}\n",
    "        for line in r:\n",
    "            res[line[0]]=line[1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = load_colloc_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities as features\n",
    "It was not useful (difference was in the third digit after decimal point), but interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ent_type': 'GPE', 'ent_iob': 'B'}\n",
      "{'+1:ent_type': 'GPE', '+1:ent_iob': 'I'}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "def retrieve_entity_features(i: int, sentence: spacy.tokens.Doc, feature_prefix=''):\n",
    "    d = {}\n",
    "    for e in sentence.ents:\n",
    "        d.update({t.i:t for t in e})\n",
    "    t = d.get(i, None)\n",
    "    if t:\n",
    "        return {feature_prefix+'ent_type':t.ent_type_, feature_prefix+'ent_iob':t.ent_iob_}\n",
    "    return {}\n",
    "    print(d)\n",
    "    \n",
    "_doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "print(retrieve_entity_features(0, _doc))\n",
    "print(retrieve_entity_features(1, _doc, '+1:'))\n",
    "print(retrieve_entity_features(2, _doc))\n",
    "del(_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(tokens, i, sentence: spacy.tokens.Doc):\n",
    "    word = tokens[i]    \n",
    "    # print(word)\n",
    "    features = {\n",
    "        #'bias': 1.0,\n",
    "        #'word.lower()': word.lower(),\n",
    "        #'word[-3:]': word[-3:],\n",
    "        #'word[-2:]': word[-2:],\n",
    "        #'word[-1:]': word[-1:],\n",
    "        #'word[:3]': word[:3],\n",
    "        #'word[:2]': word[:2],\n",
    "        #'word[:1]': word[:1],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'i': i,\n",
    "        '~i': len(tokens)-i,\n",
    "        'word.lemma': sentence[i].lemma_,\n",
    "        'pos': sentence[i].pos_,\n",
    "        'dep': sentence[i].dep_,\n",
    "        'n_lefts': sentence[i].n_lefts,\n",
    "        'n_rights': sentence[i].n_rights,\n",
    "    }\n",
    "    #features.update(retrieve_entity_features(i, sentence))\n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = tokens[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:word.lemma': sentence[i-1].lemma_,\n",
    "            '-1:pos': sentence[i-1].pos_,\n",
    "            '-1:dep': sentence[i-1].dep_,\n",
    "        })\n",
    "        #features.update(retrieve_entity_features(i-1, sentence, '-1'))\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i > 1:\n",
    "        word2 = tokens[i-2]\n",
    "        features.update({\n",
    "            '-2:word.lower()': word2.lower(),\n",
    "            '-2:word.istitle()': word2.istitle(),\n",
    "            '-2:word.isupper()': word2.isupper(),\n",
    "            '-2:word.lemma': sentence[i-2].lemma_,\n",
    "            '-2:pos': sentence[i-2].pos_,\n",
    "            '-2:dep': sentence[i-2].dep_,\n",
    "        })\n",
    "        #features.update(retrieve_entity_features(i-2, sentence, '-2'))\n",
    "    else:\n",
    "        features['BOS2'] = True\n",
    "\n",
    "        \n",
    "    if i < len(tokens)-1:\n",
    "        word1 = tokens[i+1]\n",
    "        bigram_key = word.lower()+'_'+word1.lower()\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:word.lemma': sentence[i+1].lemma_,\n",
    "            '+1:pos': sentence[i+1].pos_,\n",
    "            '+1:dep': sentence[i+1].dep_,\n",
    "            'has_bigram20': bigrams.get(bigram_key, 0)>20,\n",
    "            'has_bigram5': bigrams.get(bigram_key, 0)>5,\n",
    "            'has_bigram1': bigrams.get(bigram_key, 0)>1,\n",
    "            'bigram_counter': bigrams.get(bigram_key, 0),\n",
    "        })\n",
    "        #features.update(retrieve_entity_features(i+1, sentence, '+1'))\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    if i < len(tokens)-2:\n",
    "        word2 = tokens[i+2]\n",
    "        features.update({\n",
    "            '+2:word.lower()': word2.lower(),\n",
    "            '+2:word.istitle()': word2.istitle(),\n",
    "            '+2:word.isupper()': word2.isupper(),\n",
    "            '+2:word.lemma': sentence[i+2].lemma_,\n",
    "            '+2:pos': sentence[i+2].pos_,\n",
    "            '+2:dep': sentence[i+2].dep_,\n",
    "        })\n",
    "        #features.update(retrieve_entity_features(i+2, sentence, '+2'))\n",
    "    else:\n",
    "        features['EOS2'] = True\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2features(sentences):\n",
    "    res = []\n",
    "    for sent in tqdm(sentences, total=len(sentences)):\n",
    "        doc = nlp(' '.join(sent))\n",
    "        res.append([word2features(sent, i, doc) for i in range(len(sent))])\n",
    "    return res\n",
    "\n",
    "def flat_list(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6690b307504a39bc310b3d619f5113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'word.isupper()': True,\n",
       "   'word.istitle()': True,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 0,\n",
       "   '~i': 12,\n",
       "   'word.lemma': '-PRON-',\n",
       "   'pos': 'PRON',\n",
       "   'dep': 'nsubjpass',\n",
       "   'n_lefts': 0,\n",
       "   'n_rights': 0,\n",
       "   'BOS': True,\n",
       "   'BOS2': True,\n",
       "   '+1:word.lower()': 'was',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'be',\n",
       "   '+1:pos': 'VERB',\n",
       "   '+1:dep': 'auxpass',\n",
       "   'has_bigram20': True,\n",
       "   'has_bigram5': True,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 11665,\n",
       "   '+2:word.lower()': 'told',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': 'tell',\n",
       "   '+2:pos': 'VERB',\n",
       "   '+2:dep': 'ROOT'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 1,\n",
       "   '~i': 11,\n",
       "   'word.lemma': 'be',\n",
       "   'pos': 'VERB',\n",
       "   'dep': 'auxpass',\n",
       "   'n_lefts': 0,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'i',\n",
       "   '-1:word.istitle()': True,\n",
       "   '-1:word.isupper()': True,\n",
       "   '-1:word.lemma': '-PRON-',\n",
       "   '-1:pos': 'PRON',\n",
       "   '-1:dep': 'nsubjpass',\n",
       "   'BOS2': True,\n",
       "   '+1:word.lower()': 'told',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'tell',\n",
       "   '+1:pos': 'VERB',\n",
       "   '+1:dep': 'ROOT',\n",
       "   'has_bigram20': True,\n",
       "   'has_bigram5': True,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 605,\n",
       "   '+2:word.lower()': 'that',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': 'that',\n",
       "   '+2:pos': 'ADP',\n",
       "   '+2:dep': 'mark'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 2,\n",
       "   '~i': 10,\n",
       "   'word.lemma': 'tell',\n",
       "   'pos': 'VERB',\n",
       "   'dep': 'ROOT',\n",
       "   'n_lefts': 2,\n",
       "   'n_rights': 2,\n",
       "   '-1:word.lower()': 'was',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'be',\n",
       "   '-1:pos': 'VERB',\n",
       "   '-1:dep': 'auxpass',\n",
       "   '-2:word.lower()': 'i',\n",
       "   '-2:word.istitle()': True,\n",
       "   '-2:word.isupper()': True,\n",
       "   '-2:word.lemma': '-PRON-',\n",
       "   '-2:pos': 'PRON',\n",
       "   '-2:dep': 'nsubjpass',\n",
       "   '+1:word.lower()': 'that',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'that',\n",
       "   '+1:pos': 'ADP',\n",
       "   '+1:dep': 'mark',\n",
       "   'has_bigram20': True,\n",
       "   'has_bigram5': True,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 568,\n",
       "   '+2:word.lower()': 'john',\n",
       "   '+2:word.istitle()': True,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': 'John',\n",
       "   '+2:pos': 'PROPN',\n",
       "   '+2:dep': 'nsubj'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 3,\n",
       "   '~i': 9,\n",
       "   'word.lemma': 'that',\n",
       "   'pos': 'ADP',\n",
       "   'dep': 'mark',\n",
       "   'n_lefts': 0,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'told',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'tell',\n",
       "   '-1:pos': 'VERB',\n",
       "   '-1:dep': 'ROOT',\n",
       "   '-2:word.lower()': 'was',\n",
       "   '-2:word.istitle()': False,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': 'be',\n",
       "   '-2:pos': 'VERB',\n",
       "   '-2:dep': 'auxpass',\n",
       "   '+1:word.lower()': 'john',\n",
       "   '+1:word.istitle()': True,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'John',\n",
       "   '+1:pos': 'PROPN',\n",
       "   '+1:dep': 'nsubj',\n",
       "   'has_bigram20': True,\n",
       "   'has_bigram5': True,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 126,\n",
       "   '+2:word.lower()': 'mixed',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': 'mix',\n",
       "   '+2:pos': 'VERB',\n",
       "   '+2:dep': 'ccomp'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': True,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 4,\n",
       "   '~i': 8,\n",
       "   'word.lemma': 'John',\n",
       "   'pos': 'PROPN',\n",
       "   'dep': 'nsubj',\n",
       "   'n_lefts': 0,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'that',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'that',\n",
       "   '-1:pos': 'ADP',\n",
       "   '-1:dep': 'mark',\n",
       "   '-2:word.lower()': 'told',\n",
       "   '-2:word.istitle()': False,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': 'tell',\n",
       "   '-2:pos': 'VERB',\n",
       "   '-2:dep': 'ROOT',\n",
       "   '+1:word.lower()': 'mixed',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'mix',\n",
       "   '+1:pos': 'VERB',\n",
       "   '+1:dep': 'ccomp',\n",
       "   'has_bigram20': False,\n",
       "   'has_bigram5': False,\n",
       "   'has_bigram1': False,\n",
       "   'bigram_counter': 0,\n",
       "   '+2:word.lower()': 'his',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': '-PRON-',\n",
       "   '+2:pos': 'DET',\n",
       "   '+2:dep': 'poss'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 5,\n",
       "   '~i': 7,\n",
       "   'word.lemma': 'mix',\n",
       "   'pos': 'VERB',\n",
       "   'dep': 'ccomp',\n",
       "   'n_lefts': 2,\n",
       "   'n_rights': 2,\n",
       "   '-1:word.lower()': 'john',\n",
       "   '-1:word.istitle()': True,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'John',\n",
       "   '-1:pos': 'PROPN',\n",
       "   '-1:dep': 'nsubj',\n",
       "   '-2:word.lower()': 'that',\n",
       "   '-2:word.istitle()': False,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': 'that',\n",
       "   '-2:pos': 'ADP',\n",
       "   '-2:dep': 'mark',\n",
       "   '+1:word.lower()': 'his',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': '-PRON-',\n",
       "   '+1:pos': 'DET',\n",
       "   '+1:dep': 'poss',\n",
       "   'has_bigram20': False,\n",
       "   'has_bigram5': True,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 8,\n",
       "   '+2:word.lower()': 'last',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': 'last',\n",
       "   '+2:pos': 'ADJ',\n",
       "   '+2:dep': 'amod'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 6,\n",
       "   '~i': 6,\n",
       "   'word.lemma': '-PRON-',\n",
       "   'pos': 'DET',\n",
       "   'dep': 'poss',\n",
       "   'n_lefts': 0,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'mixed',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'mix',\n",
       "   '-1:pos': 'VERB',\n",
       "   '-1:dep': 'ccomp',\n",
       "   '-2:word.lower()': 'john',\n",
       "   '-2:word.istitle()': True,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': 'John',\n",
       "   '-2:pos': 'PROPN',\n",
       "   '-2:dep': 'nsubj',\n",
       "   '+1:word.lower()': 'last',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'last',\n",
       "   '+1:pos': 'ADJ',\n",
       "   '+1:dep': 'amod',\n",
       "   'has_bigram20': True,\n",
       "   'has_bigram5': True,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 1202,\n",
       "   '+2:word.lower()': 'drink',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': 'drink',\n",
       "   '+2:pos': 'NOUN',\n",
       "   '+2:dep': 'dobj'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 7,\n",
       "   '~i': 5,\n",
       "   'word.lemma': 'last',\n",
       "   'pos': 'ADJ',\n",
       "   'dep': 'amod',\n",
       "   'n_lefts': 0,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'his',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': '-PRON-',\n",
       "   '-1:pos': 'DET',\n",
       "   '-1:dep': 'poss',\n",
       "   '-2:word.lower()': 'mixed',\n",
       "   '-2:word.istitle()': False,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': 'mix',\n",
       "   '-2:pos': 'VERB',\n",
       "   '-2:dep': 'ccomp',\n",
       "   '+1:word.lower()': 'drink',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'drink',\n",
       "   '+1:pos': 'NOUN',\n",
       "   '+1:dep': 'dobj',\n",
       "   'has_bigram20': False,\n",
       "   'has_bigram5': False,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 2,\n",
       "   '+2:word.lower()': 'last',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': 'last',\n",
       "   '+2:pos': 'ADJ',\n",
       "   '+2:dep': 'amod'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 8,\n",
       "   '~i': 4,\n",
       "   'word.lemma': 'drink',\n",
       "   'pos': 'NOUN',\n",
       "   'dep': 'dobj',\n",
       "   'n_lefts': 2,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'last',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'last',\n",
       "   '-1:pos': 'ADJ',\n",
       "   '-1:dep': 'amod',\n",
       "   '-2:word.lower()': 'his',\n",
       "   '-2:word.istitle()': False,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': '-PRON-',\n",
       "   '-2:pos': 'DET',\n",
       "   '-2:dep': 'poss',\n",
       "   '+1:word.lower()': 'last',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'last',\n",
       "   '+1:pos': 'ADJ',\n",
       "   '+1:dep': 'amod',\n",
       "   'has_bigram20': False,\n",
       "   'has_bigram5': False,\n",
       "   'has_bigram1': False,\n",
       "   'bigram_counter': 0,\n",
       "   '+2:word.lower()': 'year',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': 'year',\n",
       "   '+2:pos': 'NOUN',\n",
       "   '+2:dep': 'npadvmod'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 9,\n",
       "   '~i': 3,\n",
       "   'word.lemma': 'last',\n",
       "   'pos': 'ADJ',\n",
       "   'dep': 'amod',\n",
       "   'n_lefts': 0,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'drink',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'drink',\n",
       "   '-1:pos': 'NOUN',\n",
       "   '-1:dep': 'dobj',\n",
       "   '-2:word.lower()': 'last',\n",
       "   '-2:word.istitle()': False,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': 'last',\n",
       "   '-2:pos': 'ADJ',\n",
       "   '-2:dep': 'amod',\n",
       "   '+1:word.lower()': 'year',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': 'year',\n",
       "   '+1:pos': 'NOUN',\n",
       "   '+1:dep': 'npadvmod',\n",
       "   'has_bigram20': True,\n",
       "   'has_bigram5': True,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 15449,\n",
       "   '+2:word.lower()': '.',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:word.lemma': '.',\n",
       "   '+2:pos': 'PUNCT',\n",
       "   '+2:dep': 'punct'},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 10,\n",
       "   '~i': 2,\n",
       "   'word.lemma': 'year',\n",
       "   'pos': 'NOUN',\n",
       "   'dep': 'npadvmod',\n",
       "   'n_lefts': 1,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'last',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'last',\n",
       "   '-1:pos': 'ADJ',\n",
       "   '-1:dep': 'amod',\n",
       "   '-2:word.lower()': 'drink',\n",
       "   '-2:word.istitle()': False,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': 'drink',\n",
       "   '-2:pos': 'NOUN',\n",
       "   '-2:dep': 'dobj',\n",
       "   '+1:word.lower()': '.',\n",
       "   '+1:word.istitle()': False,\n",
       "   '+1:word.isupper()': False,\n",
       "   '+1:word.lemma': '.',\n",
       "   '+1:pos': 'PUNCT',\n",
       "   '+1:dep': 'punct',\n",
       "   'has_bigram20': True,\n",
       "   'has_bigram5': True,\n",
       "   'has_bigram1': True,\n",
       "   'bigram_counter': 12386,\n",
       "   'EOS2': True},\n",
       "  {'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'i': 11,\n",
       "   '~i': 1,\n",
       "   'word.lemma': '.',\n",
       "   'pos': 'PUNCT',\n",
       "   'dep': 'punct',\n",
       "   'n_lefts': 0,\n",
       "   'n_rights': 0,\n",
       "   '-1:word.lower()': 'year',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:word.lemma': 'year',\n",
       "   '-1:pos': 'NOUN',\n",
       "   '-1:dep': 'npadvmod',\n",
       "   '-2:word.lower()': 'last',\n",
       "   '-2:word.istitle()': False,\n",
       "   '-2:word.isupper()': False,\n",
       "   '-2:word.lemma': 'last',\n",
       "   '-2:pos': 'ADJ',\n",
       "   '-2:dep': 'amod',\n",
       "   'EOS': True,\n",
       "   'EOS2': True}]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2features(x_train_tokens[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6a1bba44e54c40904a1cbfde15bd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfcdd7fa56c426b925c7672ed3fff72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbaf52e082be4bae97a257bada97b0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = tokens2features(x_train_tokens)\n",
    "x_dev = tokens2features(x_dev_tokens)\n",
    "x_test = tokens2features(x_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[3]))\n",
    "print(len(y_train[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "x_train_features = vectorizer.fit_transform(flat_list(x_train))\n",
    "x_dev_features = vectorizer.transform(flat_list(x_dev))\n",
    "x_test_features = vectorizer.transform(flat_list(x_test))\n",
    "\n",
    "\n",
    "\n",
    "y_train_flat = flat_list(y_train)\n",
    "y_dev_flat = flat_list(y_dev)\n",
    "y_test_flat = flat_list(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip to rare features. It adds more stability to our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectPercentile(percentile=50,\n",
       "         score_func=<function f_classif at 0x0000029AF64A2048>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectPercentile(percentile=50)\n",
    "selector.fit(x_train_features, y_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2675168\n",
      "(2675168, 713692)\n",
      "(2675168, 356846)\n",
      "2704413\n",
      "(2704413, 713692)\n",
      "4697\n",
      "(4697, 713692)\n"
     ]
    }
   ],
   "source": [
    "print(len(flat_list(x_train)))\n",
    "print(x_train_features.shape)\n",
    "print(selector.transform(x_train_features).shape)\n",
    "print(len(flat_list(x_dev)))\n",
    "print(x_dev_features.shape)\n",
    "print(len(flat_list(x_test)))\n",
    "print(x_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer = StandardScaler(with_mean=False)\n",
    "# x_train_norm = normalizer.fit_transform(x_train_features)\n",
    "# x_dev_norm = normalizer.transform(x_dev_features)\n",
    "# x_test_norm = normalizer.transform(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.5, class_weight={True: 1.5, False: 1}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=200,\n",
       "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier = RandomForestClassifier()\n",
    "\n",
    "classifier = LogisticRegression(C=1.5, verbose=1, max_iter=200,\n",
    "                                class_weight={True:1.5, False:1},\n",
    "                                #class_weight='balanced',\n",
    "                                solver='liblinear',\n",
    "                                #solver='newton-cg',\n",
    "                                #penalty='l2',\n",
    "                                penalty='l1',\n",
    "                                n_jobs=None\n",
    "                               )\n",
    "\n",
    "# classifier = svm.SVC(verbose=1, max_iter=1000)\n",
    "\n",
    "classifier.fit(selector.transform(x_train_features), y_train_flat)\n",
    "#classifier.fit(x_train_norm, y_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_pred = classifier.predict(selector.transform(x_dev_features))\n",
    "# y_dev_pred = classifier.predict(x_dev_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev set metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.992     0.996     0.994   2639384\n",
      "        True      0.790     0.680     0.731     65029\n",
      "\n",
      "   micro avg      0.988     0.988     0.988   2704413\n",
      "   macro avg      0.891     0.838     0.862   2704413\n",
      "weighted avg      0.987     0.988     0.988   2704413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(\n",
    "    y_dev_flat, y_dev_pred, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = classifier.predict(selector.transform(x_test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.990     0.987     0.989      4542\n",
      "        True      0.655     0.723     0.687       155\n",
      "\n",
      "   micro avg      0.978     0.978     0.978      4697\n",
      "   macro avg      0.823     0.855     0.838      4697\n",
      "weighted avg      0.979     0.978     0.979      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_test_flat, y_test_pred, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=112, FP=59, FN=43\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "tp, fp, fn = 0, 0, 0\n",
    "for y, p in zip(y_test_flat, y_test_pred):\n",
    "    if y and p:\n",
    "        tp +=1\n",
    "    if y and not p:\n",
    "        fn +=1\n",
    "    if not y and p:\n",
    "        fp +=1\n",
    "print(f\"TP={tp}, FP={fp}, FN={fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 positive and negative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14.254621473953462, '+1:word.lemma=Mezzanote'), (13.379201671360299, '-1:word.lemma=Bennie'), (11.459998353130892, '+1:word.lower()=balducci'), (10.764178454018191, '+1:word.lemma=Mildred'), (10.294213962157958, '-1:word.lemma=Birgitta'), (10.28332291972431, '-1:word.lemma=Benjamenta'), (10.245852488026124, '-1:word.lemma=Coester'), (9.989345017463922, '-1:word.lemma=IMC'), (9.936950231632403, '-1:word.lemma=Heaven'), (9.858583391263018, '-1:word.lemma=Maracaibo')]\n",
      "[(-6.395485295319845, '-1:word.lemma=Brick'), (-6.439546488849851, '-1:word.lemma=1.50-a'), (-6.467559908031927, '-1:word.lemma=Brickell'), (-6.5926181615164845, '-1:word.lemma=Parkville'), (-6.5978949207192, '+2:word.lower()=neidich'), (-6.739094633223976, '-1:word.lemma=Randt'), (-6.919732484067826, '+2:word.lemma=brio'), (-8.800076641512689, '+2:word.lemma=Ye'), (-9.138548564769167, '+1:word.lemma=smoked'), (-13.974373987120737, '-1:word.lemma=BRODEUR')]\n"
     ]
    }
   ],
   "source": [
    "# vectorizer.feature_names_\n",
    "# sorted(list(zip(classifier.feature_importances_, vectorizer.feature_names_)), reverse=True)\n",
    "imports = sorted(list(zip(classifier.coef_[0], vectorizer.feature_names_)), reverse=True)\n",
    "print(imports[:10])\n",
    "print(imports[-10:])\n",
    "#classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
