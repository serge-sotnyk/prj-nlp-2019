# Семантика

## Завдання

Одне із змагань на SemEval-2015 було пов'язане з пошуком перефразувань та схожих за змістом твітів - [Paraphrase and Semantic Similarity in Twitter](http://alt.qcri.org/semeval2015/task1/). Ваше завдання цього тижня - побудувати модель, яка буде визначати перефразування та/або семантичну подібність краще, ніж бейзлайн у цьому змаганні.

Побудуйте одне із запропонованих рішень для цієї задачі (на вибір):

1. Побудуйте класифікатор на лінгвістичних ознаках (сутності, енграми слів чи символів, корені слів, редакторська відстань, синтаксична та семантична подібність тощо). **Обов'язкова умова** - використання семантичних ознак. Наприклад:
   - перетин слів та їх синонімів, гіпонімів, пов'язаних за темою слів тощо
   - середня близькість речень по онтології
   - редакторська відстань по AMR-графу
   - збіги по семантичних ролях

2. Побудуйте класифікатор на основі ваших улюблений векторних представлень слів. Поекспериментуйте з різними способами агрегації, відбору та зважування векторів. **Обов'язкова умова** - використання семантично збагачених векторів (як основне рішення чи для порівняння). Можна взяти готові вектори (наприклад, [ConceptNet Numberbatch](https://github.com/commonsense/conceptnet-numberbatch)) або самостійно виконати [ретрофіттинг](https://github.com/mfaruqui/retrofitting/blob/master/retrofit.py) на будь-яких стандартних векторах.

Більше натхнення щодо ознак та варіантів побудови рішення можна черпати зі [звіту про змагання](https://www.aclweb.org/anthology/S15-2001).

### Дані

Використайте дані та метрики з репозиторію змагання SemEval-2015 (архів опубліковано через Слек). Вся інформація щодо формату корпусу та метрик міститься в Readme.md.

Запустити метрику можна таким способом:
```
$ python scripts/pit2015_eval_single.py data/test.label systemoutputs/PIT2015_BASELINE_02_LG.output
838     BASELINE        02_LG           F: 0.589        Prec: 0.679     Rec: 0.520              P-corr: 0.511   F1: 0.601       Prec: 0.674     Rec: 0.543
```

**Важливо**: не заточуйтесь на test-дані. У корпусі є окремо виділений dev-сет, на якому можна порівнювати ваші рішення.

## Оцінювання

За завдання можна отримати 100 балів.

Крайній термін: 18.05.2019

# Рішення

Рішення знаходиться у файлі [ss_classifier.ipynb](SemEval/scripts/ss_classifier.ipynb).

Задля класифікації, я підготував 4 набора фіч, які далі комбінував.

### 1. BERT
Прочитавши статтю https://habr.com/ru/post/436878/, мені здалося, що один з режимів 
роботи цієї моделі можна застосувати як генератор фіч. А саме режим, коли BERT вертає
ймовірність того, що один текст є продовженням другого. То ж, я підняв у себе цю 
модель, зробив код, який подає перший та тругий твіт на вхід, потим другий та перший,
а обидва результати зберігає.

Я недооцінив складність цієї моделі, то ж перший набір фіч обчислювався моїм ноутбуком
близько доби. І я був дуже розчарованним, коли ця фіча давала результат лише трохи 
краще за рандомний вибір. Я вже майже закинув роботу з нею, але таки знайшов, що
була помилка, через яку я кожен раз давав на розрахунок не два твіти, а лише перший.

Але, в кінці кінців, я трохи навчився робити із BERT, запустив його віддалено дома на 
машині з GPU, то ж швидкість роботи з цією моделлю в мене покращилася майже в сотню
разів. То ж може я її ще десь застосую.

### 2. LEN
Почувши про то, что довжина твітів має значення :), я не міг пройти повз цього 
читерського метода. Цей набір у мене складається з 3х чисел:
- відношення довжини першого твіта до другого
- довжина першого поділена на 100
- довжина другого, теж поділена на 100

Поділив я для того, щоб числа були біля одиниці. Звичайно, можно було б якось 
пронормувати.

### 3. BPemb

Ну, це мої улюблені ембедінги :). У якості фічі використано косінусна схожість (1-косинус)

### 4. Numberbatch

Теж спробував. Аналогічно BPemb.

## Результати
Найкращі результати дала комбінація всіх наборів фіч (класифікатор у всіх випадках - 
логістична регресія):

```
838	BASELINE	SS_ALL		
F: 0.594	Prec: 0.545	Rec: 0.651		
P-corr: 0.562	F1: 0.621	Prec: 0.615	Rec: 0.629
```

Звичайно, хотілося б подивиться, що може дати більш легкий набіл класифікаторів - 
LEN+BPemb (бо на продакшен я би взяв саме його через зручність, швидкість, та
невеликий об'єм пам'яті):

```
838	BASELINE	SS_LITE		
F: 0.579	Prec: 0.521	Rec: 0.651		
P-corr: 0.543	F1: 0.618	Prec: 0.658	Rec: 0.583
```

Хоча, ні - довжина, це трохи чітерський набір. В реальному житті, скоріш за все,
вона не спрацює. Спробуємо тільки "набір" (в лапках, бо це одна цифра) BPemb:

```
838	BASELINE	SS_VL		
F: 0.569	Prec: 0.498	Rec: 0.663		
P-corr: 0.520	F1: 0.592	Prec: 0.583	Rec: 0.600
```
Лише на 3 відсотка менше, ніж набагато складніший набір ознак!

Тепер ще декілька результатів:

Тільки Numberbatch:

```
838	BASELINE	SS_NB		
F: 0.462	Prec: 0.382	Rec: 0.583		
P-corr: 0.392	F1: 0.488	Prec: 0.366	Rec: 0.731
```

Тільки BERT:

```
838	BASELINE	SS_BERT		
F: 0.362	Prec: 0.225	Rec: 0.931		
P-corr: 0.117	F1: 0.441	Prec: 0.331	Rec: 0.657
```

Numberbatch+BERT:

```
838	BASELINE	SS_NBBERT		
F: 0.481	Prec: 0.398	Rec: 0.606		
P-corr: 0.398	F1: 0.487	Prec: 0.365	Rec: 0.731
```

І на останок, просто було цікаво, що дає фіча LEN:

```
838	BASELINE	SS_LEN		
F: 0.375	Prec: 0.308	Rec: 0.480		
P-corr: 0.216	F1: 0.392	Prec: 0.315	Rec: 0.520
```

Що можна було б ще зробити:

- перевірити інші запропоновані варианти фіч
- до набору BERT додати фічу відстані твіта до себе ж - це дало б можливість 
нормалізувати відстані між двома твітами.
- використати BERT в режимі "як ЕЛМО" - коли розбирався з ним, знайшов, як це робити,
але поки зупинився.
- інші класифікатори, звичайно ще б опробувати LSTM-класифікатори.
- опробувати різні варианти нормировки векторів ознак.